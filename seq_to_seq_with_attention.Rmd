# Sequence-to-sequence models with attention {#seq2seq_att}

As I write this, many state of the art models used in natural language processing are based on the Transformer
[@VaswaniSPUJGKP17] idea (see e.g., BERT [\@abs-1810-04805], the OpenAI Transformer [@Radford2018ImprovingLU], GPT-2
[@Radford2018ImprovingLU]).

The "shocking" thing about the original Transformer was that it did not use recurrent neural networks to process sequential
data, but mainly relied on the guiding power of *attention* to tackle the challenges inherent in -- multiple, sometimes --
sequentiality. The concept of *attention* is so central to current deep learning that we want to introduce it independently of
how it is used in *Transformer*-based models. In this chapter, thus, we show an example of neural machine translation built
"classically", that is, using RNNs, but with the crucial addition of *attention*.

## Why attention?

Translation is a prototypical example of sequence-to-sequence processing: sequence in, sequence out. The natural architectural
choice are RNNs, for they carry a hidden state through computations. The first RNN would *encode* the source sentence, at each
time taking in a token and the last hidden state. When it is done, it delivers the final state to the second RNN, the
*decoder*, which now starts generating token after token, based on what it got from the encoder and its own hidden states, as
they get updated over time. This is sufficient to produce a coherent output; but otherwise, the decoder's job is really hard:
All it gets from the encoder is a single compressed code where all sequentiality is lost.

It's here that *attention* comes in: What if the decoder had access to the encoder state *over time*, and could decide, when
generating tokens, what encoder states to preferentially look at *at a given step*? For that to happen, we need, at each
decoder time step, to compare somehow the current decoder hidden state to all encoder hidden states produced while encoding.
How this is accomplished technically may vary -- see e.g. [@LuongPM15] for a concise overview -- but the idea is always the
same. Our example will use Bahdanau-style [@2014arXiv1409.0473B] additive attention.

## Preprocessing with `torchtext`

Conveniently, we can make use of `torchtext` to handle preprocessing. A `Field` holds a specification of how to tokenize the
input and how to transform it to a tensor.

In our example, the source language will be English, and the target will be French. Here, we ask torch to use Spacy as a
tokenizer for both languages, and what tokens to use to mark sentence beginnings and endings:

```{python}
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

from torchtext.data import Field, BucketIterator
from torchtext.datasets import IWSLT

import random
import numpy as np
import math

src_spec = Field(
  tokenize = "spacy",
  tokenizer_language="en",
  init_token = '<sos>',
  eos_token = '<eos>',
  lower = True)

trg_spec = Field(
  tokenize = "spacy",
  tokenizer_language="fr",
  init_token = '<sos>',
  eos_token = '<eos>',
  lower = True)
```

These `Field`s will soon be used to store the *vocabularies* for both languages, that is, the mappings from words to integers.
But first, we need a dataset. We use the data from the 2016 IWSLT TED talk translation contest, nicely downloadable, and
already split into training, validation and testing subsets by torch:

```{python}
train_data, valid_data, test_data = IWSLT.splits(exts = ('.en', '.fr'), fields = (src_spec, trg_spec))

len(train_data.examples), len(valid_data.examples), len(test_data.examples)
```

    {(220400, 1026, 1305)}

As you see, the training set is gigantic, so one epoch of training will take some time even on a decent GPU. Let's see a few
examples:

```{python}
vars(train_data.examples[111])
vars(train_data.examples[11111])
vars(train_data.examples[111111])
```

    {'src': ['on', 'one', 'of', 'the', 'last', 'dive', 'series', ',', 'we', 'counted', '200', 'species', 'in', 'these', 'areas', '--', '198', 'were', 'new', ',', 'new', 'species', '.'], 'trg': ['dans', 'une', 'des', 'plus', 'récentes', 'plongées', 'on', 'a', 'compté', '200', 'espèces', 'dans', 'ces', 'régions', '.', '198', 'étaient', 'nouvelles', '-', 'de', 'nouvelles', 'espèces', '.']}

    {'src': ['this', 'is', 'one', 'of', 'the', 'true', 'masterpieces', 'in', 'puzzle', 'design', 'besides', 'rubik', "'s", 'cube', '.'], 'trg': ["c'", 'est', 'un', 'des', 'véritables', 'chefs', '-', "d'", 'œuvre', 'en', 'terme', 'de', 'casse-tête', ',', 'avec', 'le', "rubik'", 's', 'cube', '.']}

    {'src': ['i', 'sat', 'him', 'down', ',', 'i', 'caricatured', 'him', ',', 'and', 'since', 'then', 'i', "'ve", 'caricatured', 'hundreds', 'of', 'celebrities', '.'], 'trg': ['je', 'me', 'suis', 'assis', ',', 'je', "l'", 'ai', 'caricaturé', ',', 'et', 'vu', 'que', "j'", 'avais', 'caricaturé', 'des', 'centaines', 'de', 'célébrités', '.']}

Now we can build the vocabularies on the source and target, respectively.

```{python}
src_spec.build_vocab(train_data, min_freq = 2)

trg_spec.build_vocab(train_data, min_freq = 2)

len(src_spec.vocab), len(trg_spec.vocab)
```

    (34948, 45032)

`vocab` is a dictionary, with tokens as keys and integers as values:

```{python}
src_spec.vocab.stoi["cat"], trg_spec.vocab.stoi["chat"]
```

    (2036, 2424)

In locations 1-4, we have the special tokens indicating unknown input, padding, start of sentence and end of sentence.

```{python}
src_spec.vocab.itos[0], src_spec.vocab.itos[1], src_spec.vocab.itos[2], src_spec.vocab.itos[3]
```

    ('<unk>', '<pad>', '<sos>', '<eos>')

Preprocessing-wise, that's already it. All that remains to be done before proceeding to model definition is to create
iterators for the training, validation and testing sets. The `BucketIterator` class will assemble batches so that sentences of
similar length go together:

```{python}
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

batch_size = 8

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, valid_data, test_data),
    batch_size = batch_size,
    device = device)
    
```

Let's inspect the first batch.

```{python}
batch = next(iter(train_iterator))
batch.src.shape, batch.trg.shape
```

    (torch.Size([26, 8]), torch.Size([34, 8]))

If you've been working with Keras / TensorFlow, you may be surprised to see the batch dimension winding up in second place.
This is due to a default setting of `torchtext` `Field`s (which we could have changed had we wanted to):

```{python}
train_data.fields["src"]._batch_first
```

    False

Another interesting observation, especially if you come from Keras: Every batch really is of (potentially) different sentence
length, keeping the amount of padding minimal:

```{python}
batch.src[ :, 0]
```

    tensor([    2,    59,    10, 15954,     0,    64,     4,    15,   242,    42,
              690,     4,    28,    14,    18,     6,  1239, 17430,    13,   445,
                5,     3,     1,     1,     1,     1], device='cuda:0')

## Model

The model is a hierarchical composition of *modules*. We start with the encoder.

### Encoder

The encoder embeds its input, runs it through a bidirectional RNN (a GRU, to be precise), and returns the GRU's outputs as
well as a processed version of the final hidden states (the plural -- outputs, states -- being due to the RNN's
bidirectionality).

For Keras users, it makes sense to pay special attention to the tensor shapes returned by the GRU (the same would hold were we
using an LSTM instead): The respective tensors are

-   the outputs from all time steps (corresponding to what you'd get from Keras if you specified `return_sequences = True`),
    and
-   the last hidden state (available from Keras using `return_state = True`)

```{python}
num_input_features = len(src_spec.vocab)

encoder_embedding_dim = 32
encoder_hidden_dim = 64

decoder_hidden_dim = 64

encoder_dropout = 0.5


class Encoder(nn.Module):
  
    def __init__(self, num_input_features, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout):
        super().__init__()
        self.num_input_features = num_input_features
        self.embedding_dim = embedding_dim
        self.encoder_hidden_dim = encoder_hidden_dim
        self.decoder_hidden_dim = decoder_hidden_dim
        self.dropout = dropout
        self.embedding = nn.Embedding(num_input_features, embedding_dim)
        self.rnn = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional = True)
        self.fc = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)
        self.dropout = nn.Dropout(dropout)
    
    # src: seq_len * bs
    def forward(self, src):
        
        # embedded: seq_len * bs * embedding_dim
        # each input token gets embedded to degree embedding_dim
        embedded = self.dropout(self.embedding(src))
        # output: seq_len * bs * (2 * hidden_size)
        #  => tensor containing the output features h_t for each t (!)
        # hidden: 2 * bs * hidden_size
        #  => tensor containing the hidden state for t = seq_len
        outputs, hidden = self.rnn(embedded)
        # concatenate last state from both directions
        # input size to fc then is bs * 2 * hidden_size
        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))
        # hidden is now bs * decoder_hidden_dim
        return outputs, hidden

encoder = Encoder(num_input_features, encoder_embedding_dim, encoder_hidden_dim, decoder_hidden_dim, encoder_dropout).to(device)
```

As a quick check, let's call the encoder on the first batch's input sentence:

```{python}
encoder_output = encoder.forward(batch.src)
encoder_outputs = encoder_output[0]
decoder_hidden = encoder_output[1]
encoder_outputs.size(), decoder_hidden.size()
```

    # output is seq_len * bs * (2 * hidden_size)
    # hidden is bs * decoder_hidden_dim
    (torch.Size([26, 8, 128]), torch.Size([8, 64]))

Next, we create the attention module, to be used by the decoder.

### Attention module

Every time it is called to generate a single target token, the decoder will ask the attention module to *score* every token in
the input sequence as to its relevance in the current context. To this end, it will need to know about the current decoder
state as well the whole of the input sequence:

```{python}
a = self.attention(decoder_hidden, encoder_outputs)
```

The attention module correlates decoder hidden state with all input tokens in the sequence (line 22) and returns a normalized
relevance score for each of them (lines 25/26). Like we said above, different ways exist to calculate the scores -- this one
concatenates the things to be correlated and passes them through a linear layer; one popular alternative would be to multiply
them.

```{python}
attention_dim = 8

class Attention(nn.Module):
    def __init__(self, encoder_hidden_dim, decoder_hidden_dim, attention_dim):
        super().__init__()
        self.encoder_hidden_dim = encoder_hidden_dim
        self.decoder_hidden_dim = decoder_hidden_dim
        self.attention_in = (encoder_hidden_dim * 2) + decoder_hidden_dim
        self.attention = nn.Linear(self.attention_in, attention_dim)
    def forward(self, decoder_hidden, encoder_outputs):
        src_len = encoder_outputs.shape[0]
        # bs * decoder_hidden_dim ->  bs * 1 * decoder_hidden_dim -> bs * seq_len * decoder_hidden_dim
        # repeats hidden for every source token
        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)
        # bs * seq_len * (2 * hidden)
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        # after cat: bs * seq_len * (hidden + 2 * hidden)
        # => concatenates, for every batch item and source token, hidden state from decoder
        # (encoder, initially) and encoder output
        # energy then is bs * seq_len * attention_dim
        energy = torch.tanh(self.attention(torch.cat((repeated_decoder_hidden, encoder_outputs), dim = 2)))
        # bs * seq_len 
        # a score for every source token
        attention = torch.sum(energy, dim=2)
        return F.softmax(attention, dim=1)

attention = Attention(encoder_hidden_dim, decoder_hidden_dim, attention_dim).to(device)
```

As we know how this module will be called, we can try it, too, in isolation:

```{python}
a = attention(decoder_hidden, encoder_outputs)
# will be bs * seq_len
a.size()
```

Now to the place where it gets used, the decoder.

### Decoder

To translate one sentence, the decoder will be called in a loop, each time generating one token based on three things:

-   The previous token. In inference mode, this will be the token it has just generated, in the previous loop iteration;
    however in training mode, this is often chosen to be the "correct" answer -- the thing it *should* have chosen. This
    technique is called *teacher forcing*; the below implementation applies it randomly in 50% of loop iterations. (Don't look
    for it here, in the decoder; we'll see it in the top-level `Seq2Seq` module.)
-   The hidden state. This will be the *encoder*'s hidden state on the very first call, and the *decoder\'*s own on every
    subsequent one.
-   The complete output (at all timesteps) from the encoder. We already know we always need the complete output so we can
    compute the attention weights.

The decoder calls the attention module to obtain the attention weights, and multiplies these with the encoder outputs to
obtain what is sometimes called the *attention vector* (line 26). This is then passed to an RNN, together with the embedded
input data. A processed version of the RNN's output, as well as the last hidden state, are returned, ready for use in
generating the next token.

```{python}
num_output_features = len(trg_spec.vocab)
decoder_embedding_dim = 32
decoder_dropout = 0.5

class Decoder(nn.Module):
    def __init__(self, num_output_features, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout, attention):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.encoder_hidden_dim = encoder_hidden_dim
        self.decoder_hidden_dim = decoder_hidden_dim
        self.num_output_features = num_output_features
        self.dropout = dropout
        self.attention = attention
        self.embedding = nn.Embedding(num_output_features, embedding_dim)
        self.rnn = nn.GRU((encoder_hidden_dim * 2) + embedding_dim, decoder_hidden_dim)
        self.out = nn.Linear(self.attention.attention_in + embedding_dim, num_output_features)
        self.dropout = nn.Dropout(dropout)
    def _weighted_encoder_rep(self, decoder_hidden, encoder_outputs):
        # bs * seq_len
        a = self.attention(decoder_hidden, encoder_outputs)
        # bs * 1 * seq_len
        a = a.unsqueeze(1)
        # bs * seq_len * (2 * hidden_size)
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        # bs * 1 * (2 * hidden_size)
        weighted_encoder_rep = torch.bmm(a, encoder_outputs)
        # 1 * bs * (2 * hidden_size)
        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)
        return weighted_encoder_rep
    def forward(self, input, decoder_hidden, encoder_outputs):
        # 1 * bs
        input = input.unsqueeze(0)
        # 1 * bs * decoder_embedding_dim
        embedded = self.dropout(self.embedding(input))
        # 1 * bs * (2 * hidden_size)
        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, encoder_outputs)
        # concatenate input embedding and score from attention module
        # embedded: 1 * bs * decoder_embedding_dim
        # weighted_encoder_rep: 1 * bs * (2 * hidden_size)
        # rnn_input: 1 * bs * (decoder_embedding_dim + (2 * hidden_size))
        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)
        # output: 1 * bs * decoder_hidden_dim
        # decoder_hidden: 1 * bs * decoder_hidden_dim (after unsqueeze)
        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))
        embedded = embedded.squeeze(0)
        output = output.squeeze(0)
        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)
        output = self.out(torch.cat((output, weighted_encoder_rep, embedded), dim = 1))
        # output is bs * num_output_features
        return output, decoder_hidden.squeeze(0)
  
```

Now we put it all together.

### Seq2seq module

A single call of the top-level `Seq2Seq` module will translate a single sentence -- or a batch of sentences, rather --,
calling the encoder just once and the decoder, in a loop.

```{python}
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
    def forward(self, src, trg, teacher_forcing_ratio = 0.5):
        batch_size = src.shape[1]
        max_len = trg.shape[0]
        trg_vocab_size = self.decoder.num_output_features
        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)
        encoder_outputs, hidden = self.encoder(src)
        # first input to the decoder is the <sos> token
        output = trg[0,:]
        for t in range(1, max_len):
            output, hidden = self.decoder(output, hidden, encoder_outputs)
            outputs[t] = output
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.max(1)[1]
            output = (trg[t] if teacher_force else top1)
        return outputs

model = Seq2Seq(encoder, decoder, device).to(device)


def init_weights(m):
    for name, param in m.named_parameters():
        if 'weight' in name:
            nn.init.normal_(param.data, mean=0, std=0.01)
        else:
            nn.init.constant_(param.data, 0)
model.apply(init_weights)
```

Now we're ready to train.

## Training and evaluation

All logic being contained in the modules, the training loop is concise:

```{python}
optimizer = optim.Adam(model.parameters())
pad_idx = trg_spec.vocab.stoi['<pad>']
criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)

def train(model, iterator, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0
    for i, batch in enumerate(iterator):
        if i % 1000 == 0: print(i, end = " ", flush=True)
        src = batch.src
        trg = batch.trg
        optimizer.zero_grad()
        # seq_len * bs * num_output_features
        output = model(src, trg)
        # ((seq_len - 1) * bs) * num_output_features (output[1:] is (seq_len - 1) * bs * num_output_features))
        output = output[1:].view(-1, output.shape[-1])
        # (trg_len - 1) 
        trg = trg[1:].view(-1)
        loss = criterion(output, trg)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        epoch_loss += loss.item()
    print()
    return epoch_loss / len(iterator)
```

We will want to monitor performance while training, so here is a helper function that sets the model to evaluation mode, turns
off teacher forcing and computes the loss:

```{python}
def evaluate(model, iterator, criterion):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for _, batch in enumerate(iterator):
            src = batch.src
            trg = batch.trg
            output = model(src, trg, 0) 
            output = output[1:].view(-1, output.shape[-1])
            trg = trg[1:].view(-1)
            loss = criterion(output, trg)
            epoch_loss += loss.item()
    return epoch_loss / len(iterator)
```

Of course, what we really are interested in is how the translations look to us humans; here is a function that translates
whatever we pass it:

```{python}
def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):
    model.eval()
    if isinstance(sentence, str):
        nlp = spacy.load('en')
        tokens = [token.text.lower() for token in nlp(sentence)]
    else:
        tokens = [token.lower() for token in sentence]
    tokens = [src_field.init_token] + tokens + [src_field.eos_token]
    src_indexes = [src_field.vocab.stoi[token] for token in tokens]
    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
    src_len = torch.LongTensor([len(src_indexes)]).to(device)
    with torch.no_grad():
        encoder_outputs, hidden = model.encoder(src_tensor)
    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]
    for i in range(max_len):
        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)
        with torch.no_grad():
            output, hidden = model.decoder(trg_tensor, hidden, encoder_outputs)
            pred_token = output.argmax(1).item()
            trg_indexes.append(pred_token)
            if pred_token == trg_field.vocab.stoi[trg_field.eos_token]: break
    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]
    return trg_tokens[1:]
```

Now here is the training loop. Every epoch, we print the translations of eight sentences, picked at random from the training
set:

```{python}
n_epochs = 10
clip = 1

example_idx = [11, 77, 133, 241, 333, 477, 555, 777]

for epoch in range(n_epochs):
    train_loss = train(model, train_iterator, optimizer, criterion, clip)
    valid_loss = evaluate(model, valid_iterator, criterion)
    test_loss = evaluate(model, test_iterator, criterion)
    print(f'Epoch: {epoch+1:02}')
    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
    print(f'\tTest Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')
    for i in range(8):
        example_src = vars(train_data.examples[example_idx[i]])['src']
        example_trg = vars(train_data.examples[example_idx[i]])['trg']
        translation = translate_sentence(example_src, src_spec, trg_spec, model, device)
        src_sentence = " ".join(i for i in example_src)
        target_sentence = " ".join(i for i in example_trg)
        translated_sentence = " ".join(i for i in translation)
        print("Source: " + src_sentence)
        print("Target: " + target_sentence)
        print("Predicted: " + translated_sentence + "\n")
```

You may be curious to see the output - right?

## Results

We show losses and translations after epochs 1, 5, and 9. From how training progresses, we don't expect this model to result
in perfect translations for the given dataset. Remember, this is a real-world dataset, unlike the toy datasets often used in
deep learning tutorials -- model architecture may well be far too unsophisticated under these circumstances.

We'll see a *Transformer*-based model in the next chapter -- feel free to compare both models on the same task!

    Epoch: 01
        Train Loss: 5.252 | Train PPL: 190.952
        Val. Loss: 4.944  |  Val. PPL: 140.359
        Test Loss: 4.951  | Test PPL: 141.259 


    Epoch: 05
        Train Loss: 4.153 | Train PPL:  63.636
        Val. Loss: 4.559  |  Val. PPL:  95.500
        Test Loss: 4.555  | Test PPL:  95.071 

    Epoch: 09
          Train Loss: 4.005 | Train PPL:  54.870
          Val. Loss: 4.551  |  Val. PPL:  94.684
        Test Loss: 4.530  | Test PPL:  92.792 

+---------+------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                             |
+=========+==================================================================================================================+
| Source  | most of the earthquakes and volcanoes are in the sea , at the bottom of the sea .                                |
+---------+------------------------------------------------------------------------------------------------------------------+
| Target  | la plupart des tremblements de terre et de volcans se produisent dans la mer - au fond de la mer .               |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | la plupart des les et les les dans la eau , la eau .                                                             |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | la plupart des les et les des sont dans la mer , au sommet de la mer .                                           |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | la plupart des terres et les volcans sont dans la mer , au fond de la mer .                                      |
+---------+------------------------------------------------------------------------------------------------------------------+

+---------+------------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                                   |
+=========+========================================================================================================================+
| Source  | and then we had some hint that these things existed all along the axis of it , because if you 've got volcanism ,      |
|         | water 's going to get down from the sea into cracks in the sea floor , come in contact with magma , and come shooting  |
|         | out hot .                                                                                                              |
+---------+------------------------------------------------------------------------------------------------------------------------+
| Target  | nous avions une idée que ces choses existaient tout au long de cet axe , car s' il y a du volcanisme , l´eau va        |
|         | descendre de la mer dans les fentes du sol marin , se mettre en contact avec le magma , et jaillir avec de hautes      |
|         | températures .                                                                                                         |
+---------+------------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | let nous avons eu beaucoup de ces ces ces choses qui sont en fait , parce que si vous avez , , , , , , à la , dans la  |
|         | eau dans dans la eau , dans le sol , , , avec les \<unk\> , et , et                                                    |
+---------+------------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | et puis nous avons eu une chose que ces choses ont été l' axe de l' , , parce que si vous avez \<unk\> \<unk\> , eau   |
|         | eau , la mer dans les dans dans la mer dans la mer , et avec son contact avec la . et et                               |
+---------+------------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | et puis nous avons eu un idée que ces choses ont tous les axe de , , parce que si vous avez \<unk\> , eau , l' eau va  |
|         | descendre dans la mer dans dans dans mer dans la mer , dans la pluie , et de la de de                                  |
+---------+------------------------------------------------------------------------------------------------------------------------+

+---------+------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                             |
+=========+==================================================================================================================+
| Source  | they do n't need the sun at all .                                                                                |
+---------+------------------------------------------------------------------------------------------------------------------+
| Target  | ils n´ont pas du tout besoin de soleil .                                                                         |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | ils n' ont pas besoin de la terre .                                                                              |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | ls n' ont pas besoin de soleil .                                                                                 |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | ils ne ont pas besoin de soleil soleil tout tout .                                                               |
+---------+------------------------------------------------------------------------------------------------------------------+

+---------+-----------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                                  |
+=========+=======================================================================================================================+
| Source  | because it 's shown the way we apply , generate and use knowledge is affected by our social and institutional context |
|         | , which told us what in communism ?                                                                                   |
+---------+-----------------------------------------------------------------------------------------------------------------------+
| Target  | parce qu' il est démontré que la façon dont nous appliquons , générons , et utilisons les connaissances est affectée  |
|         | par notre contexte social et institutionnel , qui nous a dit quoi pendant le communisme ?                             |
+---------+-----------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | parce que c' est que nous nous nous , , , , , et de l' information , , et notre , , et , et , ce qui a ce que ?       |
+---------+-----------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | parce que c' est la façon dont nous on , , et et et la connaissance , et et et et et le contexte , qui nous nous      |
|         | demandé ce qu' il est ?                                                                                               |
+---------+-----------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | parce que c' est montré comment nous nous , , et et et connaissances et et le monde social et et et nous nous a       |
|         | demandé à le ? ?                                                                                                      |
+---------+-----------------------------------------------------------------------------------------------------------------------+

+---------+--------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                               |
+=========+====================================================================================================================+
| Source  | it 's not fiction , it 's not story tales , it 's not make - believe ; it 's cold , hard science .                 |
+---------+--------------------------------------------------------------------------------------------------------------------+
| Target  | ce n' est pas de la fiction , ce n' est pas des histoires , ce n' est pas des fadaises ; c' est de la science pure |
|         | .                                                                                                                  |
+---------+--------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | ce n' est pas pas , , n' n' est pas de de , , n' est pas pas , , c' est , , .                                      |
+---------+--------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | c' n' est pas pas la fiction , c' n' est pas pas de histoires , ça ne est pas pas croire , c' est extrêmement      |
|         | difficile .                                                                                                        |
+---------+--------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | c' n' est pas pas fiction , c' n' est pas pas de histoires , c' est est pas facile . c' est froid , la science .   |
+---------+--------------------------------------------------------------------------------------------------------------------+

+---------+---------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                                |
+=========+=====================================================================================================================+
| Source  | we all want to be there , in the upper right quadrant , where performance is strong and learning opportunities are  |
|         | equally distributed .                                                                                               |
+---------+---------------------------------------------------------------------------------------------------------------------+
| Target  | on veut tous être dans le quadrant supérieur droit , où les performances sont remarquables et les opportunités d'   |
|         | apprentissage sont égales .                                                                                         |
+---------+---------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | nous voulons tous seulement , , dans le cas , , où les les sont et et les les les sont . .                          |
+---------+---------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | nous voulons voulons être être , dans dans le coin droite , où où la est est est des opportunités et des            |
|         | apprentissage sont des .                                                                                            |
+---------+---------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | ous voulons voulons être là , dans le haut , , où les performances est plus et et les opportunités sont sont .      |
+---------+---------------------------------------------------------------------------------------------------------------------+

+---------+------------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                                   |
+=========+========================================================================================================================+
| Source  | those are the critical questions , and what we have learned from pisa is that , in high - performing education systems |
|         | , the leaders have convinced their citizens to make choices that value education , their future , more than            |
|         | consumption today .                                                                                                    |
+---------+------------------------------------------------------------------------------------------------------------------------+
| Target  | ce sont là les questions importantes , et ce qu' on a appris de pisa est que dans les systèmes éducatifs très          |
|         | performants les dirigeants ont aujourd'hui convaincu leurs citoyens de faire des choix qui font valoir leur éducation  |
|         | , leur avenir , plus que la consommation .                                                                             |
+---------+------------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | ces sont des idées , et ce que nous avons avons appris à l' de de , , , dans les des de des , , les les les les les    |
|         | pour les les de la santé , les les , , , , , , ,                                                                       |
+---------+------------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | ces sont questions questions et et ce nous nous avons appris à l' , c' est dans les systèmes de systèmes , , les       |
|         | organisations qui ont des citoyens de leur pour les choix , des systèmes , , , , , , , plus de énergie aujourd'hui .   |
+---------+------------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | ces sont questions questions questions et et ce dont nous avons appris à l' est est , , dans les systèmes de systèmes  |
|         | de , , les dirigeants ont citoyens leurs citoyens pour faire les choix , les futur , , futur , plus de consommation de |
|         | plus .                                                                                                                 |
+---------+------------------------------------------------------------------------------------------------------------------------+

+---------+---------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                                |
+=========+=====================================================================================================================+
| Source  | when the sailors mutinied at sea in a demand for humane conditions , it was these teenagers that fed the crew .     |
+---------+---------------------------------------------------------------------------------------------------------------------+
| Target  | quand les marins se sont mutinés en mer pour exiger des conditions plus humaines , ce sont ces adolescents qui ont  |
|         | nourri l' équipage .                                                                                                |
+---------+---------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | quand les \<unk\> de l' eau dans dans la de de pour les les , , , ces ces qui ont été été .                         |
+---------+---------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | quand les \<unk\> \<unk\> dans la mer dans une vie avec la conditions conditions , les parents , c' était ces       |
|         | adolescents qui ont été équipe .                                                                                    |
+---------+---------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | quand les marins \<unk\> dans la mer dans une demande de la nature , , c' était ces adolescents qui qui l' équipe . |
+---------+---------------------------------------------------------------------------------------------------------------------+
