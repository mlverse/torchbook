# Using torch optimizers {#simple_net_optim}

So far, we have been updating model parameters ourselves, following a simple algorithm: The calculated gradients told us which
direction, on the loss curve, was "downward"; the learning rate told us how big a step to take in that direction. This is a
straightforward implementation of *gradient descent*.

However, more sophisticated optimization algorithms exist, and many of these are provided by torch: for example, [Rmsprop](),
[Adam](), or [Adadelta](). In this chapter, we'll see how to replace our manual updates using `SGD`, torch's stochastic
gradient descent optimizer.

## Torch optimizers

For demonstration, here is a very simple network, consisting of just a single linear layer, and a single data point to call
the model on.

```{python}
import torch
model = torch.nn.Linear(3, 1)
[(p[0], p[1]) for p in model.named_parameters()]

data  = torch.randn(1, 3)

```

On creation, an optimizer is told what parameters to work on:

```{python}
opt = torch.optim.SGD(model.parameters(), lr = 0.01)
opt
```

At any time, we can inspect those parameters:

```{python}
opt.param_groups[0]["params"]
```

Now we perform the forward and the backward pass. The backward pass calculates the gradients, but does not update the
parameters, as we can see both from the model *and* the optimizer objects:

```{python}
out = model(data)
out.backward()
opt.param_groups[0]["params"]
[(p[0], p[1]) for p in model.named_parameters()]
```

Calling `step()` on the optimizer actually does the updates:

```{python}
opt.step()
opt.param_groups[0]["params"]
[(p[0], p[1]) for p in model.named_parameters()]
```

If we perform optimization in a loop, as we'd usually do with neural networks, we need to make sure we call
`optimizer.zero_grad()` on every step, as otherwise gradients would be accumulated. We'll see this in our final version of the
network.

## Simple net with `torch.optim`

```{python}

import torch

### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in = 3
# output dimensionality (number of predicted features)
d_out = 1
# number of observations in training set
n = 100

# create random data
x = torch.randn(n, d_in) 
y = x[ : , 0, None] * 0.2 - x[ : , 1, None] * 1.3 - x[ : , 2, None] * 0.5 + torch.randn(n, 1)

### define the network ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden = 32

model = torch.nn.Sequential(
    torch.nn.Linear(d_in, d_hidden),
    torch.nn.ReLU(),
    torch.nn.Linear(d_hidden, d_out),
)

mse_loss = torch.nn.MSELoss(reduction='sum')


### training loop --------------------------------------------------------------
learning_rate = 1e-4

optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)

for t in range(200):
    
    ### -------- Forward pass -------- 

    y_pred = model(x)

    ### -------- compute loss -------- 
    loss = mse_loss(y_pred, y)
    if t % 10 == 0: print(t, loss.item())

    ### -------- Backpropagation -------- 

    # Still need to zero out the gradients before the backward pass, only this time, on the optimizer object
    optimizer.zero_grad()
    
    # gradients are still computed on the loss tensor
    loss.backward()
 
    ### -------- Update weights -------- 
    
     # use the ptimizer to update model parameters
    optimizer.step()


```

At this point, we're fully completed the transformation; the network has been fully torchified. In the next section, we'll see
classical applications of deep learning for supervised and unsupervised learning.
