# Using torch modules {#simple_net_modules}

As next-to-last step of our torchifying the simple network, we modularize it -- in a very literal sense, actually -- by
replacing our manual function calls with `torch` *modules*.

## Modules

From other frameworks, you may be used to distinguishing between models and layers. In `torch`, both are instances of
`nn_Module()` and thus, have some methods in common.

### Layers as modules

For example, instead of writing out an affine operation by hand -- say `x$mm(w1) + b1`, as we've been doing so far -- we can
create a linear module. This line creates a linear layer that expects an input with three features, and outputs a single
output per observation:

```{r}
l <- nn_linear(3, 1)
```

It has two parameters, "weight" and "bias":

```{r}
l$parameters
```

Both parameters have automatically been initialized for us.

Modules are callable; calling a module runs its `forward()` method, which for a linear layer just matrix-multiplies the input
with the weights and adds the bias.

Let's try this:

```{r}
data  <- torch_randn(10, 3)
out <- l(data)
```

The output is a tensor that not just contains data

```{r}
###TBD###
out$data()
```

but also, knows what has to be done to obtain gradients:

```{r}
out$grad_fn
```

At this point, we have done a forward pass, but as we haven't called `backward(),` no gradients have yet been calculated:

```{r}
l$weight$grad
l$bias$grad
```

Let's change this:

```{python, error = TRUE}
out$backward()
```

Autograd expects the output tensor to be a scalar, while in our example, we have a tensor of size `(10, 1)`. This error won't
happen in our "real example" below, where we'll work with *batches* of inputs (or rather, a single batch, for simplicity). But
still, it's interesting to see how to resolve this.

To make the example work, we introduce a -- virtual -- final aggregation step, the mean say. Let's call it `avg`. If such a
mean were taken, its gradient with respect to `l$weight` would be obtained via the chain rule:

\begin{equation*} 
 \frac{\partial avg}{\partial w} = \frac{\partial avg}{\partial out}  \frac{\partial out}{\partial w}
 (\#eq:backwardgradient)
\end{equation*}

Of the quantities on the right side, we're interested in the second. We need to provide the first one, the way it would look
*if really we were taking the mean*:

```{r}
out$backward(gradient = torch_tensor(10)$`repeat`(10)$unsqueeze(0)$t())
```

Now, `l$weight$grad` and `l$bias$grad` *will* contain the respective gradients:

```{r}
l$weight$grad
l$bias$grad
```

Back to the main thread. `nn_linear()` is one of the most often used layers in neural networks; we'll see others
(convolutional, recurrent...) in later chapters. In the usual lingo, combining layers yields *models*.

### Models as modules

Now, *models* are just modules that contain other modules. For example, if all data is supposed to flow through the same
nodes, in a unidirectional fashion, then `nn_sequential()` can be used to build a simple graph.

For example:

```{r}
model <- nn_sequential(
    nn_linear(3, 16),
    nn_relu(),
    nn_linear(16, 1),
)
```

We can use the same technique used on the single linear layer to get an overview of all model parameters:

```{r}
model$parameters
```

Individual parameters can just be directly inspected making use of their position in the sequential model, e.g.

```{r}
# TBD
model[[1]]$bias
```

And just like the simple `Linear` module, this model can directly be called on data:

```{r}
out <- model(data)
```

On this composite module, calling `backward()` will effectuate a backward pass through all the layers:

```{r}
out$backward(gradient = torch_tensor(10)$`repeat`(10)$unsqueeze(0)$t())

# e.g.
# TBD
model[[1]]$bias$grad
```

And placing the composite on the GPU will move all tensors there:

```{r}
model$cuda()
# TBD
model[[1]]$bias$grad
```

Now let's see how using `nn_sequential()` can simplify our example network. As an act of further simplication, we won't
calculate mean squared error "by hand" anymore either; instead, we'll use one of the loss functions torch provides,
`torch.nn.MSELoss`.

## Simple network using modules

```{r}
library(torch)

### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in <- 3
# output dimensionality (number of predicted features)
d_out <- 1
# number of observations in training set
n <- 100


# create random data
x <- torch_randn(n, d_in)
y <-
  x[, 0, NULL] * 0.2 - x[, 1, NULL] * 1.3 - x[, 2, NULL] * 0.5 + torch_randn(n, 1)



### define the network ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden <- 32

model <- nn_sequential(
  nn_linear(d_in, d_hidden),
  nn_relu(),
  nn_linear(d_hidden, d_out)
)

# TBD
#mse_loss = torch.nn.MSELoss(reduction='sum')


### network parameters ---------------------------------------------------------

learning_rate <- 1e-4

### training loop --------------------------------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass -------- 
  
  y_pred <- model(x)
  
  ### -------- compute loss -------- 
  loss <- nnf_mse_loss(y_pred, y, reduction = "sum")
  if t % 10 == 0: print(t, as.numeric(loss))
  
  ### -------- Backpropagation -------- 
  
  # Zero the gradients before running the backward pass.
  # TBD
  # model$zero_grad()
  if (t > 1) model$parameters %>% purrr::walk(function(param) param$grad$zero_())
  
  # compute gradient of the loss w.r.t. all learnable parameters of the model
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # Wrap in with_no_grad() because this is a part we DON'T want to record for automatic gradient computation
  # Update each parameter by its `grad`
  
  with_no_grad({
    model$parameters %>% purrr::walk(function(param) param$sub_(learning_rate * param$grad))
  })
  
}
```

## Appendix: Python code

```{python}
import torch

### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in = 3
# output dimensionality (number of predicted features)
d_out = 1
# number of observations in training set
n = 100

# create random data
x = torch.randn(n, d_in) 
y = x[ : , 0, None] * 0.2 - x[ : , 1, None] * 1.3 - x[ : , 2, None] * 0.5 + torch.randn(n, 1)

### define the network ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden = 32

model = torch.nn.Sequential(
    torch.nn.Linear(d_in, d_hidden),
    torch.nn.ReLU(),
    torch.nn.Linear(d_hidden, d_out),
)

mse_loss = torch.nn.MSELoss(reduction='sum')


### training loop --------------------------------------------------------------
learning_rate = 1e-4

for t in range(200):
    
    ### -------- Forward pass -------- 

    y_pred = model(x)

    ### -------- compute loss -------- 
    loss = mse_loss(y_pred, y)
    if t % 10 == 0: print(t, loss.item())

    ### -------- Backpropagation -------- 

    # Zero the gradients before running the backward pass.
    model.zero_grad()
    
    # compute gradient of the loss w.r.t. all learnable parameters of the model
    loss.backward()
 
    ### -------- Update weights -------- 
    
    # Wrap in torch.no_grad() because this is a part we DON'T want to record for automatic gradient computation
    # Update each parameter by its `grad`
    with torch.no_grad():
        for param in model.parameters():
            param -= learning_rate * param.grad

```

The forward pass looks a lot better now; however, we still loop through the model's parameters and update each one by hand. As
a final act of simplification, the next chapter will show how to make use of torch optimizers instead.
