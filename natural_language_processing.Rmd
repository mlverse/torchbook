# (PART) Natural language processing {.unnumbered}

# Introduction {#NLP-intro .unnumbered}

As we write this, deep learning has firmly made its way into natural language processing, complementing its dominance in image
recognition. Two years ago already, Sebastian Ruder proclaimed that [NLP\'s ImageNet moment has
arrived](https://thegradient.pub/nlp-imagenet/). The analogy here is not about the dataset per se, but about availability of
models, trained on a rich collection of data, that can generalize to a variety of tasks in the overall domain. Just like in
image recognition, a model trained to classify ImageNet is assumed to have learned a lot about *features* on different scales
-- edges, corners, shapes, components of objects, objects -- in NLP a *language model*, trained to predict wods from their
surroundings,is assumed to have learned a lot about syntax, semantics, and -- perhaps -- even about the world. This general
knowledge is then useful in tasks such as ...

