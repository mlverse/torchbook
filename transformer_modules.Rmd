# Torch transformer modules {#transformer}

## "Attention is all you need"

When the original Transformer paper [@VaswaniSPUJGKP17] appeared, its provocative title can only have speeded up its road to
fame. Why would it be provocative? At that time, sequential data were firmly thought to be the realm of RNNs (albeit extended
by encoder-decoder attention): If one input token's probability depends on the previous input(s) (as in time series, language,
or music), it seems we need to keep some form of *state* to preserve sequential relationships over the whole calculation.

In fact, Transformer did not have RNNs, but compensated for lacking state in two ways: adding *positional encoding*, thus in
some way keeping track of where in the phrase a token is located, and most importantly, *self-attention*: making use of
context (surrounding tokens) when encoding each input token. With self-attention, *no token is an island*; instead, it only
gains its meaning through how it relates to its neighbors.

Seeing how excellent architectural explanations abund, ranging from code-oriented [\@rush-2018-annotated] to
[visual](https://jalammar.github.io/illustrated-transformer/), we just give a brief conceptual characterization.

#### Self-attention and "multi-head attention"

Each input word (after the usual embedding) plays three roles, designated by terms coming from *information retrieval*: query,
key, and value. In this chapter, we won't be coding attention from scratch, but relying on `torch` modules; so strictly, we
don't need to go there. But as query, key and value vectors have become part of the official transformer lingo, it's good to
have heard those terms.

In a nutshell, self-attention does not encode every word separately, but at every position, works with a conglomerate of
semantic and syntactic information that is made up, in some way, of the complete input phrase. The basic operation, like in
the encoder-decoder setup, is a *dot product* used to determine some form of similarity/promixity/relevance in semantic space.

This dot product occurs between the word that is being encoded -- appearing in its role as *query vector* -- and every other
word, each wearing their *key vector* hats. Essentially, these measures of affinity are normalized and used to weight the
*value vectors* corresponding to every *key* that was used in the comparison. Finally, for every *query* we aggregate the
weighted value vectors into a composite result, which is passed on to the next layer.

Why does each token have to wear three different hats? If it didn't, these affinity relationships would be symmetric (the dot
product per se being commutative), thus badly conforming with semantic and (especially!) syntactic reality. Thus, technically,
a word's query, key and value vectors are not the same; instead, each is obtained as the output of a different feedforward
layer.

So that is self-attention - now what does "multi-head attention" refer to? This simply is a "bag of attention modules", all
operating in parallel. Like that, multi-head attention is said to take care of taking into account multiple "representation
subspaces".

#### Overall architecture

Overall, transformer is an encoder stack, followed by a decoder stack. Both stacks are composed of several, identical
submodules combining multi-head attention, layer normalization [@2016arXiv160706450L], residual connections, and feedforward
neural networks applied pointwise to each input. [^1] In addition to the self-attention mechanism (conceptually) shared with
encoder layers, the decoder layers exercise a second form of attention: *encoder-decoder* attention allows them to
differentially pay attention to the output passed by the encoder.

## Implementation: building blocks

While it is certainly possible, and instructive, to build a transformer network from scratch, we won't reinvent the wheel but
instead, make use of `torch` layers that simplify the process significantly. `TransformerEncoderLayer` and
`TransformerDecoderLayer` are the basic modules that make up encoder and decoder stacks, respectively.

Here is a single encoder submodule comprising multi-head self-attention, layer normalization and feedforward networks:

```{python}
from torch.nn import TransformerEncoderLayer

e = TransformerEncoderLayer(d_model = 256, nhead = 2, dim_feedforward = 256, dropout = 0.2)
e
```

    TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): Linear(in_features=256, out_features=256, bias=True)
      )
      (linear1): Linear(in_features=256, out_features=256, bias=True)
      (dropout): Dropout(p=0.2, inplace=False)
      (linear2): Linear(in_features=256, out_features=256, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.2, inplace=False)
      (dropout2): Dropout(p=0.2, inplace=False)
    )

A decoder submodule looks similar, apart from the fact that it has an additional `MultiHeadAttention` (sub-)submodule. One is
for self-attention, the other, for attending to encoder input:

```{python}
from torch.nn import TransformerDecoderLayer

d = TransformerDecoderLayer(d_model = 256, nhead = 2, dim_feedforward = 256, dropout = 0.2)
```

Next up in the hierarchy are `TransformerEncoder` and `TransformerDecoder`. These are just containers, making up the
respective stacks:

```{python}
from torch.nn import TransformerEncoder

TransformerEncoder(encoder_layer = e, num_layers = 6)
```

```{python}
from torch.nn import TransformerDecoder

TransformerDecoder(decoder_layer = d, num_layers = 6)
```

There is even a `Transformer` module that takes in parameters for the sublayers
(`TransformerEncoderLayer`/`TransformerDecoderLayer`) as well as the containers (`TransformerEncoder`/`TransformerDecoder`).

## Results

Like in the previous chapter, we show losses and translations after epochs 1, 5, and 9.

[...]

    Epoch: 01 
        Train Loss: 5.286 | Train PPL: 197.526
          Val. Loss: 3.937 |  Val. PPL:  51.272
        Test Loss: 3.937 | Test PPL:  51.272 |


    Epoch: 05
        Train Loss: 2.844 | Train PPL:  17.187
          Val. Loss: 3.296 |  Val. PPL:  27.016
          Test Loss: 3.296 | Test PPL:  27.016 |
        
    Epoch: 09
        Train Loss: 2.318 | Train PPL:  10.152
          Val. Loss: 3.391 |  Val. PPL:  29.701
          Test Loss: 3.391 | Test PPL:  29.701 |

+---------+------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                             |
+=========+==================================================================================================================+
| Source  | most of the earthquakes and volcanoes are in the sea , at the bottom of the sea .                                |
+---------+------------------------------------------------------------------------------------------------------------------+
| Target  | většina zemětřesení a vulkánů je v moři - na mořském dně .                                                       |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | většina z moře a \<unk v moře .                                                                                  |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | většina zemětřesení a \<unk jsou v moři , na mořském dně .                                                       |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | většina zemětřesení a \<unk jsou v moře a dole na dně moře z moře .                                              |
+---------+------------------------------------------------------------------------------------------------------------------+

+--------+-------------------------------------------------------------------------------------------------------------------+
|        | Text                                                                                                              |
+========+===================================================================================================================+
| Source | and we knew it was volcanic back in the ' 60s , ' 70s .                                                           |
+--------+-------------------------------------------------------------------------------------------------------------------+
| Target | víme , že tam byl vulkanismus , v 60 . a 70 . letech .                                                            |
+--------+-------------------------------------------------------------------------------------------------------------------+
| Epoch  | a věděl jsme , že v 60 . letech , 70 . letech .                                                                   |
| 1      |                                                                                                                   |
+--------+-------------------------------------------------------------------------------------------------------------------+
| Epoch  | věděli jsme , že jsme , že jsme to bylo v 70 . 70 . letech .                                                      |
| 5      |                                                                                                                   |
+--------+-------------------------------------------------------------------------------------------------------------------+
| Epoch  | věděli jsme , že je ropa , v 60 . léta .                                                                          |
| 9      |                                                                                                                   |
+--------+-------------------------------------------------------------------------------------------------------------------+

+---------+------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                             |
+=========+==================================================================================================================+
| Source  | so here , you 've got this valley with this incredible alien landscape of pillars and hot springs and volcanic   |
|         | eruptions and earthquakes , inhabited by these very strange animals that live only on chemical energy coming out |
|         | of the ground .                                                                                                  |
+---------+------------------------------------------------------------------------------------------------------------------+
| Target  | takže tady máme to údolí s mimořádně nepřátelskou krajinou sloupů , horkých pramenů , vulkanických erupcí a      |
|         | zemětřesení , obydlených těmito velmi zvláštními živočichy , co žijí pouze z chemické energie vycházející ze     |
|         | země .                                                                                                           |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | tady máme údolí , s touto údolí \<unk \<unk a \<unk \<unk \<unk \<unk \<unk \<unk a \<unk \<unk tyto \<unk \<unk |
|         | \<unk \<unk , které žijí jen na povrch .                                                                         |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | takže tady máte tento údolí \<unk \<unk \<unk \<unk \<unk \<unk \<unk a zemětřesení a zemětřesení , \<unk \<unk  |
|         | \<unk tyto velmi neobvyklé živočichy , které žijí na tomto místě , které žijí na zemi .                          |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | tady máte tady s touto neuvěřitelnou čistotu .                                                                   |
+---------+------------------------------------------------------------------------------------------------------------------+

+---------+------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                             |
+=========+==================================================================================================================+
| Source  | and instead , what do we value ?                                                                                 |
+---------+------------------------------------------------------------------------------------------------------------------+
| Target  | a místo toho , čeho si vážíme ?                                                                                  |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | a místo , co děláme hodnotu ?                                                                                    |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | a místo toho , co děláme ?                                                                                       |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | a místo toho , co děláme ?                                                                                       |
+---------+------------------------------------------------------------------------------------------------------------------+

+---------+------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                             |
+=========+==================================================================================================================+
| Source  | these guys are facts .                                                                                           |
+---------+------------------------------------------------------------------------------------------------------------------+
| Target  | tohle jsou fakta , lidi .                                                                                        |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | tito jsou fakta .                                                                                                |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | tihle chlapíci jsou fakta .                                                                                      |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | tito chlapíci jsou fakta .                                                                                       |
+---------+------------------------------------------------------------------------------------------------------------------+

+---------+------------------------------------------------------------------------------------------------------------------+
|         | Text                                                                                                             |
+=========+==================================================================================================================+
| Source  | we see in other countries that it matters much less into which social context you 're born .                     |
+---------+------------------------------------------------------------------------------------------------------------------+
| Target  | v jiných zemích naopak vidíme , že záleží mnohem méně na tom , do jaké sociální vrstvy se kdo narodí .           |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 1 | vidíme , že v jiných zemích záleží na sociální kontextu , které se \<unk\> .                                     |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 5 | vidíme v jiných zemích , které záleží na tom , že záleží na tom , že se rodíme .                                 |
+---------+------------------------------------------------------------------------------------------------------------------+
| Epoch 9 | vidíme v jiných zemích , které záleží na tom , že záleží mnohem méně sociální kontext , ve kterém se dostanete . |
+---------+------------------------------------------------------------------------------------------------------------------+

+----------------+-----------------------------------------------------------------------------------------------------------+
|                | Text                                                                                                      |
+================+===========================================================================================================+
| Source         | how do the media talk about schools and teachers ?                                                        |
+----------------+-----------------------------------------------------------------------------------------------------------+
| Target         | jak reflektují školy a učitele média ?                                                                    |
+----------------+-----------------------------------------------------------------------------------------------------------+
| Epoch 1        | jak se média o školách a učitelé ?                                                                        |
+----------------+-----------------------------------------------------------------------------------------------------------+
| Epoch 5        | jak se média mluví o školách a učitelé ?                                                                  |
+----------------+-----------------------------------------------------------------------------------------------------------+
| Epoch 9        | jak se média mluví o školách a učitelé ?                                                                  |
+----------------+-----------------------------------------------------------------------------------------------------------+

+------+---------------------------------------------------------------------------------------------------------------------+
|      | Text                                                                                                                |
+======+=====================================================================================================================+
| So   | when peter moves his arm , that yellow spot you see there is the interface to the functioning of peter 's mind      |
| urce | taking place .                                                                                                      |
+------+---------------------------------------------------------------------------------------------------------------------+
| Ta   | když petr pohne svojí paží , ta žlutá tečka , kterou vidíte tady je rozhraní petrovi mysli k této aktivitě .        |
| rget |                                                                                                                     |
+------+---------------------------------------------------------------------------------------------------------------------+
| E    | když petr pohne svojí paží , ta žlutá tečka , kterou vidíte tady je rozhraní petrovi mysli k této aktivitě .        |
| poch |                                                                                                                     |
| 1    |                                                                                                                     |
+------+---------------------------------------------------------------------------------------------------------------------+
| E    | když peter diamandis svého paže , která je zde rozhraní mezi tímhle rozhraním , které se peter diamandis .          |
| poch |                                                                                                                     |
| 5    |                                                                                                                     |
+------+---------------------------------------------------------------------------------------------------------------------+
| E    | když se peter paži , ta žlutá tečka , kterou vidíte je rozhraní petrovi mysli k fungující peter \<unk\> si místo .  |
| poch |                                                                                                                     |
| 9    |                                                                                                                     |
+------+---------------------------------------------------------------------------------------------------------------------+

[^1]: Here "pointwise" implies that the weights are the same for every input fed into the layer. These feedforward neural
    networks would therefore be analogous to convolutional layers with kernel siez 1.
