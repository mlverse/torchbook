# Using autograd {#simple_net_autograd}

So far, all we've been using from `torch` is *tensors*. Predictions, loss, gradients, desired weight updates, new weights --
all these things we've been computing ourselves. In this chapter, we'll make a significant change: Namely, we spare ourselves
the cumbersome calculation of gradients, and have `torch` do it for us.

Before we see that in action, let's get some more background.

## Automatic differentiation with autograd

Torch uses a module called *autograd* to record operations performed on tensors, and store what has to be done to obtain the
respective gradients. These actions are stored as functions, and those functions are applied in order when the gradient of the
output (the loss, usually) with respect to those tensors is calculated: Function application starts from the output node, and
the gradients calculated are *propagated* *back* through the network. This is a form of *reverse mode automatic
differentiation*.

As users, we can see a bit of this implementation. As a prerequisite for this "recording" to happen, tensors have to be
created with `requires_grad = TRUE`. E.g.

```{r}
x <- torch_ones(2, 2, requires_grad = TRUE)
```

To be clear, this is a tensor *with respect to which* gradients have to be calculated -- normally, a tensor representing a
weight or a bias, not the input data [^1]. If we now perform some operation on that tensor, assigning the result to `y`

[^1]: Unless we *want* to change the data, as in adversarial example generation.

```{r}
y <- x$mean()
```

we find that `y` now has a non-empty `grad_fn` that tells `torch` how to compute the gradient of `y` with respect to `x`:

```{r}
y$grad_fn
```

Actual computation of gradients is triggered by calling `backward()` on the output tensor.

```{r}
y$backward()
```

After `backward()` has been called, `x` has a non-empty field `grad` that stores the gradient of `y` with respect to `x`:

```{r}
x$grad
```

With longer chains of computations, we can peek at how `torch` builds up a graph of backward operations.

Here is a slightly more complex example. We call `retain_grad()` on `y` and `z` just for demonstration purposes; by default,
intermediate gradients -- while of course they have to be computed -- aren't stored, in order to save memory.

```{r}
x1 <- torch_ones(2, 2, requires_grad = TRUE)
x2 <- torch_tensor(1.1, requires_grad = TRUE)

y <- x1 * (x2 + 2)
y$retain_grad()

z <- y$pow(2) * 3
z$retain_grad()

out <- z$mean()
```

Starting from `out$grad_fn`, we can follow the graph all back to the leaf nodes:

```{r}
# how to compute the gradient for mean, the last operation executed
out$grad_fn

# how to compute the gradient for the multiplication by 3 in z = y.pow(2) * 3
out$grad_fn$next_functions

# how to compute the gradient for pow in z = y.pow(2) * 3
out$grad_fn$next_functions[[1]]$next_functions

# how to compute the gradient for the multiplication in y = x * (x + 2)
out$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions

# how to compute the gradient for the two branches of y = x * (x + 2),
# where the left branch is a leaf node (AccumulateGrad for x1)
out$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions

# here we arrive at the other leaf node (AccumulateGrad for x2)
out$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions[[2]]$next_functions

```

After calling `out$backward()`, all tensors in the graph will have their respective gradients created. Without our calls to
`retain_grad` above, `z$grad` and `y$grad` would be empty:

```{r}
out$backward()

z$grad
y$grad
x2$grad
x1$grad
```

Thus acquainted with autograd, we're ready to modify our example.

## The simple network, now using autograd

For a single new line calling `loss$backward()`, now a number of lines (that did manual backprop) are gone:

```{r}
library(torch)

### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in <- 3
# output dimensionality (number of predicted features)
d_out <- 1
# number of observations in training set
n <- 100


# create random data
x <- torch_randn(n, d_in)
y <-
  x[, 0, NULL] * 0.2 - x[, 1, NULL] * 1.3 - x[, 2, NULL] * 0.5 + torch_randn(n, 1)


### initialize weights ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden <- 32
# weights connecting input to hidden layer
w1 <- torch_randn(d_in, d_hidden, requires_grad = TRUE)
# weights connecting hidden to output layer
w2 <- torch_randn(d_hidden, d_out, requires_grad = TRUE)

# hidden layer bias
b1 <- torch_zeros(1, d_hidden, requires_grad = TRUE)
# output layer bias
b2 <- torch_zeros(1, d_out, requires_grad = TRUE)

### network parameters ---------------------------------------------------------

learning_rate <- 1e-4

### training loop --------------------------------------------------------------

for (t in 1:200) {
  ### -------- Forward pass --------
  
  y_pred <- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)
  
  ### -------- compute loss -------- 
  loss <- (y_pred - y)$pow(2).sum()
  if (t %% 10 == 0)
    cat("Epoch: ", t, "   Loss: ", loss$item(), "\n")
  
  ### -------- Backpropagation --------
  
  # compute the gradient of loss with respect to all tensors with requires_grad = True.
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # Wrap in with_no_grad() because this is a part we DON'T want to record for automatic gradient computation
   with_no_grad({
     w1 <- w1 - learning_rate * w1$grad
     w2 <- w2 - learning_rate * w2$grad
     b1 <- b1 - learning_rate * b1$grad
     b2 <- b2 - learning_rate * b2$grad     
     
     # Zero the gradients after every pass, because they'd accumulate otherwise
     w1$grad$zero_()
     w2$grad$zero_()
     b1$grad$zero_()
     b2$grad$zero_()  
   })

}
```

We still manually compute the forward pass, and we still manually update the weights. In the last two chapters of this
section, we'll see how these parts of the logic can be made more modular and reusable, as well.

## Appendix: Python code

```{python}
import torch

### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in = 3
# output dimensionality (number of predicted features)
d_out = 1
# number of observations in training set
n = 100

# create random data
x = torch.randn(n, d_in) 
y = x[ : , 0, None] * 0.2 - x[ : , 1, None] * 1.3 - x[ : , 2, None] * 0.5 + torch.randn(n, 1)


### initialize weights ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden = 32
# weights connecting input to hidden layer
w1 = torch.randn(d_in, d_hidden, requires_grad = True)
# weights connecting hidden to output layer
w2 = torch.randn(d_hidden, d_out, requires_grad = True)

# hidden layer bias
b1 = torch.zeros((1, d_hidden), requires_grad = True)
# output layer bias
b2 = torch.zeros((1, d_out), requires_grad = True)

### network parameters ---------------------------------------------------------

learning_rate = 1e-4

### training loop --------------------------------------------------------------

for t in range(200):
    
    ### -------- Forward pass -------- 

    y_pred = x.mm(w1).add(b1).clamp(min = 0).mm(w2).add(b2)

    ### -------- compute loss -------- 
    loss = (y_pred - y).pow(2).sum()
    if t % 10 == 0: print(t, loss.item())

    ### -------- Backpropagation -------- 

    # compute the gradient of loss with respect to all tensors with requires_grad = True.
    loss.backward()
 
    ### -------- Update weights -------- 
    
    # Wrap in torch.no_grad() because this is a part we DON'T want to record for automatic gradient computation
    with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad
        b1 -= learning_rate * b1.grad
        b2 -= learning_rate * b2.grad

        # Zero the gradients after every pass, because they'd accumulate otherwise
        w1.grad.zero_();
        w2.grad.zero_();
        b1.grad.zero_();
        b2.grad.zero_();
 
```

## 
