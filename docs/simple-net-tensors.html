<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Modifying the simple network to use torch tensors | Applied deep learning with torch from R</title>
  <meta name="description" content="tbd" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Modifying the simple network to use torch tensors | Applied deep learning with torch from R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="tbd" />
  <meta property="og:image" content="tbdcover.png" />
  <meta property="og:description" content="tbd" />
  <meta name="github-repo" content="tbd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Modifying the simple network to use torch tensors | Applied deep learning with torch from R" />
  
  <meta name="twitter:description" content="tbd" />
  <meta name="twitter:image" content="tbdcover.png" />

<meta name="author" content="tbd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-net-R.html"/>
<link rel="next" href="simple-net-autograd.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
</ul></li>
<li class="part"><span><b>I Using Torch</b></span></li>
<li class="chapter" data-level="" data-path="using-torch-intro.html"><a href="using-torch-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="2" data-path="simple-net-R.html"><a href="simple-net-R.html"><i class="fa fa-check"></i><b>2</b> A simple neural network in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#whats-in-a-network"><i class="fa fa-check"></i><b>2.1</b> What’s in a network?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="simple-net-R.html"><a href="simple-net-R.html#gradient-descent"><i class="fa fa-check"></i><b>2.1.1</b> Gradient descent</a></li>
<li class="chapter" data-level="2.1.2" data-path="simple-net-R.html"><a href="simple-net-R.html#from-linear-regression-to-a-simple-network"><i class="fa fa-check"></i><b>2.1.2</b> From linear regression to a simple network</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#a-simple-network"><i class="fa fa-check"></i><b>2.2</b> A simple network</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#simulate-data"><i class="fa fa-check"></i><b>2.2.1</b> Simulate data</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#initialize-weights-and-biases"><i class="fa fa-check"></i><b>2.2.2</b> Initialize weights and biases</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>2.2.3</b> Training loop</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple-net-R.html"><a href="simple-net-R.html#complete-code"><i class="fa fa-check"></i><b>2.2.4</b> Complete code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>2.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html"><i class="fa fa-check"></i><b>3</b> Modifying the simple network to use torch tensors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#simple-network-torchified-step-1"><i class="fa fa-check"></i><b>3.1</b> Simple network torchified, step 1</a></li>
<li class="chapter" data-level="3.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#more-on-tensors"><i class="fa fa-check"></i><b>3.2</b> More on tensors</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#creating-tensors"><i class="fa fa-check"></i><b>3.2.1</b> Creating tensors</a></li>
<li class="chapter" data-level="3.2.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#conversion-between-torch-tensors-and-r-values"><i class="fa fa-check"></i><b>3.2.2</b> Conversion between <code>torch</code> tensors and R values</a></li>
<li class="chapter" data-level="3.2.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#indexing-and-slicing-tensors"><i class="fa fa-check"></i><b>3.2.3</b> Indexing and slicing tensors</a></li>
<li class="chapter" data-level="3.2.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#reshaping-tensors"><i class="fa fa-check"></i><b>3.2.4</b> Reshaping tensors</a></li>
<li class="chapter" data-level="3.2.5" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#operations-on-tensors"><i class="fa fa-check"></i><b>3.2.5</b> Operations on tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#broadcasting"><i class="fa fa-check"></i><b>3.3</b> Broadcasting</a></li>
<li class="chapter" data-level="3.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#running-on-gpu"><i class="fa fa-check"></i><b>3.4</b> Running on GPU</a></li>
<li class="chapter" data-level="3.5" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>3.5</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html"><i class="fa fa-check"></i><b>4</b> Using autograd</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#automatic-differentiation-with-autograd"><i class="fa fa-check"></i><b>4.1</b> Automatic differentiation with autograd</a></li>
<li class="chapter" data-level="4.2" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#the-simple-network-now-using-autograd"><i class="fa fa-check"></i><b>4.2</b> The simple network, now using autograd</a></li>
<li class="chapter" data-level="4.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>4.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple-net-modules.html"><a href="simple-net-modules.html"><i class="fa fa-check"></i><b>5</b> Using torch modules</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#modules"><i class="fa fa-check"></i><b>5.1</b> Modules</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#layers-as-modules"><i class="fa fa-check"></i><b>5.1.1</b> Layers as modules</a></li>
<li class="chapter" data-level="5.1.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#models-as-modules"><i class="fa fa-check"></i><b>5.1.2</b> Models as modules</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#simple-network-using-modules"><i class="fa fa-check"></i><b>5.2</b> Simple network using modules</a></li>
<li class="chapter" data-level="5.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>5.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="simple-net-optim.html"><a href="simple-net-optim.html"><i class="fa fa-check"></i><b>6</b> Using torch optimizers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="simple-net-optim.html"><a href="simple-net-optim.html#torch-optimizers"><i class="fa fa-check"></i><b>6.1</b> Torch optimizers</a></li>
<li class="chapter" data-level="6.2" data-path="simple-net-optim.html"><a href="simple-net-optim.html#simple-net-with-torch.optim"><i class="fa fa-check"></i><b>6.2</b> Simple net with <code>torch.optim</code></a></li>
</ul></li>
<li class="part"><span><b>II Image Recognition</b></span></li>
<li class="chapter" data-level="" data-path="image-recognition-intro.html"><a href="image-recognition-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="7" data-path="image-classification.html"><a href="image-classification.html"><i class="fa fa-check"></i><b>7</b> Classifying images</a>
<ul>
<li class="chapter" data-level="7.1" data-path="image-classification.html"><a href="image-classification.html#data-loading-and-transformation"><i class="fa fa-check"></i><b>7.1</b> Data loading and transformation</a></li>
<li class="chapter" data-level="7.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>7.2</b> Model</a></li>
<li class="chapter" data-level="7.3" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>7.3</b> Training</a></li>
<li class="chapter" data-level="7.4" data-path="image-classification.html"><a href="image-classification.html#performance-on-the-test-set"><i class="fa fa-check"></i><b>7.4</b> Performance on the test set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="unet.html"><a href="unet.html"><i class="fa fa-check"></i><b>8</b> Brain Image Segmentation with U-Net</a>
<ul>
<li class="chapter" data-level="8.1" data-path="unet.html"><a href="unet.html#image-segmentation-in-a-nutshell"><i class="fa fa-check"></i><b>8.1</b> Image segmentation in a nutshell</a></li>
<li class="chapter" data-level="8.2" data-path="unet.html"><a href="unet.html#u-net"><i class="fa fa-check"></i><b>8.2</b> U-Net</a></li>
<li class="chapter" data-level="8.3" data-path="unet.html"><a href="unet.html#example-application-mri-images"><i class="fa fa-check"></i><b>8.3</b> Example application: MRI images</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="unet.html"><a href="unet.html#preprocessing"><i class="fa fa-check"></i><b>8.3.1</b> Preprocessing</a></li>
<li class="chapter" data-level="8.3.2" data-path="unet.html"><a href="unet.html#u-net-model"><i class="fa fa-check"></i><b>8.3.2</b> U-Net model</a></li>
<li class="chapter" data-level="8.3.3" data-path="unet.html"><a href="unet.html#loss"><i class="fa fa-check"></i><b>8.3.3</b> Loss</a></li>
<li class="chapter" data-level="8.3.4" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>8.3.4</b> Training</a></li>
<li class="chapter" data-level="8.3.5" data-path="unet.html"><a href="unet.html#predictions"><i class="fa fa-check"></i><b>8.3.5</b> Predictions</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Natural language processing</b></span></li>
<li class="chapter" data-level="" data-path="NLP-intro.html"><a href="NLP-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="9" data-path="seq2seq-att.html"><a href="seq2seq-att.html"><i class="fa fa-check"></i><b>9</b> Sequence-to-sequence models with attention</a>
<ul>
<li class="chapter" data-level="9.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#why-attention"><i class="fa fa-check"></i><b>9.1</b> Why attention?</a></li>
<li class="chapter" data-level="9.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#preprocessing-with-torchtext"><i class="fa fa-check"></i><b>9.2</b> Preprocessing with <code>torchtext</code></a></li>
<li class="chapter" data-level="9.3" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>9.3</b> Model</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>9.3.1</b> Encoder</a></li>
<li class="chapter" data-level="9.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#attention-module"><i class="fa fa-check"></i><b>9.3.2</b> Attention module</a></li>
<li class="chapter" data-level="9.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>9.3.3</b> Decoder</a></li>
<li class="chapter" data-level="9.3.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#seq2seq-module"><i class="fa fa-check"></i><b>9.3.4</b> Seq2seq module</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#training-and-evaluation"><i class="fa fa-check"></i><b>9.4</b> Training and evaluation</a></li>
<li class="chapter" data-level="9.5" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>9.5</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="transformer.html"><a href="transformer.html"><i class="fa fa-check"></i><b>10</b> Torch transformer modules</a>
<ul>
<li class="chapter" data-level="10.1" data-path="transformer.html"><a href="transformer.html#attention-is-all-you-need"><i class="fa fa-check"></i><b>10.1</b> “Attention is all you need”</a></li>
<li class="chapter" data-level="10.2" data-path="transformer.html"><a href="transformer.html#implementation-building-blocks"><i class="fa fa-check"></i><b>10.2</b> Implementation: building blocks</a></li>
<li class="chapter" data-level="10.3" data-path="transformer.html"><a href="transformer.html#a-transformer-for-natural-language-translation"><i class="fa fa-check"></i><b>10.3</b> A Transformer for natural language translation</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="transformer.html"><a href="transformer.html#load-data"><i class="fa fa-check"></i><b>10.3.1</b> Load data</a></li>
<li class="chapter" data-level="10.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>10.3.2</b> Encoder</a></li>
<li class="chapter" data-level="10.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>10.3.3</b> Decoder</a></li>
<li class="chapter" data-level="10.3.4" data-path="transformer.html"><a href="transformer.html#overall-model"><i class="fa fa-check"></i><b>10.3.4</b> Overall model</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>10.4</b> Results</a></li>
</ul></li>
<li class="part"><span><b>IV Deep learning for tabular data</b></span></li>
<li class="chapter" data-level="" data-path="tabular-intro.html"><a href="tabular-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>V Time series</b></span></li>
<li class="chapter" data-level="" data-path="timeseries-intro.html"><a href="timeseries-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VI Generative deep learning</b></span></li>
<li class="chapter" data-level="" data-path="generative-intro.html"><a href="generative-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="11" data-path="gans.html"><a href="gans.html"><i class="fa fa-check"></i><b>11</b> Generative adversarial networks</a>
<ul>
<li class="chapter" data-level="11.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>11.1</b> Dataset</a></li>
<li class="chapter" data-level="11.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>11.2</b> Model</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="gans.html"><a href="gans.html#generator"><i class="fa fa-check"></i><b>11.2.1</b> Generator</a></li>
<li class="chapter" data-level="11.2.2" data-path="gans.html"><a href="gans.html#discriminator"><i class="fa fa-check"></i><b>11.2.2</b> Discriminator</a></li>
<li class="chapter" data-level="11.2.3" data-path="gans.html"><a href="gans.html#optimizers-and-loss-function"><i class="fa fa-check"></i><b>11.2.3</b> Optimizers and loss function</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>11.3</b> Training loop</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="vaes.html"><a href="vaes.html"><i class="fa fa-check"></i><b>12</b> Variational autoencoders</a>
<ul>
<li class="chapter" data-level="12.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>12.1</b> Dataset</a></li>
<li class="chapter" data-level="12.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>12.2</b> Model</a></li>
<li class="chapter" data-level="12.3" data-path="vaes.html"><a href="vaes.html#training-the-vae"><i class="fa fa-check"></i><b>12.3</b> Training the VAE</a></li>
<li class="chapter" data-level="12.4" data-path="vaes.html"><a href="vaes.html#latent-space"><i class="fa fa-check"></i><b>12.4</b> Latent space</a></li>
</ul></li>
<li class="part"><span><b>VII Deep learning on graphs</b></span></li>
<li class="chapter" data-level="" data-path="graph-intro.html"><a href="graph-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VIII Probabilistic deep learning</b></span></li>
<li class="chapter" data-level="" data-path="probabilistic-intro.html"><a href="probabilistic-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>IX Private and secure deep learning</b></span></li>
<li class="chapter" data-level="" data-path="private-secure-intro.html"><a href="private-secure-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>X Research topics</b></span></li>
<li class="chapter" data-level="" data-path="research-intro.html"><a href="research-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied deep learning with torch from R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple_net_tensors" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Modifying the simple network to use torch tensors</h1>
<p>Our first step of “torchifying” the network is to use torch <em>tensors</em> instead of <code>rray</code>. This means:</p>
<ul>
<li>We create data structures directly as torch tensors, e.g.using <code>torch_randn()</code> instead of <code>rray(rnorm...)</code>.</li>
<li>We use tensor operations to create new tensors from existing ones, e.g. by matrix multiplication (<code>torch_mm (t1, t2)</code>),
setting negative elements to 0 <code>(torch_clamp (t, min = 0</code>) or matrix transposition (<code>torch_t(t)</code>).</li>
</ul>
<p>Apart from these changes, everything else stays the same (for now). We directly show the modified code for you to try it out,
and then, go into more detail on tensor creation, indexing, broadcasting and that essential question – how do I run this on a
GPU?</p>
<div id="simple-network-torchified-step-1" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Simple network torchified, step 1</h2>
<p>Again, if you’re just starting out, don’t worry about the details; you will never have to calculate gradients yourself this
way.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="simple-net-tensors.html#cb10-1"></a><span class="kw">library</span>(torch)</span>
<span id="cb10-2"><a href="simple-net-tensors.html#cb10-2"></a></span>
<span id="cb10-3"><a href="simple-net-tensors.html#cb10-3"></a><span class="co">### generate training data -----------------------------------------------------</span></span>
<span id="cb10-4"><a href="simple-net-tensors.html#cb10-4"></a></span>
<span id="cb10-5"><a href="simple-net-tensors.html#cb10-5"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb10-6"><a href="simple-net-tensors.html#cb10-6"></a>d_in &lt;-<span class="st"> </span><span class="dv">3</span></span>
<span id="cb10-7"><a href="simple-net-tensors.html#cb10-7"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb10-8"><a href="simple-net-tensors.html#cb10-8"></a>d_out &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb10-9"><a href="simple-net-tensors.html#cb10-9"></a><span class="co"># number of observations in training set</span></span>
<span id="cb10-10"><a href="simple-net-tensors.html#cb10-10"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb10-11"><a href="simple-net-tensors.html#cb10-11"></a></span>
<span id="cb10-12"><a href="simple-net-tensors.html#cb10-12"></a></span>
<span id="cb10-13"><a href="simple-net-tensors.html#cb10-13"></a><span class="co"># create random data</span></span>
<span id="cb10-14"><a href="simple-net-tensors.html#cb10-14"></a>x &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(n, d_in)</span>
<span id="cb10-15"><a href="simple-net-tensors.html#cb10-15"></a>y &lt;-</span>
<span id="cb10-16"><a href="simple-net-tensors.html#cb10-16"></a><span class="st">  </span>x[, <span class="dv">0</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">0.2</span> <span class="op">-</span><span class="st"> </span>x[, <span class="dv">1</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">1.3</span> <span class="op">-</span><span class="st"> </span>x[, <span class="dv">2</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">+</span><span class="st"> </span><span class="kw">torch_randn</span>(n, <span class="dv">1</span>)</span>
<span id="cb10-17"><a href="simple-net-tensors.html#cb10-17"></a></span>
<span id="cb10-18"><a href="simple-net-tensors.html#cb10-18"></a></span>
<span id="cb10-19"><a href="simple-net-tensors.html#cb10-19"></a><span class="co">### initialize weights ---------------------------------------------------------</span></span>
<span id="cb10-20"><a href="simple-net-tensors.html#cb10-20"></a></span>
<span id="cb10-21"><a href="simple-net-tensors.html#cb10-21"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb10-22"><a href="simple-net-tensors.html#cb10-22"></a>d_hidden &lt;-<span class="st"> </span><span class="dv">32</span></span>
<span id="cb10-23"><a href="simple-net-tensors.html#cb10-23"></a><span class="co"># weights connecting input to hidden layer</span></span>
<span id="cb10-24"><a href="simple-net-tensors.html#cb10-24"></a>w1 &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(d_in, d_hidden)</span>
<span id="cb10-25"><a href="simple-net-tensors.html#cb10-25"></a><span class="co"># weights connecting hidden to output layer</span></span>
<span id="cb10-26"><a href="simple-net-tensors.html#cb10-26"></a>w2 &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(d_hidden, d_out)</span>
<span id="cb10-27"><a href="simple-net-tensors.html#cb10-27"></a></span>
<span id="cb10-28"><a href="simple-net-tensors.html#cb10-28"></a><span class="co"># hidden layer bias</span></span>
<span id="cb10-29"><a href="simple-net-tensors.html#cb10-29"></a>b1 &lt;-<span class="st"> </span><span class="kw">torch_zeros</span>(<span class="dv">1</span>, d_hidden)</span>
<span id="cb10-30"><a href="simple-net-tensors.html#cb10-30"></a><span class="co"># output layer bias</span></span>
<span id="cb10-31"><a href="simple-net-tensors.html#cb10-31"></a>b2 &lt;-<span class="st"> </span><span class="kw">torch_zeros</span>(<span class="dv">1</span>, d_out)</span>
<span id="cb10-32"><a href="simple-net-tensors.html#cb10-32"></a></span>
<span id="cb10-33"><a href="simple-net-tensors.html#cb10-33"></a><span class="co">### network parameters ---------------------------------------------------------</span></span>
<span id="cb10-34"><a href="simple-net-tensors.html#cb10-34"></a></span>
<span id="cb10-35"><a href="simple-net-tensors.html#cb10-35"></a>learning_rate &lt;-<span class="st"> </span><span class="fl">1e-4</span></span>
<span id="cb10-36"><a href="simple-net-tensors.html#cb10-36"></a></span>
<span id="cb10-37"><a href="simple-net-tensors.html#cb10-37"></a><span class="co">### training loop --------------------------------------------------------------</span></span>
<span id="cb10-38"><a href="simple-net-tensors.html#cb10-38"></a></span>
<span id="cb10-39"><a href="simple-net-tensors.html#cb10-39"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">200</span>) {</span>
<span id="cb10-40"><a href="simple-net-tensors.html#cb10-40"></a>  <span class="co">### -------- Forward pass --------</span></span>
<span id="cb10-41"><a href="simple-net-tensors.html#cb10-41"></a>  </span>
<span id="cb10-42"><a href="simple-net-tensors.html#cb10-42"></a>  <span class="co"># compute pre-activations of hidden layers (dim: 100 x 32)</span></span>
<span id="cb10-43"><a href="simple-net-tensors.html#cb10-43"></a>  h &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">mm</span>(w1) <span class="op">+</span><span class="st"> </span>b1</span>
<span id="cb10-44"><a href="simple-net-tensors.html#cb10-44"></a>  <span class="co"># apply activation function (dim: 100 x 32)</span></span>
<span id="cb10-45"><a href="simple-net-tensors.html#cb10-45"></a>  h_relu &lt;-<span class="st"> </span>h<span class="op">$</span><span class="kw">clamp</span>(<span class="dt">min =</span> <span class="dv">0</span>)</span>
<span id="cb10-46"><a href="simple-net-tensors.html#cb10-46"></a>  <span class="co"># compute output (dim: 100 x 1)</span></span>
<span id="cb10-47"><a href="simple-net-tensors.html#cb10-47"></a>  y_pred &lt;-<span class="st"> </span>h_relu<span class="op">$</span><span class="kw">mm</span>(w2) <span class="op">+</span><span class="st"> </span>b2</span>
<span id="cb10-48"><a href="simple-net-tensors.html#cb10-48"></a>  </span>
<span id="cb10-49"><a href="simple-net-tensors.html#cb10-49"></a>  <span class="co">### -------- compute loss --------</span></span>
<span id="cb10-50"><a href="simple-net-tensors.html#cb10-50"></a></span>
<span id="cb10-51"><a href="simple-net-tensors.html#cb10-51"></a>  loss &lt;-<span class="st"> </span>(y_pred <span class="op">-</span><span class="st"> </span>y)<span class="op">$</span><span class="kw">pow</span>(<span class="dv">2</span>)<span class="op">$</span><span class="kw">sum</span>()<span class="op">$</span><span class="kw">item</span>()</span>
<span id="cb10-52"><a href="simple-net-tensors.html#cb10-52"></a>  </span>
<span id="cb10-53"><a href="simple-net-tensors.html#cb10-53"></a>  <span class="cf">if</span> (t <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)</span>
<span id="cb10-54"><a href="simple-net-tensors.html#cb10-54"></a>    <span class="kw">cat</span>(<span class="st">&quot;Epoch: &quot;</span>, t, <span class="st">&quot;   Loss: &quot;</span>, loss, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb10-55"><a href="simple-net-tensors.html#cb10-55"></a>  </span>
<span id="cb10-56"><a href="simple-net-tensors.html#cb10-56"></a>  <span class="co">### -------- Backpropagation --------</span></span>
<span id="cb10-57"><a href="simple-net-tensors.html#cb10-57"></a>  </span>
<span id="cb10-58"><a href="simple-net-tensors.html#cb10-58"></a>  <span class="co"># gradient of loss w.r.t. prediction (dim: 100 x 1)</span></span>
<span id="cb10-59"><a href="simple-net-tensors.html#cb10-59"></a>  grad_y_pred &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(y_pred <span class="op">-</span><span class="st"> </span>y)</span>
<span id="cb10-60"><a href="simple-net-tensors.html#cb10-60"></a>  <span class="co"># gradient of loss w.r.t. w2 (dim: 32 x 1)</span></span>
<span id="cb10-61"><a href="simple-net-tensors.html#cb10-61"></a>  grad_w2 &lt;-<span class="st"> </span>h_relu<span class="op">$</span><span class="kw">t</span>()<span class="op">$</span><span class="kw">mm</span>(grad_y_pred)</span>
<span id="cb10-62"><a href="simple-net-tensors.html#cb10-62"></a>  <span class="co"># gradient of loss w.r.t. hidden activation (dim: 100 x 32)</span></span>
<span id="cb10-63"><a href="simple-net-tensors.html#cb10-63"></a>  grad_h_relu &lt;-<span class="st"> </span>grad_y_pred<span class="op">$</span><span class="kw">mm</span>(</span>
<span id="cb10-64"><a href="simple-net-tensors.html#cb10-64"></a>    w2<span class="op">$</span><span class="kw">t</span>())</span>
<span id="cb10-65"><a href="simple-net-tensors.html#cb10-65"></a>  <span class="co"># gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)</span></span>
<span id="cb10-66"><a href="simple-net-tensors.html#cb10-66"></a>  grad_h &lt;-<span class="st"> </span>grad_h_relu<span class="op">$</span><span class="kw">clone</span>()</span>
<span id="cb10-67"><a href="simple-net-tensors.html#cb10-67"></a>  </span>
<span id="cb10-68"><a href="simple-net-tensors.html#cb10-68"></a>  <span class="co">###TBD###</span></span>
<span id="cb10-69"><a href="simple-net-tensors.html#cb10-69"></a>  <span class="co"># grad_h[h &lt; 0, ] &lt;- 0</span></span>
<span id="cb10-70"><a href="simple-net-tensors.html#cb10-70"></a>  grad_h &lt;-<span class="st"> </span>h <span class="op">*</span><span class="st"> </span>(h <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</span>
<span id="cb10-71"><a href="simple-net-tensors.html#cb10-71"></a>  </span>
<span id="cb10-72"><a href="simple-net-tensors.html#cb10-72"></a>  <span class="co"># gradient of loss w.r.t. b2 (shape: ())</span></span>
<span id="cb10-73"><a href="simple-net-tensors.html#cb10-73"></a>  grad_b2 &lt;-<span class="st"> </span>grad_y_pred<span class="op">$</span><span class="kw">sum</span>()</span>
<span id="cb10-74"><a href="simple-net-tensors.html#cb10-74"></a>  </span>
<span id="cb10-75"><a href="simple-net-tensors.html#cb10-75"></a>  <span class="co"># gradient of loss w.r.t. w1 (dim: 3 x 32)</span></span>
<span id="cb10-76"><a href="simple-net-tensors.html#cb10-76"></a>  grad_w1 &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">t</span>()<span class="op">$</span><span class="kw">mm</span>(grad_h)</span>
<span id="cb10-77"><a href="simple-net-tensors.html#cb10-77"></a>  <span class="co"># gradient of loss w.r.t. b1 (shape: (32, ))</span></span>
<span id="cb10-78"><a href="simple-net-tensors.html#cb10-78"></a>  grad_b1 &lt;-<span class="st"> </span>grad_h<span class="op">$</span><span class="kw">sum</span>(<span class="dt">dim =</span> <span class="dv">0</span>)</span>
<span id="cb10-79"><a href="simple-net-tensors.html#cb10-79"></a>  </span>
<span id="cb10-80"><a href="simple-net-tensors.html#cb10-80"></a>  <span class="co">### -------- Update weights --------</span></span>
<span id="cb10-81"><a href="simple-net-tensors.html#cb10-81"></a>  </span>
<span id="cb10-82"><a href="simple-net-tensors.html#cb10-82"></a>  w2 &lt;-<span class="st"> </span>w2 <span class="op">-</span><span class="st"> </span>learning_rate <span class="op">*</span><span class="st"> </span>grad_w2</span>
<span id="cb10-83"><a href="simple-net-tensors.html#cb10-83"></a>  b2 &lt;-<span class="st"> </span>b2 <span class="op">-</span><span class="st"> </span>learning_rate <span class="op">*</span><span class="st"> </span>grad_b2</span>
<span id="cb10-84"><a href="simple-net-tensors.html#cb10-84"></a>  w1 &lt;-<span class="st"> </span>w1 <span class="op">-</span><span class="st"> </span>learning_rate <span class="op">*</span><span class="st"> </span>grad_w1</span>
<span id="cb10-85"><a href="simple-net-tensors.html#cb10-85"></a>  b1 &lt;-<span class="st"> </span>b1 <span class="op">-</span><span class="st"> </span>learning_rate <span class="op">*</span><span class="st"> </span>grad_b1</span>
<span id="cb10-86"><a href="simple-net-tensors.html#cb10-86"></a>  </span>
<span id="cb10-87"><a href="simple-net-tensors.html#cb10-87"></a>}</span></code></pre></div>
</div>
<div id="more-on-tensors" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> More on tensors</h2>
<p>Now let’s get a bit more background on tensors. There will be more in the next chapter; for now, we stay with what is needed
to fully understand their use in the above example.</p>
<div id="creating-tensors" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Creating tensors</h3>
<p>Tensors can be created by specifying individual values. Here we create two one-dimensional tensors (vectors), of type <code>float</code>
and <code>bool</code>, resp.:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="simple-net-tensors.html#cb11-1"></a><span class="co"># a 1d vector of length 2</span></span>
<span id="cb11-2"><a href="simple-net-tensors.html#cb11-2"></a>t <span class="op">&lt;-</span> torch_tensor(c(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb11-3"><a href="simple-net-tensors.html#cb11-3"></a>t</span>
<span id="cb11-4"><a href="simple-net-tensors.html#cb11-4"></a></span>
<span id="cb11-5"><a href="simple-net-tensors.html#cb11-5"></a><span class="co"># same, but of type boolean</span></span>
<span id="cb11-6"><a href="simple-net-tensors.html#cb11-6"></a>t <span class="op">&lt;-</span> torch_tensor(c(TRUE, FALSE))</span>
<span id="cb11-7"><a href="simple-net-tensors.html#cb11-7"></a>t</span></code></pre></div>
<p>And here are two ways to create 2d tensors (matrices). Note how in the second approach, you need to specify <code>byrow = TRUE</code> to
get values arranged in row-major order.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="simple-net-tensors.html#cb12-1"></a><span class="co"># a 3 x 3 (= 2d) tensor (matrix)</span></span>
<span id="cb12-2"><a href="simple-net-tensors.html#cb12-2"></a>t &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="kw">rbind</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>), <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)))</span>
<span id="cb12-3"><a href="simple-net-tensors.html#cb12-3"></a>t</span>
<span id="cb12-4"><a href="simple-net-tensors.html#cb12-4"></a></span>
<span id="cb12-5"><a href="simple-net-tensors.html#cb12-5"></a><span class="co"># same</span></span>
<span id="cb12-6"><a href="simple-net-tensors.html#cb12-6"></a>t &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="kw">matrix</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">9</span>, <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>))</span>
<span id="cb12-7"><a href="simple-net-tensors.html#cb12-7"></a>t</span></code></pre></div>
<p>In higher dimensions, especially, it can be easier to specify the type of tensor abstractly, as in: "Give me a tensor of
<span class="math display">\[fill in characteristic here\]</span> of shape n1 x n2":</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="simple-net-tensors.html#cb13-1"></a><span class="co"># a 3 x 3 tensor of standard normally distributed values</span></span>
<span id="cb13-2"><a href="simple-net-tensors.html#cb13-2"></a>t &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb13-3"><a href="simple-net-tensors.html#cb13-3"></a>t</span>
<span id="cb13-4"><a href="simple-net-tensors.html#cb13-4"></a></span>
<span id="cb13-5"><a href="simple-net-tensors.html#cb13-5"></a><span class="co"># a 4 x 2 x 2 (3d) tensor of zeroes</span></span>
<span id="cb13-6"><a href="simple-net-tensors.html#cb13-6"></a>t &lt;-<span class="st"> </span><span class="kw">torch_zeros</span>(<span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb13-7"><a href="simple-net-tensors.html#cb13-7"></a>t</span></code></pre></div>
<p>Many similar function exist, including <code>torch_arange</code> to create a tensor holding a sequence of values, <code>torch_eye</code> that
returns an identity matrix, and <code>torch_logspace</code> that will fill a specified range with a list of logarithmically spaced
values.</p>
<p>If no <code>dtype</code> is specified, <code>torch</code> will infer it from the data. For example:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="simple-net-tensors.html#cb14-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>))</span>
<span id="cb14-2"><a href="simple-net-tensors.html#cb14-2"></a>t<span class="op">$</span><span class="kw">dtype</span>()</span>
<span id="cb14-3"><a href="simple-net-tensors.html#cb14-3"></a></span>
<span id="cb14-4"><a href="simple-net-tensors.html#cb14-4"></a>t &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(1L)</span>
<span id="cb14-5"><a href="simple-net-tensors.html#cb14-5"></a>t<span class="op">$</span><span class="kw">dtype</span>()</span></code></pre></div>
<p>But we can explicitly pass a different dtype if we want:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="simple-net-tensors.html#cb15-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="dv">2</span>, <span class="dt">dtype =</span> <span class="kw">torch_double</span>())</span>
<span id="cb15-2"><a href="simple-net-tensors.html#cb15-2"></a>t<span class="op">$</span><span class="kw">dtype</span>()</span></code></pre></div>
<p>Torch tensors live on a <em>device</em>. By default, this will be the CPU:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="simple-net-tensors.html#cb16-1"></a>t<span class="op">$</span><span class="kw">device</span>()</span></code></pre></div>
<p>But we could also define a tensor to live on, for example, the GPU:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="simple-net-tensors.html#cb17-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="dv">2</span>, <span class="dt">device =</span> <span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb17-2"><a href="simple-net-tensors.html#cb17-2"></a>t<span class="op">$</span><span class="kw">device</span>()</span></code></pre></div>
<p>We’ll talk more about devices below.</p>
<p>There is another very important parameter to the tensor creation functions: <code>requires_grad</code>. This one we’ll discuss in the
next chapter on <em>autograd</em>.</p>
</div>
<div id="conversion-between-torch-tensors-and-r-values" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Conversion between <code>torch</code> tensors and R values</h3>
<p>To convert between <code>torch</code> tensors and R vectors/matrices/arrays, we can use <code>as_numeric</code>, <code>as_integer</code>, <code>as_array</code> and their
likes. If a tensor is on GPU, we need to move it to CPU first:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="simple-net-tensors.html#cb18-1"></a><span class="kw">as_array</span>(t<span class="op">$</span><span class="kw">cpu</span>())</span></code></pre></div>
<p>###TBD###</p>
<p>shared storage</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; t = torch.zeros(2,2)
&gt;&gt;&gt; t
tensor([[0., 0.],
        [0., 0.]])
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; n = t.numpy()
&gt;&gt;&gt; n
array([[0., 0.],
       [0., 0.]], dtype=float32)
&gt;&gt;&gt; n[0,0] = 777
&gt;&gt;&gt; n
array([[777.,   0.],
       [  0.,   0.]], dtype=float32)
&gt;&gt;&gt; t
tensor([[777.,   0.],
        [  0.,   0.]])</code></pre>
</div>
<div id="indexing-and-slicing-tensors" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Indexing and slicing tensors</h3>
<p>Indexing and slicing tensors is 1-based and works, to a high degree, like you’d expect from R. For easy retrieval, we split
this section into two subsections: things that work as expected, from the point of view of an R user, and things you might not
be expecting a priori, but that are inspired by powerful indexing functionality in Python’s NumPy.</p>
<div id="r-like-behavior" class="section level4" number="3.2.3.1">
<h4><span class="header-section-number">3.2.3.1</span> R-like behavior</h4>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="simple-net-tensors.html#cb20-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="kw">rbind</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)))</span>
<span id="cb20-2"><a href="simple-net-tensors.html#cb20-2"></a>t</span>
<span id="cb20-3"><a href="simple-net-tensors.html#cb20-3"></a></span>
<span id="cb20-4"><a href="simple-net-tensors.html#cb20-4"></a><span class="co"># a single value</span></span>
<span id="cb20-5"><a href="simple-net-tensors.html#cb20-5"></a>t[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb20-6"><a href="simple-net-tensors.html#cb20-6"></a></span>
<span id="cb20-7"><a href="simple-net-tensors.html#cb20-7"></a><span class="co"># one row, all columns</span></span>
<span id="cb20-8"><a href="simple-net-tensors.html#cb20-8"></a>t[<span class="dv">1</span>, ]</span>
<span id="cb20-9"><a href="simple-net-tensors.html#cb20-9"></a></span>
<span id="cb20-10"><a href="simple-net-tensors.html#cb20-10"></a><span class="co"># one row, a subset of columns</span></span>
<span id="cb20-11"><a href="simple-net-tensors.html#cb20-11"></a>t[<span class="dv">1</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]</span></code></pre></div>
<p>Note how just like in R, singleton dimensions are dropped:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="simple-net-tensors.html#cb21-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="kw">rbind</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)))</span>
<span id="cb21-2"><a href="simple-net-tensors.html#cb21-2"></a></span>
<span id="cb21-3"><a href="simple-net-tensors.html#cb21-3"></a>t<span class="op">$</span><span class="kw">size</span>()</span>
<span id="cb21-4"><a href="simple-net-tensors.html#cb21-4"></a>t[<span class="dv">1</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]<span class="op">$</span><span class="kw">size</span>()</span>
<span id="cb21-5"><a href="simple-net-tensors.html#cb21-5"></a>t[<span class="dv">1</span>, <span class="dv">1</span>]<span class="op">$</span><span class="kw">size</span>()</span></code></pre></div>
<p>And just like in R, you can specify <code>drop = FALSE</code> to keep those dimensions:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="simple-net-tensors.html#cb22-1"></a>t[<span class="dv">1</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, drop =<span class="st"> </span><span class="ot">FALSE</span>]<span class="op">$</span><span class="kw">size</span>()</span>
<span id="cb22-2"><a href="simple-net-tensors.html#cb22-2"></a>t[<span class="dv">1</span>, <span class="dv">1</span>, drop =<span class="st"> </span><span class="ot">FALSE</span>]<span class="op">$</span><span class="kw">size</span>()</span></code></pre></div>
</div>
<div id="functionality-beyond-what-youd-expect-from-r" class="section level4" number="3.2.3.2">
<h4><span class="header-section-number">3.2.3.2</span> Functionality beyond what you’d expect from R</h4>
<p>Whereas R uses negative numbers to remove elements at specified positions, in <code>torch</code> negative values indicate we start
counting from the end of a tensor – with <code>-1</code> pointing to its last element:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="simple-net-tensors.html#cb23-1"></a>t[<span class="dv">1</span>, <span class="dv">-1</span>]</span>
<span id="cb23-2"><a href="simple-net-tensors.html#cb23-2"></a>t[ , <span class="dv">-2</span><span class="op">:-</span><span class="dv">1</span>] </span></code></pre></div>
<p>This is a feature you might know from NumPy. Same with the following: When the slicing expression <code>n1:n2</code> is augmented by
another semicolon and a third number – <code>n1:n2:n3</code> – we will take, in the range specified by <code>n1</code> and <code>n2</code>, every <code>n3</code>rd
item:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="simple-net-tensors.html#cb24-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)</span>
<span id="cb24-2"><a href="simple-net-tensors.html#cb24-2"></a>t[<span class="dv">2</span><span class="op">:</span><span class="dv">10</span><span class="op">:</span><span class="dv">2</span>]</span></code></pre></div>
<p>Sometimes we don’t know how many dimensions a tensor has, but we do know what to do with the last available dimension, or the
first one. To subsume all others, we can use <code>..</code>:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="simple-net-tensors.html#cb25-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_randint</span>(<span class="op">-</span><span class="dv">7</span>, <span class="dv">7</span>, <span class="dt">size =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb25-2"><a href="simple-net-tensors.html#cb25-2"></a>t</span>
<span id="cb25-3"><a href="simple-net-tensors.html#cb25-3"></a>t[.., <span class="dv">1</span>]</span>
<span id="cb25-4"><a href="simple-net-tensors.html#cb25-4"></a>t[<span class="dv">2</span>, ..]</span></code></pre></div>
</div>
</div>
<div id="reshaping-tensors" class="section level3" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Reshaping tensors</h3>
<p>The need to reshape tensors occurs quite often in practice. For performance reasons, it is important to know whether a
reshaping operation does or does not copy data.</p>
<div id="zero-copy-reshaping" class="section level4" number="3.2.4.1">
<h4><span class="header-section-number">3.2.4.1</span> Zero-copy reshaping</h4>
<p>Zero-copy methods are used whenever possible. A special case, often seen in practice, is adding or removing a singleton
dimension. <code>torch::unsqueeze()</code> adds a dimension of size <code>1</code> at a position specified by <code>dim</code>:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="simple-net-tensors.html#cb26-1"></a><span class="co">###TBD### change dim numbering</span></span>
<span id="cb26-2"><a href="simple-net-tensors.html#cb26-2"></a>t1 &lt;-<span class="st"> </span><span class="kw">torch_randint</span>(<span class="dt">low =</span> <span class="dv">3</span>, <span class="dt">high =</span> <span class="dv">7</span>, <span class="dt">size =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb26-3"><a href="simple-net-tensors.html#cb26-3"></a>t1</span>
<span id="cb26-4"><a href="simple-net-tensors.html#cb26-4"></a></span>
<span id="cb26-5"><a href="simple-net-tensors.html#cb26-5"></a>t2 &lt;-<span class="st"> </span>t1<span class="op">$</span><span class="kw">unsqueeze</span>(<span class="dt">dim =</span> <span class="dv">0</span>)</span>
<span id="cb26-6"><a href="simple-net-tensors.html#cb26-6"></a>t2<span class="op">$</span><span class="kw">size</span>()</span>
<span id="cb26-7"><a href="simple-net-tensors.html#cb26-7"></a>t2</span>
<span id="cb26-8"><a href="simple-net-tensors.html#cb26-8"></a></span>
<span id="cb26-9"><a href="simple-net-tensors.html#cb26-9"></a>t3 =<span class="st"> </span>t1<span class="op">$</span><span class="kw">unsqueeze</span>(<span class="dt">dim =</span> <span class="dv">1</span>)</span>
<span id="cb26-10"><a href="simple-net-tensors.html#cb26-10"></a>t3<span class="op">$</span><span class="kw">size</span>()</span>
<span id="cb26-11"><a href="simple-net-tensors.html#cb26-11"></a>t3</span></code></pre></div>
<p>Conversely, <code>torch::squeeze()</code> removes singleton dimensions:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="simple-net-tensors.html#cb27-1"></a>t4 &lt;-<span class="st"> </span>t3<span class="op">$</span><span class="kw">squeeze</span>()</span>
<span id="cb27-2"><a href="simple-net-tensors.html#cb27-2"></a>t4<span class="op">$</span><span class="kw">size</span>()</span>
<span id="cb27-3"><a href="simple-net-tensors.html#cb27-3"></a>t4</span></code></pre></div>
<p>The same could be accomplished with <code>torch::view()</code>, but this method is more general, in that it allows you to reshape the
data to any dimension, provided it matches the number of elements in the tensor. Here we have a <code>3x2</code> tensor that is reshaped
to size <code>2x3</code>:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="simple-net-tensors.html#cb28-1"></a>t1 &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="kw">rbind</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">6</span>)))</span>
<span id="cb28-2"><a href="simple-net-tensors.html#cb28-2"></a>t1</span>
<span id="cb28-3"><a href="simple-net-tensors.html#cb28-3"></a></span>
<span id="cb28-4"><a href="simple-net-tensors.html#cb28-4"></a>t2 &lt;-<span class="st"> </span>t1<span class="op">$</span><span class="kw">view</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb28-5"><a href="simple-net-tensors.html#cb28-5"></a>t2</span></code></pre></div>
<p>There is no need to keep overall dimensionality. Here we add a third dimension:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="simple-net-tensors.html#cb29-1"></a>t3 &lt;-<span class="st"> </span>t1<span class="op">$</span><span class="kw">view</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>))</span>
<span id="cb29-2"><a href="simple-net-tensors.html#cb29-2"></a>t3<span class="op">$</span><span class="kw">size</span>()</span>
<span id="cb29-3"><a href="simple-net-tensors.html#cb29-3"></a>t3</span></code></pre></div>
<p>Note how this prints differently from a three-dimensional array in R. R will normally print 2d matrices for every position in
dimension 3; <code>torch</code> – like Python – does this for every position in the first dimension.</p>
<p>Instead of going from two to three dimensions, we can flatten the matrix to a vector.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="simple-net-tensors.html#cb30-1"></a>t4 &lt;-<span class="st"> </span>t1<span class="op">$</span><span class="kw">view</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>))</span>
<span id="cb30-2"><a href="simple-net-tensors.html#cb30-2"></a>t4<span class="op">$</span><span class="kw">size</span>()</span>
<span id="cb30-3"><a href="simple-net-tensors.html#cb30-3"></a>t4</span></code></pre></div>
<p>In contrast to indexing operations, this does not drop dimensions.</p>
<p>Like we said above, operations like <code>squeeze</code> or <code>view()</code><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> does not make copies, or put differently: The output tensor
shares storage with the input tensor. To verify:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="simple-net-tensors.html#cb31-1"></a>t1<span class="op">$</span><span class="kw">storage</span>()<span class="op">$</span><span class="kw">data_ptr</span>()</span>
<span id="cb31-2"><a href="simple-net-tensors.html#cb31-2"></a>t2<span class="op">$</span><span class="kw">storage</span>()<span class="op">$</span><span class="kw">data_ptr</span>()</span></code></pre></div>
<p>What differs is the storage <em>metadata</em> <code>torch</code> keeps about both tensors. A tensor’s <code>stride</code> method tracks, for every
dimension, how many elements have to be traversed to arrive at its next unit (row or column, in two dimensions). For <code>t1</code>
above, of shape 3x2, we have to skip over 2 elements to arrive at the next row. To arrive at the next column though, in every
row we just have to skip a single entry:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="simple-net-tensors.html#cb32-1"></a>t1<span class="op">$</span><span class="kw">stride</span>()</span></code></pre></div>
<p>For <code>t2</code>, of shape 3x2, the distance between column elements is the same, but the distance between rows is now 3:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="simple-net-tensors.html#cb33-1"></a>t2<span class="op">$</span><span class="kw">stride</span>()</span></code></pre></div>
<p>There are cases where <code>view()</code> will not work. This can happen when a tensor was obtained via an operation – other than
<code>view()</code> itself – that only changes <em>strides</em>.</p>
<p>One example is <code>transpose()</code>:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="simple-net-tensors.html#cb34-1"></a>t1 <span class="op">&lt;-</span> torch_tensor(rbind(c(<span class="dv">1</span>, <span class="dv">2</span>), c(<span class="dv">3</span>, <span class="dv">4</span>), c(<span class="dv">5</span>, <span class="dv">6</span>)))</span>
<span id="cb34-2"><a href="simple-net-tensors.html#cb34-2"></a>t2 <span class="op">&lt;-</span> t1$t()</span>
<span id="cb34-3"><a href="simple-net-tensors.html#cb34-3"></a>t2$stride()</span></code></pre></div>
<p>Note how <code>t2</code> is “the same” as <code>t1</code> (storage-wise), but has <code>strides</code> indicating that the physical data should be read
differently.</p>
<p>These tensors, in torch terminology, are not “contiguous”.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> One way to reshape them is to use <code>contiguous()</code> on them
before. This will copy the data to make them contiguous – so even though the next code listing still uses <code>view</code>, a copy is
taking place.</p>
</div>
<div id="reshape-with-copy" class="section level4" number="3.2.4.2">
<h4><span class="header-section-number">3.2.4.2</span> Reshape with copy</h4>
<p>In the following snippet, the first try of reshaping by means of <code>view()</code> fails, because the tensor we’re trying to reshape
already carries metadata signifying that the underlying data should not be read in physical order. The call to <code>contiguous</code>
creates a new tensor, which we can then <span class="math display">\[virtually\]</span> reshape using <code>view()</code>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="simple-net-tensors.html#cb35-1"></a>t1 &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="kw">rbind</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">6</span>)))</span>
<span id="cb35-2"><a href="simple-net-tensors.html#cb35-2"></a></span>
<span id="cb35-3"><a href="simple-net-tensors.html#cb35-3"></a>t2 &lt;-<span class="st"> </span>t1<span class="op">$</span><span class="kw">t</span>()</span>
<span id="cb35-4"><a href="simple-net-tensors.html#cb35-4"></a>t2<span class="op">$</span><span class="kw">is_contiguous</span>()</span>
<span id="cb35-5"><a href="simple-net-tensors.html#cb35-5"></a>t2<span class="op">$</span><span class="kw">view</span>(<span class="dv">6</span>)</span>
<span id="cb35-6"><a href="simple-net-tensors.html#cb35-6"></a></span>
<span id="cb35-7"><a href="simple-net-tensors.html#cb35-7"></a>t3 &lt;-<span class="st"> </span>t2<span class="op">$</span><span class="kw">contiguous</span>()</span>
<span id="cb35-8"><a href="simple-net-tensors.html#cb35-8"></a>t3</span>
<span id="cb35-9"><a href="simple-net-tensors.html#cb35-9"></a>t3<span class="op">$</span><span class="kw">view</span>(<span class="dv">6</span>)</span></code></pre></div>
<p>Alternatively, we can use <code>reshape()</code>. <code>reshape()</code> defaults to <code>view()</code>-like behavior if possible; otherwise it will create a
physical copy.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="simple-net-tensors.html#cb36-1"></a>t2<span class="op">$</span><span class="kw">storage</span>()<span class="op">$</span><span class="kw">data_ptr</span>()</span>
<span id="cb36-2"><a href="simple-net-tensors.html#cb36-2"></a></span>
<span id="cb36-3"><a href="simple-net-tensors.html#cb36-3"></a>t4 &lt;-<span class="st"> </span>t2<span class="op">$</span><span class="kw">reshape</span>(<span class="dv">6</span>)</span>
<span id="cb36-4"><a href="simple-net-tensors.html#cb36-4"></a>t4</span>
<span id="cb36-5"><a href="simple-net-tensors.html#cb36-5"></a>t4<span class="op">$</span><span class="kw">storage</span>()<span class="op">$</span><span class="kw">data_ptr</span>()</span></code></pre></div>
</div>
</div>
<div id="operations-on-tensors" class="section level3" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Operations on tensors</h3>
<p>Tensor methods normally return references to new objects. Here, <code>t1</code> is not modified:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="simple-net-tensors.html#cb37-1"></a>t1 &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="kw">rbind</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">6</span>)))</span>
<span id="cb37-2"><a href="simple-net-tensors.html#cb37-2"></a>t2 &lt;-<span class="st"> </span>t1<span class="op">$</span><span class="kw">clone</span>()</span>
<span id="cb37-3"><a href="simple-net-tensors.html#cb37-3"></a></span>
<span id="cb37-4"><a href="simple-net-tensors.html#cb37-4"></a>t1<span class="op">$</span><span class="kw">add</span>(t2)</span>
<span id="cb37-5"><a href="simple-net-tensors.html#cb37-5"></a>t1</span></code></pre></div>
<p>Often, there are variants for mutating operations; these are discernible by a trailing underscore:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="simple-net-tensors.html#cb38-1"></a>t1<span class="op">$</span><span class="kw">add_</span>(t2)</span>
<span id="cb38-2"><a href="simple-net-tensors.html#cb38-2"></a>t1</span></code></pre></div>
<p>Alternatively, you can of course assign the new object to a new reference variable:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="simple-net-tensors.html#cb39-1"></a>t3 &lt;-<span class="st"> </span>t1<span class="op">$</span><span class="kw">add</span>(t1)</span>
<span id="cb39-2"><a href="simple-net-tensors.html#cb39-2"></a>t3</span></code></pre></div>
<p>Torch provides a bunch of mathematical operations on tensors; we’ll encounter some of them in the rest of this book.</p>
</div>
</div>
<div id="broadcasting" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Broadcasting</h2>
<p>We often have to perform operations on tensors with shapes that don’t match exactly.</p>
<p>Unsurprisingly, we can add a scalar to a tensor:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="simple-net-tensors.html#cb40-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb40-2"><a href="simple-net-tensors.html#cb40-2"></a>t <span class="op">+</span><span class="st"> </span><span class="dv">22</span></span></code></pre></div>
<p>The same will work if we add tensor of size 1:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="simple-net-tensors.html#cb41-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch.randn</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb41-2"><a href="simple-net-tensors.html#cb41-2"></a>t <span class="op">+</span><span class="st"> </span><span class="kw">torch_tensor</span>(<span class="kw">c</span>(<span class="dv">22</span>))</span></code></pre></div>
<p>Adding tensors of different sizes normally won’t work:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="simple-net-tensors.html#cb42-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb42-2"><a href="simple-net-tensors.html#cb42-2"></a>t2 &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb42-3"><a href="simple-net-tensors.html#cb42-3"></a>t1<span class="op">$</span><span class="kw">add</span>(t2)</span></code></pre></div>
<p>However, under certain conditions, one or both tensors may be virtually expanded so both tensors line up. This behavior is
called <em>broadcasting</em>, and is identical to that of Python’s NumPy.</p>
<p>The rules are:</p>
<ol style="list-style-type: decimal">
<li><p>We align array shapes, <em>starting from the right</em>.</p>
<p>Say we have two tensors, one of size <code>8 x 1 x 6 x 1</code>, the other of size <code>`7 x 1 x 5`</code>.</p>
<p>Here they are, right-aligned:</p></li>
</ol>
<!-- -->
<pre><code>   # t1, shape:     8  1  6  1
   # t2, shape:        7  1  5</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><p>Starting to look from the right, the sizes along aligned axes either have to match exactly, or one of them has to be 1: In
which case the latter is <em>broadcast</em> to the one not equal to 1.</p>
<p>In the above example, this is the case for the second-from-last dimension. This now gives</p></li>
</ol>
<!-- -->
<pre><code>   # t1, shape:     8  1  6  1
   # t2, shape:        7  6  5</code></pre>
<p>, with broadcasting happening in <code>t2</code>.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>If on the left, one of the arrays has an additional axis (or more than one), the other is virtually expanded to have a 1
in that place, in which case broadcasting will happen as stated in (2).</p>
<p>This is the case with <code>t1</code>’s leftmost dimension. First, there’s a virtual expansion</p></li>
</ol>
<!-- -->
<pre><code>   # t1, shape:     8  1  6  1
   # t2, shape:     1  7  1  5</code></pre>
<p>and then, broadcasting happens:</p>
<pre><code>   # t1, shape:     8  1  6  1
   # t2, shape:     8  7  1  5</code></pre>
<p>According to these rules, our above example</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="simple-net-tensors.html#cb47-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb47-2"><a href="simple-net-tensors.html#cb47-2"></a>t2 &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb47-3"><a href="simple-net-tensors.html#cb47-3"></a>t<span class="op">$</span><span class="kw">add</span>(t2)</span></code></pre></div>
<p>could be modified in various ways that would allow for adding two tensors:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="simple-net-tensors.html#cb48-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb48-2"><a href="simple-net-tensors.html#cb48-2"></a>t2 &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>))</span>
<span id="cb48-3"><a href="simple-net-tensors.html#cb48-3"></a>t<span class="op">$</span><span class="kw">add</span>(t2)</span></code></pre></div>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="simple-net-tensors.html#cb49-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb49-2"><a href="simple-net-tensors.html#cb49-2"></a>t2 &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">5</span>))</span>
<span id="cb49-3"><a href="simple-net-tensors.html#cb49-3"></a>t<span class="op">$</span><span class="kw">add</span>(t2)</span></code></pre></div>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="simple-net-tensors.html#cb50-1"></a>t &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>))</span>
<span id="cb50-2"><a href="simple-net-tensors.html#cb50-2"></a>t2 &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb50-3"><a href="simple-net-tensors.html#cb50-3"></a>t<span class="op">$</span><span class="kw">add</span>(t2)</span></code></pre></div>
<p>As a nice final example, through broadcasting, an outer product can be computed like so:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="simple-net-tensors.html#cb51-1"></a>t1 &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>))</span>
<span id="cb51-2"><a href="simple-net-tensors.html#cb51-2"></a>t1<span class="op">$</span><span class="kw">size</span>()</span>
<span id="cb51-3"><a href="simple-net-tensors.html#cb51-3"></a></span>
<span id="cb51-4"><a href="simple-net-tensors.html#cb51-4"></a>t2 &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb51-5"><a href="simple-net-tensors.html#cb51-5"></a>t2<span class="op">$</span><span class="kw">size</span>()</span>
<span id="cb51-6"><a href="simple-net-tensors.html#cb51-6"></a></span>
<span id="cb51-7"><a href="simple-net-tensors.html#cb51-7"></a>t1<span class="op">$</span><span class="kw">view</span>(<span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">1</span>)) <span class="op">*</span><span class="st"> </span>t2</span></code></pre></div>
</div>
<div id="running-on-gpu" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Running on GPU</h2>
<p>To check if your GPU(s) is/are visible to torch, run</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="simple-net-tensors.html#cb52-1"></a><span class="kw">cuda_is_available</span>()</span>
<span id="cb52-2"><a href="simple-net-tensors.html#cb52-2"></a><span class="kw">cuda_device_count</span>()</span></code></pre></div>
<p>Above, we already saw that tensors can be created <em>on</em> a device, like so</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="simple-net-tensors.html#cb53-1"></a>device &lt;-<span class="st"> </span><span class="kw">torch_device</span>(<span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb53-2"><a href="simple-net-tensors.html#cb53-2"></a>t &lt;-<span class="st"> </span><span class="kw">torch_ones</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">device =</span> device) </span></code></pre></div>
<p>Tensors previously created on the CPU can be moved to GPU, and vice versa:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="simple-net-tensors.html#cb54-1"></a>t2 &lt;-<span class="st"> </span>t<span class="op">$</span><span class="kw">cuda</span>()</span>
<span id="cb54-2"><a href="simple-net-tensors.html#cb54-2"></a>t2</span></code></pre></div>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="simple-net-tensors.html#cb55-1"></a>t3 =<span class="st"> </span>t2<span class="op">$</span><span class="kw">cpu</span>()</span>
<span id="cb55-2"><a href="simple-net-tensors.html#cb55-2"></a>t3</span></code></pre></div>
<p>Naturally, we don’t have to specify a device for every single tensor. To perform a whole computation on the GPU, put
everything in a <em>with</em> block. This will use the first GPU, as determined by <em>cuda</em>:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="simple-net-tensors.html#cb56-1"></a><span class="co">###TBD###</span></span>
<span id="cb56-2"><a href="simple-net-tensors.html#cb56-2"></a>with <span class="kw">torch.cuda.device</span>(<span class="dv">0</span>)<span class="op">:</span></span>
<span id="cb56-3"><a href="simple-net-tensors.html#cb56-3"></a><span class="st">    </span>t1 =<span class="st"> </span><span class="kw">torch.ones</span>((<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb56-4"><a href="simple-net-tensors.html#cb56-4"></a>    t2 =<span class="st"> </span><span class="kw">torch.randn</span>((<span class="dv">2</span>,<span class="dv">4</span>))</span>
<span id="cb56-5"><a href="simple-net-tensors.html#cb56-5"></a>    <span class="kw">t1.mm</span>(t2)</span></code></pre></div>
<p>Still less granular, we can set a default device to use for all tensors created:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="simple-net-tensors.html#cb57-1"></a><span class="co">###TBD###</span></span>
<span id="cb57-2"><a href="simple-net-tensors.html#cb57-2"></a><span class="kw">torch.set_default_tensor_type</span>(<span class="st">&quot;torch.cuda.FloatTensor&quot;</span>)</span>
<span id="cb57-3"><a href="simple-net-tensors.html#cb57-3"></a>t =<span class="st"> </span><span class="kw">torch.tensor</span>(<span class="dv">25</span>)</span>
<span id="cb57-4"><a href="simple-net-tensors.html#cb57-4"></a>t</span></code></pre></div>
<p>Torch offers detailed information on GPU memory usage, e.g.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="simple-net-tensors.html#cb58-1"></a><span class="co">###TBD###</span></span>
<span id="cb58-2"><a href="simple-net-tensors.html#cb58-2"></a>torch.cuda.memory_summary()</span></code></pre></div>
<p>To quickly determine memory allocated, run</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="simple-net-tensors.html#cb59-1"></a><span class="co">###TBD###</span></span>
<span id="cb59-2"><a href="simple-net-tensors.html#cb59-2"></a>torch.cuda.memory_allocated()</span></code></pre></div>
<p>This is memory actually occupied by tensors having valid references. This is not always identical to memory claimed by torch,
as torch caches memory. To see the amount reserved by the caching allocator, see</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="simple-net-tensors.html#cb60-1"></a><span class="co">###TBD###</span></span>
<span id="cb60-2"><a href="simple-net-tensors.html#cb60-2"></a>torch.cuda.memory_reserved()</span></code></pre></div>
<p>To actually restore unused memory to Cuda, issue</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="simple-net-tensors.html#cb61-1"></a><span class="co">###TBD###</span></span>
<span id="cb61-2"><a href="simple-net-tensors.html#cb61-2"></a>torch.cuda.empty_cache()</span></code></pre></div>
<p>Here is a quick experiment illustrating the process, also referring memory usage as indicated by <code>nvidia-smi</code>. This was run in
a fresh session.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="simple-net-tensors.html#cb62-1"></a><span class="co">###TBD###</span></span>
<span id="cb62-2"><a href="simple-net-tensors.html#cb62-2"></a></span>
<span id="cb62-3"><a href="simple-net-tensors.html#cb62-3"></a><span class="co"># nvidia-smi before: 559 MiB</span></span>
<span id="cb62-4"><a href="simple-net-tensors.html#cb62-4"></a></span>
<span id="cb62-5"><a href="simple-net-tensors.html#cb62-5"></a>torch.set_default_tensor_type(torch.cuda.FloatTensor)</span>
<span id="cb62-6"><a href="simple-net-tensors.html#cb62-6"></a>t <span class="op">=</span> torch.randn((<span class="dv">1000</span>, <span class="dv">1000</span>, <span class="dv">500</span>))</span>
<span id="cb62-7"><a href="simple-net-tensors.html#cb62-7"></a>torch.cuda.memory_allocated()</span>
<span id="cb62-8"><a href="simple-net-tensors.html#cb62-8"></a><span class="op">&gt;</span> <span class="dv">2000683008</span></span>
<span id="cb62-9"><a href="simple-net-tensors.html#cb62-9"></a>torch.cuda.memory_reserved()</span>
<span id="cb62-10"><a href="simple-net-tensors.html#cb62-10"></a><span class="op">&gt;</span> <span class="dv">2000683008</span></span>
<span id="cb62-11"><a href="simple-net-tensors.html#cb62-11"></a><span class="co"># nvidia-smi with tensor allocated: 2891 MiB</span></span>
<span id="cb62-12"><a href="simple-net-tensors.html#cb62-12"></a></span>
<span id="cb62-13"><a href="simple-net-tensors.html#cb62-13"></a><span class="kw">del</span> t</span>
<span id="cb62-14"><a href="simple-net-tensors.html#cb62-14"></a>torch.cuda.memory_allocated()</span>
<span id="cb62-15"><a href="simple-net-tensors.html#cb62-15"></a><span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb62-16"><a href="simple-net-tensors.html#cb62-16"></a>torch.cuda.memory_reserved()</span>
<span id="cb62-17"><a href="simple-net-tensors.html#cb62-17"></a><span class="op">&gt;</span> <span class="dv">2000683008</span></span>
<span id="cb62-18"><a href="simple-net-tensors.html#cb62-18"></a><span class="co"># nvidia-smi after deleting reference: 2891 MiB</span></span>
<span id="cb62-19"><a href="simple-net-tensors.html#cb62-19"></a></span>
<span id="cb62-20"><a href="simple-net-tensors.html#cb62-20"></a>torch.cuda.empty_cache()</span>
<span id="cb62-21"><a href="simple-net-tensors.html#cb62-21"></a>torch.cuda.memory_reserved()</span>
<span id="cb62-22"><a href="simple-net-tensors.html#cb62-22"></a><span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb62-23"><a href="simple-net-tensors.html#cb62-23"></a><span class="co"># nvidia-smi after emptying cache: 982 MiB</span></span></code></pre></div>
<p>That’s it for a first introduction to torch tensors. In the next chapter, we continue our torchification of the simple
network.</p>
</div>
<div id="appendix-python-code" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Appendix: Python code</h2>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="simple-net-tensors.html#cb63-1"></a><span class="im">import</span> torch</span>
<span id="cb63-2"><a href="simple-net-tensors.html#cb63-2"></a></span>
<span id="cb63-3"><a href="simple-net-tensors.html#cb63-3"></a><span class="co">### generate training data -----------------------------------------------------</span></span>
<span id="cb63-4"><a href="simple-net-tensors.html#cb63-4"></a></span>
<span id="cb63-5"><a href="simple-net-tensors.html#cb63-5"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb63-6"><a href="simple-net-tensors.html#cb63-6"></a>d_in <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb63-7"><a href="simple-net-tensors.html#cb63-7"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb63-8"><a href="simple-net-tensors.html#cb63-8"></a>d_out <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb63-9"><a href="simple-net-tensors.html#cb63-9"></a><span class="co"># number of observations in training set</span></span>
<span id="cb63-10"><a href="simple-net-tensors.html#cb63-10"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb63-11"><a href="simple-net-tensors.html#cb63-11"></a></span>
<span id="cb63-12"><a href="simple-net-tensors.html#cb63-12"></a><span class="co"># create random data</span></span>
<span id="cb63-13"><a href="simple-net-tensors.html#cb63-13"></a>x <span class="op">=</span> torch.randn(n, d_in) </span>
<span id="cb63-14"><a href="simple-net-tensors.html#cb63-14"></a>y <span class="op">=</span> x[ : , <span class="dv">0</span>, <span class="va">None</span>] <span class="op">*</span> <span class="fl">0.2</span> <span class="op">-</span> x[ : , <span class="dv">1</span>, <span class="va">None</span>] <span class="op">*</span> <span class="fl">1.3</span> <span class="op">-</span> x[ : , <span class="dv">2</span>, <span class="va">None</span>] <span class="op">*</span> <span class="fl">0.5</span> <span class="op">+</span> torch.randn(n, <span class="dv">1</span>)</span>
<span id="cb63-15"><a href="simple-net-tensors.html#cb63-15"></a></span>
<span id="cb63-16"><a href="simple-net-tensors.html#cb63-16"></a></span>
<span id="cb63-17"><a href="simple-net-tensors.html#cb63-17"></a><span class="co">### initialize weights ---------------------------------------------------------</span></span>
<span id="cb63-18"><a href="simple-net-tensors.html#cb63-18"></a></span>
<span id="cb63-19"><a href="simple-net-tensors.html#cb63-19"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb63-20"><a href="simple-net-tensors.html#cb63-20"></a>d_hidden <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb63-21"><a href="simple-net-tensors.html#cb63-21"></a><span class="co"># weights connecting input to hidden layer</span></span>
<span id="cb63-22"><a href="simple-net-tensors.html#cb63-22"></a>w1 <span class="op">=</span> torch.randn(d_in, d_hidden)</span>
<span id="cb63-23"><a href="simple-net-tensors.html#cb63-23"></a><span class="co"># weights connecting hidden to output layer</span></span>
<span id="cb63-24"><a href="simple-net-tensors.html#cb63-24"></a>w2 <span class="op">=</span> torch.randn(d_hidden, d_out)</span>
<span id="cb63-25"><a href="simple-net-tensors.html#cb63-25"></a></span>
<span id="cb63-26"><a href="simple-net-tensors.html#cb63-26"></a><span class="co"># hidden layer bias</span></span>
<span id="cb63-27"><a href="simple-net-tensors.html#cb63-27"></a>b1 <span class="op">=</span> torch.zeros((<span class="dv">1</span>, d_hidden))</span>
<span id="cb63-28"><a href="simple-net-tensors.html#cb63-28"></a><span class="co"># output layer bias</span></span>
<span id="cb63-29"><a href="simple-net-tensors.html#cb63-29"></a>b2 <span class="op">=</span> torch.zeros((<span class="dv">1</span>, d_out))</span>
<span id="cb63-30"><a href="simple-net-tensors.html#cb63-30"></a></span>
<span id="cb63-31"><a href="simple-net-tensors.html#cb63-31"></a><span class="co">### network parameters ---------------------------------------------------------</span></span>
<span id="cb63-32"><a href="simple-net-tensors.html#cb63-32"></a></span>
<span id="cb63-33"><a href="simple-net-tensors.html#cb63-33"></a>learning_rate <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb63-34"><a href="simple-net-tensors.html#cb63-34"></a></span>
<span id="cb63-35"><a href="simple-net-tensors.html#cb63-35"></a><span class="co">### training loop --------------------------------------------------------------</span></span>
<span id="cb63-36"><a href="simple-net-tensors.html#cb63-36"></a></span>
<span id="cb63-37"><a href="simple-net-tensors.html#cb63-37"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb63-38"><a href="simple-net-tensors.html#cb63-38"></a>    </span>
<span id="cb63-39"><a href="simple-net-tensors.html#cb63-39"></a>    <span class="co">### -------- Forward pass -------- </span></span>
<span id="cb63-40"><a href="simple-net-tensors.html#cb63-40"></a>    </span>
<span id="cb63-41"><a href="simple-net-tensors.html#cb63-41"></a>    <span class="co"># compute pre-activations of hidden layers (dim: 100 x 32)</span></span>
<span id="cb63-42"><a href="simple-net-tensors.html#cb63-42"></a>    h <span class="op">=</span> x.mm(w1) <span class="op">+</span> b1</span>
<span id="cb63-43"><a href="simple-net-tensors.html#cb63-43"></a>    <span class="co"># apply activation function (dim: 100 x 32)</span></span>
<span id="cb63-44"><a href="simple-net-tensors.html#cb63-44"></a>    h_relu <span class="op">=</span> h.clamp(<span class="bu">min</span> <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb63-45"><a href="simple-net-tensors.html#cb63-45"></a>    <span class="co"># compute output (dim: 100 x 1)</span></span>
<span id="cb63-46"><a href="simple-net-tensors.html#cb63-46"></a>    y_pred <span class="op">=</span> h_relu.mm(w2) <span class="op">+</span> b2</span>
<span id="cb63-47"><a href="simple-net-tensors.html#cb63-47"></a></span>
<span id="cb63-48"><a href="simple-net-tensors.html#cb63-48"></a>    <span class="co">### -------- compute loss -------- </span></span>
<span id="cb63-49"><a href="simple-net-tensors.html#cb63-49"></a>    loss <span class="op">=</span> (y_pred <span class="op">-</span> y).<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>().item()</span>
<span id="cb63-50"><a href="simple-net-tensors.html#cb63-50"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span>(t, loss)</span>
<span id="cb63-51"><a href="simple-net-tensors.html#cb63-51"></a></span>
<span id="cb63-52"><a href="simple-net-tensors.html#cb63-52"></a>    <span class="co">### -------- Backpropagation -------- </span></span>
<span id="cb63-53"><a href="simple-net-tensors.html#cb63-53"></a>    </span>
<span id="cb63-54"><a href="simple-net-tensors.html#cb63-54"></a>    <span class="co"># gradient of loss w.r.t. prediction (dim: 100 x 1)</span></span>
<span id="cb63-55"><a href="simple-net-tensors.html#cb63-55"></a>    grad_y_pred <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> (y_pred <span class="op">-</span> y)</span>
<span id="cb63-56"><a href="simple-net-tensors.html#cb63-56"></a>    <span class="co"># gradient of loss w.r.t. w2 (dim: 32 x 1)</span></span>
<span id="cb63-57"><a href="simple-net-tensors.html#cb63-57"></a>    grad_w2 <span class="op">=</span> h_relu.t().mm(grad_y_pred)</span>
<span id="cb63-58"><a href="simple-net-tensors.html#cb63-58"></a>    <span class="co"># gradient of loss w.r.t. hidden activation (dim: 100 x 32)</span></span>
<span id="cb63-59"><a href="simple-net-tensors.html#cb63-59"></a>    grad_h_relu <span class="op">=</span> grad_y_pred.mm(w2.t())</span>
<span id="cb63-60"><a href="simple-net-tensors.html#cb63-60"></a>    <span class="co"># gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)</span></span>
<span id="cb63-61"><a href="simple-net-tensors.html#cb63-61"></a>    grad_h <span class="op">=</span> grad_h_relu.clone()</span>
<span id="cb63-62"><a href="simple-net-tensors.html#cb63-62"></a>    grad_h[h <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb63-63"><a href="simple-net-tensors.html#cb63-63"></a>    <span class="co"># gradient of loss w.r.t. b2 (shape: ())</span></span>
<span id="cb63-64"><a href="simple-net-tensors.html#cb63-64"></a>    grad_b2 <span class="op">=</span> grad_y_pred.<span class="bu">sum</span>()</span>
<span id="cb63-65"><a href="simple-net-tensors.html#cb63-65"></a>    </span>
<span id="cb63-66"><a href="simple-net-tensors.html#cb63-66"></a>    <span class="co"># gradient of loss w.r.t. w1 (dim: 3 x 32)</span></span>
<span id="cb63-67"><a href="simple-net-tensors.html#cb63-67"></a>    grad_w1 <span class="op">=</span> x.t().mm(grad_h)</span>
<span id="cb63-68"><a href="simple-net-tensors.html#cb63-68"></a>    <span class="co"># gradient of loss w.r.t. b1 (shape: (32, ))</span></span>
<span id="cb63-69"><a href="simple-net-tensors.html#cb63-69"></a>    grad_b1 <span class="op">=</span> grad_h.<span class="bu">sum</span>(axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb63-70"><a href="simple-net-tensors.html#cb63-70"></a></span>
<span id="cb63-71"><a href="simple-net-tensors.html#cb63-71"></a>    <span class="co">### -------- Update weights -------- </span></span>
<span id="cb63-72"><a href="simple-net-tensors.html#cb63-72"></a>    </span>
<span id="cb63-73"><a href="simple-net-tensors.html#cb63-73"></a>    w2 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w2</span>
<span id="cb63-74"><a href="simple-net-tensors.html#cb63-74"></a>    b2 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b2</span>
<span id="cb63-75"><a href="simple-net-tensors.html#cb63-75"></a>    w1 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w1</span>
<span id="cb63-76"><a href="simple-net-tensors.html#cb63-76"></a>    b1 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b1</span></code></pre></div>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>As well as a few others (for an overview see <a href="https://pytorch.org/docs/stable/tensor_view.html#tensor-view-doc" class="uri">https://pytorch.org/docs/stable/tensor_view.html#tensor-view-doc</a>).<a href="simple-net-tensors.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Although it may sound like, “contiguous” does not correspond to what we’d call “contiguous in memory” in casual
language.<a href="simple-net-tensors.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>For correctness’ sake, <code>contiguous()</code> will only make a copy if the tensor it is called on is <em>not contiguous already.</em><a href="simple-net-tensors.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-net-R.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple-net-autograd.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
