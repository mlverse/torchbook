<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Modifying the simple network to use torch tensors | Applied deep learning with torch from R</title>
  <meta name="description" content="tbd" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Modifying the simple network to use torch tensors | Applied deep learning with torch from R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="tbd" />
  <meta property="og:image" content="tbdcover.png" />
  <meta property="og:description" content="tbd" />
  <meta name="github-repo" content="tbd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Modifying the simple network to use torch tensors | Applied deep learning with torch from R" />
  
  <meta name="twitter:description" content="tbd" />
  <meta name="twitter:image" content="tbdcover.png" />

<meta name="author" content="tbd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-net-R.html"/>
<link rel="next" href="simple-net-autograd.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
</ul></li>
<li class="part"><span><b>I Using Torch</b></span></li>
<li class="chapter" data-level="" data-path="using-torch-intro.html"><a href="using-torch-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="2" data-path="simple-net-R.html"><a href="simple-net-R.html"><i class="fa fa-check"></i><b>2</b> A simple neural network in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#whats-in-a-network"><i class="fa fa-check"></i><b>2.1</b> What’s in a network?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="simple-net-R.html"><a href="simple-net-R.html#gradient-descent"><i class="fa fa-check"></i><b>2.1.1</b> Gradient descent</a></li>
<li class="chapter" data-level="2.1.2" data-path="simple-net-R.html"><a href="simple-net-R.html#from-linear-regression-to-a-simple-network"><i class="fa fa-check"></i><b>2.1.2</b> From linear regression to a simple network</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#a-simple-network"><i class="fa fa-check"></i><b>2.2</b> A simple network</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#simulate-data"><i class="fa fa-check"></i><b>2.2.1</b> Simulate data</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#initialize-weights-and-biases"><i class="fa fa-check"></i><b>2.2.2</b> Initialize weights and biases</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>2.2.3</b> Training loop</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple-net-R.html"><a href="simple-net-R.html#complete-code"><i class="fa fa-check"></i><b>2.2.4</b> Complete code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html"><i class="fa fa-check"></i><b>3</b> Modifying the simple network to use torch tensors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#tensors"><i class="fa fa-check"></i><b>3.1</b> Tensors</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#creation"><i class="fa fa-check"></i><b>3.1.1</b> Creation</a></li>
<li class="chapter" data-level="3.1.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#conversion-to-built-in-r-data-types"><i class="fa fa-check"></i><b>3.1.2</b> Conversion to built-in R data types</a></li>
<li class="chapter" data-level="3.1.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#indexing-and-slicing-tensors"><i class="fa fa-check"></i><b>3.1.3</b> Indexing and slicing tensors</a></li>
<li class="chapter" data-level="3.1.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#reshaping-tensors"><i class="fa fa-check"></i><b>3.1.4</b> Reshaping tensors</a></li>
<li class="chapter" data-level="3.1.5" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#operations-on-tensors"><i class="fa fa-check"></i><b>3.1.5</b> Operations on tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#running-on-gpu"><i class="fa fa-check"></i><b>3.2</b> Running on GPU</a></li>
<li class="chapter" data-level="3.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#broadcasting"><i class="fa fa-check"></i><b>3.3</b> Broadcasting</a></li>
<li class="chapter" data-level="3.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#simple-neural-network-using-torch-tensors"><i class="fa fa-check"></i><b>3.4</b> Simple neural network using <code>torch</code> tensors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html"><i class="fa fa-check"></i><b>4</b> Using autograd</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#automatic-differentiation-with-autograd"><i class="fa fa-check"></i><b>4.1</b> Automatic differentiation with <em>autograd</em></a></li>
<li class="chapter" data-level="4.2" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#the-simple-network-now-using-autograd"><i class="fa fa-check"></i><b>4.2</b> The simple network, now using <em>autograd</em></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple-net-modules.html"><a href="simple-net-modules.html"><i class="fa fa-check"></i><b>5</b> Using torch modules</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#modules"><i class="fa fa-check"></i><b>5.1</b> Modules</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#base-modules-layers"><i class="fa fa-check"></i><b>5.1.1</b> Base modules (“layers”)</a></li>
<li class="chapter" data-level="5.1.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#container-modules-models"><i class="fa fa-check"></i><b>5.1.2</b> Container modules (“models”)</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#simple-network-using-modules"><i class="fa fa-check"></i><b>5.2</b> Simple network using modules</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="simple-net-optim.html"><a href="simple-net-optim.html"><i class="fa fa-check"></i><b>6</b> Using <code>torch</code> optimizers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="simple-net-optim.html"><a href="simple-net-optim.html#losses-and-loss-functions"><i class="fa fa-check"></i><b>6.1</b> Losses and loss functions</a></li>
<li class="chapter" data-level="6.2" data-path="simple-net-optim.html"><a href="simple-net-optim.html#optimizers"><i class="fa fa-check"></i><b>6.2</b> Optimizers</a></li>
<li class="chapter" data-level="6.3" data-path="simple-net-optim.html"><a href="simple-net-optim.html#simple-network-final-version"><i class="fa fa-check"></i><b>6.3</b> Simple network: final version</a></li>
</ul></li>
<li class="part"><span><b>II Image Recognition</b></span></li>
<li class="chapter" data-level="" data-path="image-recognition-intro.html"><a href="image-recognition-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="7" data-path="image-classification.html"><a href="image-classification.html"><i class="fa fa-check"></i><b>7</b> Classifying images</a>
<ul>
<li class="chapter" data-level="7.1" data-path="image-classification.html"><a href="image-classification.html#data-loading-and-preprocessing"><i class="fa fa-check"></i><b>7.1</b> Data loading and preprocessing</a></li>
<li class="chapter" data-level="7.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>7.2</b> Model</a></li>
<li class="chapter" data-level="7.3" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>7.3</b> Training</a></li>
<li class="chapter" data-level="7.4" data-path="image-classification.html"><a href="image-classification.html#test-set-accuracy"><i class="fa fa-check"></i><b>7.4</b> Test set accuracy</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="unet.html"><a href="unet.html"><i class="fa fa-check"></i><b>8</b> Brain Image Segmentation with U-Net</a>
<ul>
<li class="chapter" data-level="8.1" data-path="unet.html"><a href="unet.html#u-net"><i class="fa fa-check"></i><b>8.1</b> U-Net</a></li>
<li class="chapter" data-level="8.2" data-path="unet.html"><a href="unet.html#brain-image-segmentation"><i class="fa fa-check"></i><b>8.2</b> Brain image segmentation</a></li>
<li class="chapter" data-level="8.3" data-path="unet.html"><a href="unet.html#data"><i class="fa fa-check"></i><b>8.3</b> Data</a></li>
<li class="chapter" data-level="8.4" data-path="unet.html"><a href="unet.html#dataset"><i class="fa fa-check"></i><b>8.4</b> Dataset</a></li>
<li class="chapter" data-level="8.5" data-path="unet.html"><a href="unet.html#model-1"><i class="fa fa-check"></i><b>8.5</b> Model</a></li>
<li class="chapter" data-level="8.6" data-path="unet.html"><a href="unet.html#optimization"><i class="fa fa-check"></i><b>8.6</b> Optimization</a></li>
<li class="chapter" data-level="8.7" data-path="unet.html"><a href="unet.html#training-1"><i class="fa fa-check"></i><b>8.7</b> Training</a></li>
<li class="chapter" data-level="8.8" data-path="unet.html"><a href="unet.html#evaluation"><i class="fa fa-check"></i><b>8.8</b> Evaluation</a></li>
</ul></li>
<li class="part"><span><b>III Time series</b></span></li>
<li class="chapter" data-level="" data-path="timeseries-intro.html"><a href="timeseries-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="9" data-path="RNN.html"><a href="RNN.html"><i class="fa fa-check"></i><b>9</b> Torch transformer modules</a></li>
<li class="chapter" data-level="10" data-path="seq2seq-att.html"><a href="seq2seq-att.html"><i class="fa fa-check"></i><b>10</b> Sequence-to-sequence models with attention</a></li>
<li class="chapter" data-level="11" data-path="transformer.html"><a href="transformer.html"><i class="fa fa-check"></i><b>11</b> Torch transformer modules</a></li>
<li class="part"><span><b>IV Natural language processing</b></span></li>
<li class="chapter" data-level="" data-path="NLP-intro.html"><a href="NLP-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="12" data-path="NLPseq2seq-att.html"><a href="NLPseq2seq-att.html"><i class="fa fa-check"></i><b>12</b> Sequence-to-sequence models with attention</a></li>
<li class="chapter" data-level="13" data-path="NLPtransformer.html"><a href="NLPtransformer.html"><i class="fa fa-check"></i><b>13</b> Torch transformer modules</a></li>
<li class="part"><span><b>V Tabular data</b></span></li>
<li class="chapter" data-level="" data-path="tabular-intro.html"><a href="tabular-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="14" data-path="categorical.html"><a href="categorical.html"><i class="fa fa-check"></i><b>14</b> Handling categorical data</a>
<ul>
<li class="chapter" data-level="14.1" data-path="categorical.html"><a href="categorical.html#agenda"><i class="fa fa-check"></i><b>14.1</b> Agenda</a></li>
<li class="chapter" data-level="14.2" data-path="categorical.html"><a href="categorical.html#dataset-1"><i class="fa fa-check"></i><b>14.2</b> Dataset</a></li>
<li class="chapter" data-level="14.3" data-path="categorical.html"><a href="categorical.html#model-2"><i class="fa fa-check"></i><b>14.3</b> Model</a></li>
<li class="chapter" data-level="14.4" data-path="categorical.html"><a href="categorical.html#training-2"><i class="fa fa-check"></i><b>14.4</b> Training</a></li>
<li class="chapter" data-level="14.5" data-path="categorical.html"><a href="categorical.html#evaluation-1"><i class="fa fa-check"></i><b>14.5</b> Evaluation</a></li>
<li class="chapter" data-level="14.6" data-path="categorical.html"><a href="categorical.html#making-the-task-harder"><i class="fa fa-check"></i><b>14.6</b> Making the task harder</a></li>
<li class="chapter" data-level="14.7" data-path="categorical.html"><a href="categorical.html#a-look-at-the-hidden-representations"><i class="fa fa-check"></i><b>14.7</b> A look at the hidden representations</a></li>
</ul></li>
<li class="part"><span><b>VI Generative deep learning</b></span></li>
<li class="chapter" data-level="" data-path="generative-intro.html"><a href="generative-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="15" data-path="gans.html"><a href="gans.html"><i class="fa fa-check"></i><b>15</b> Generative adversarial networks</a>
<ul>
<li class="chapter" data-level="15.1" data-path="gans.html"><a href="gans.html#dataset-2"><i class="fa fa-check"></i><b>15.1</b> Dataset</a></li>
<li class="chapter" data-level="15.2" data-path="gans.html"><a href="gans.html#model-3"><i class="fa fa-check"></i><b>15.2</b> Model</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="gans.html"><a href="gans.html#generator"><i class="fa fa-check"></i><b>15.2.1</b> Generator</a></li>
<li class="chapter" data-level="15.2.2" data-path="gans.html"><a href="gans.html#discriminator"><i class="fa fa-check"></i><b>15.2.2</b> Discriminator</a></li>
<li class="chapter" data-level="15.2.3" data-path="gans.html"><a href="gans.html#optimizers-and-loss-function"><i class="fa fa-check"></i><b>15.2.3</b> Optimizers and loss function</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="gans.html"><a href="gans.html#training-loop-3"><i class="fa fa-check"></i><b>15.3</b> Training loop</a></li>
<li class="chapter" data-level="15.4" data-path="gans.html"><a href="gans.html#artifacts"><i class="fa fa-check"></i><b>15.4</b> Artifacts</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="vaes.html"><a href="vaes.html"><i class="fa fa-check"></i><b>16</b> Variational autoencoders</a>
<ul>
<li class="chapter" data-level="16.1" data-path="vaes.html"><a href="vaes.html#dataset-3"><i class="fa fa-check"></i><b>16.1</b> Dataset</a></li>
<li class="chapter" data-level="16.2" data-path="vaes.html"><a href="vaes.html#model-4"><i class="fa fa-check"></i><b>16.2</b> Model</a></li>
<li class="chapter" data-level="16.3" data-path="vaes.html"><a href="vaes.html#training-the-vae"><i class="fa fa-check"></i><b>16.3</b> Training the VAE</a></li>
<li class="chapter" data-level="16.4" data-path="vaes.html"><a href="vaes.html#latent-space"><i class="fa fa-check"></i><b>16.4</b> Latent space</a></li>
</ul></li>
<li class="part"><span><b>VII Deep learning on graphs</b></span></li>
<li class="chapter" data-level="" data-path="graph-intro.html"><a href="graph-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VIII Probabilistic deep learning</b></span></li>
<li class="chapter" data-level="" data-path="probabilistic-intro.html"><a href="probabilistic-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>IX Private and secure deep learning</b></span></li>
<li class="chapter" data-level="" data-path="private-secure-intro.html"><a href="private-secure-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>X Research topics</b></span></li>
<li class="chapter" data-level="" data-path="research-intro.html"><a href="research-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied deep learning with torch from R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple_net_tensors" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Modifying the simple network to use torch tensors</h1>
<p>Our first step of “torchifying” the network is to use torch <em>tensors</em> instead of <code>rray</code>. This means:</p>
<ul>
<li>We create data structures directly as torch tensors, e.g.using <code>torch_randn()</code> instead of <code>rray(rnorm...)</code>.</li>
<li>We use tensor operations to create new tensors from existing ones, by e.g. matrix multiplication (<code>torch_mm (t1, t2)</code>), matrix transposition (<code>torch_t(t)</code>), or setting negative elements to 0 <code>(torch_clamp (t, min = 0</code>).</li>
</ul>
<p>But first, let’s learn about <code>torch</code> tensors: How to create them; how to manipulate their contents and/or modify their shapes; how to convert them to R arrays, matrices or vectors; and of course, given the omnipresent need for speed: how to get all those operations executed on the GPU.</p>
<div id="tensors" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Tensors</h2>
<div id="creation" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Creation</h3>
<p>Tensors may be created by specifying individual values. Here we create two one-dimensional tensors (vectors), of types <code>float</code> and <code>bool</code>, respectively:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="simple-net-tensors.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb9-2"><a href="simple-net-tensors.html#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># a 1d vector of length 2</span></span>
<span id="cb9-3"><a href="simple-net-tensors.html#cb9-3" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb9-4"><a href="simple-net-tensors.html#cb9-4" aria-hidden="true" tabindex="-1"></a>t</span>
<span id="cb9-5"><a href="simple-net-tensors.html#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="simple-net-tensors.html#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># also 1d, but of type boolean</span></span>
<span id="cb9-7"><a href="simple-net-tensors.html#cb9-7" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>))</span>
<span id="cb9-8"><a href="simple-net-tensors.html#cb9-8" aria-hidden="true" tabindex="-1"></a>t</span></code></pre></div>
<pre><code>torch_tensor 
 1
 2
[ CPUFloatType{2} ]

torch_tensor 
 1
 0
[ CPUBoolType{2} ]</code></pre>
<p>And here are two ways to create two-dimensional tensors (matrices). Note how in the second approach, you need to specify <code>byrow = TRUE</code> in the call to <code>matrix()</code> to get values arranged in row-major order.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="simple-net-tensors.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a 3x3 tensor (matrix)</span></span>
<span id="cb11-2"><a href="simple-net-tensors.html#cb11-2" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>), <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)))</span>
<span id="cb11-3"><a href="simple-net-tensors.html#cb11-3" aria-hidden="true" tabindex="-1"></a>t</span>
<span id="cb11-4"><a href="simple-net-tensors.html#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="simple-net-tensors.html#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># also 3x3</span></span>
<span id="cb11-6"><a href="simple-net-tensors.html#cb11-6" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">matrix</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>, <span class="at">ncol =</span> <span class="dv">3</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>))</span>
<span id="cb11-7"><a href="simple-net-tensors.html#cb11-7" aria-hidden="true" tabindex="-1"></a>t</span></code></pre></div>
<pre><code>torch_tensor 
 1  2  0
 3  0  0
 4  5  6
[ CPUFloatType{3,3} ]

torch_tensor 
 1  2  3
 4  5  6
 7  8  9
[ CPULongType{3,3} ]</code></pre>
<p>In higher dimensions especially, it can be easier to specify the type of tensor abstractly, as in: “give me a tensor of &lt;…&gt; of shape n1 x n2,” where &lt;…&gt; could be “zeros”; or “ones”; or, say, “values drawn from a standard normal distribution”:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="simple-net-tensors.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a 3x3 tensor of standard-normally distributed values</span></span>
<span id="cb13-2"><a href="simple-net-tensors.html#cb13-2" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb13-3"><a href="simple-net-tensors.html#cb13-3" aria-hidden="true" tabindex="-1"></a>t</span>
<span id="cb13-4"><a href="simple-net-tensors.html#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="simple-net-tensors.html#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># a 4x2x2 (3d) tensor of zeroes</span></span>
<span id="cb13-6"><a href="simple-net-tensors.html#cb13-6" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_zeros</span>(<span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb13-7"><a href="simple-net-tensors.html#cb13-7" aria-hidden="true" tabindex="-1"></a>t</span></code></pre></div>
<pre><code>torch_tensor 
-2.1563  1.7085  0.5245
 0.8955 -0.6854  0.2418
 0.4193 -0.7742 -1.0399
[ CPUFloatType{3,3} ]

torch_tensor 
(1,.,.) = 
  0  0
  0  0

(2,.,.) = 
  0  0
  0  0

(3,.,.) = 
  0  0
  0  0

(4,.,.) = 
  0  0
  0  0
[ CPUFloatType{4,2,2} ]</code></pre>
<p>Many similar functions exist, including, e.g., <code>torch_arange()</code> to create a tensor holding a sequence of evenly spaced values, <code>torch_eye()</code> which returns an identity matrix, and <code>torch_logspace()</code> which fills a specified range with a list of values spaced logarithmically.</p>
<p>If no <code>dtype</code> argument is specified, <code>torch</code> will infer the data type from the passed-in value(s). For example:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="simple-net-tensors.html#cb15-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>))</span>
<span id="cb15-2"><a href="simple-net-tensors.html#cb15-2" aria-hidden="true" tabindex="-1"></a>t<span class="sc">$</span>dtype</span>
<span id="cb15-3"><a href="simple-net-tensors.html#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="simple-net-tensors.html#cb15-4" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(1L)</span>
<span id="cb15-5"><a href="simple-net-tensors.html#cb15-5" aria-hidden="true" tabindex="-1"></a>t<span class="sc">$</span>dtype</span></code></pre></div>
<pre><code>torch_Float
torch_Long</code></pre>
<p>But we can explicitly request a different <code>dtype</code> if we want:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="simple-net-tensors.html#cb17-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="dv">2</span>, <span class="at">dtype =</span> <span class="fu">torch_double</span>())</span>
<span id="cb17-2"><a href="simple-net-tensors.html#cb17-2" aria-hidden="true" tabindex="-1"></a>t<span class="sc">$</span>dtype</span></code></pre></div>
<pre><code>torch_Double</code></pre>
<p><code>torch</code> tensors live on a <em>device</em>. By default, this will be the CPU:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="simple-net-tensors.html#cb19-1" aria-hidden="true" tabindex="-1"></a>t<span class="sc">$</span>device</span></code></pre></div>
<pre><code>torch_device(type=&#39;cpu&#39;)</code></pre>
<p>But we could also define a tensor to live on the GPU:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="simple-net-tensors.html#cb21-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="dv">2</span>, <span class="at">device =</span> <span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb21-2"><a href="simple-net-tensors.html#cb21-2" aria-hidden="true" tabindex="-1"></a>t<span class="sc">$</span>device</span></code></pre></div>
<pre><code>torch_device(type=&#39;cuda&#39;, index=0)</code></pre>
<p>We’ll talk more about devices below.</p>
<p>There is another very important parameter to the tensor-creation functions: <code>requires_grad</code>. Here though, I need to ask for your patience: This one will prominently figure in the next section.</p>
</div>
<div id="conversion-to-built-in-r-data-types" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Conversion to built-in R data types</h3>
<p>To convert <code>torch</code> tensors to R, use <code>as_array()</code>:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="simple-net-tensors.html#cb23-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">matrix</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>, <span class="at">ncol =</span> <span class="dv">3</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>))</span>
<span id="cb23-2"><a href="simple-net-tensors.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">as_array</span>(t)</span></code></pre></div>
<pre><code>     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9</code></pre>
<p>Depending on whether the tensor is one-, two-, or three-dimensional, the resulting R object will be a vector, a matrix, or an array:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="simple-net-tensors.html#cb25-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb25-2"><a href="simple-net-tensors.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">as_array</span>(t) <span class="sc">%&gt;%</span> <span class="fu">class</span>()</span>
<span id="cb25-3"><a href="simple-net-tensors.html#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="simple-net-tensors.html#cb25-4" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_ones</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb25-5"><a href="simple-net-tensors.html#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="fu">as_array</span>(t) <span class="sc">%&gt;%</span> <span class="fu">class</span>()</span>
<span id="cb25-6"><a href="simple-net-tensors.html#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="simple-net-tensors.html#cb25-7" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_ones</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb25-8"><a href="simple-net-tensors.html#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="fu">as_array</span>(t) <span class="sc">%&gt;%</span> <span class="fu">class</span>()</span></code></pre></div>
<pre><code>[1] &quot;numeric&quot;

[1] &quot;matrix&quot; &quot;array&quot; 

[1] &quot;array&quot;</code></pre>
<p>For one-dimensional and two-dimensional tensors, it is also possible to use <code>as.integer()</code> / <code>as.matrix()</code>. (One reason you might want to do this is to have more self-documenting code.)</p>
<p>If a tensor currently lives on the GPU, you need to move it to the CPU first:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="simple-net-tensors.html#cb27-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="dv">2</span>, <span class="at">device =</span> <span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb27-2"><a href="simple-net-tensors.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">as.integer</span>(t<span class="sc">$</span><span class="fu">cpu</span>())</span></code></pre></div>
<pre><code>[1] 2</code></pre>
</div>
<div id="indexing-and-slicing-tensors" class="section level3" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Indexing and slicing tensors</h3>
<p>Often, we want to retrieve not a complete tensor, but only some of the values it holds, or even just a single value. In these cases, we talk about <em>slicing</em> and <em>indexing</em>, respectively.</p>
<p>In R, these operations are 1-based, meaning that when we specify offsets, we assume for the very first element in an array to reside at offset <code>1</code>. The same behavior was implemented for <code>torch</code>. Thus, a lot of the functionality described in this section should feel intuitive.</p>
<p>The way I’m organizing this section is the following. We’ll inspect the intuitive parts first, where by intuitive I mean: intuitive to the R user who has not yet worked with Python’s <a href="https://numpy.org/">NumPy</a>. Then come things which, to this user, may look more surprising, but will turn out to be pretty useful.</p>
<div id="indexing-and-slicing-the-r-like-part" class="section level4" number="3.1.3.1">
<h4><span class="header-section-number">3.1.3.1</span> Indexing and slicing: the R-like part</h4>
<p>None of these should be overly surprising:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="simple-net-tensors.html#cb29-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)))</span>
<span id="cb29-2"><a href="simple-net-tensors.html#cb29-2" aria-hidden="true" tabindex="-1"></a>t</span>
<span id="cb29-3"><a href="simple-net-tensors.html#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="simple-net-tensors.html#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="co"># a single value</span></span>
<span id="cb29-5"><a href="simple-net-tensors.html#cb29-5" aria-hidden="true" tabindex="-1"></a>t[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb29-6"><a href="simple-net-tensors.html#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="simple-net-tensors.html#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co"># first row, all columns</span></span>
<span id="cb29-8"><a href="simple-net-tensors.html#cb29-8" aria-hidden="true" tabindex="-1"></a>t[<span class="dv">1</span>, ]</span>
<span id="cb29-9"><a href="simple-net-tensors.html#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="simple-net-tensors.html#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co"># first row, a subset of columns</span></span>
<span id="cb29-11"><a href="simple-net-tensors.html#cb29-11" aria-hidden="true" tabindex="-1"></a>t[<span class="dv">1</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span></code></pre></div>
<pre><code>torch_tensor 
 1  2  3
 4  5  6
[ CPUFloatType{2,3} ]

torch_tensor 
1
[ CPUFloatType{} ]

torch_tensor 
 1
 2
 3
[ CPUFloatType{3} ]

torch_tensor 
 1
 2
[ CPUFloatType{2} ]</code></pre>
<p>Note how, just as in R, singleton dimensions are dropped:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="simple-net-tensors.html#cb31-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)))</span>
<span id="cb31-2"><a href="simple-net-tensors.html#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="simple-net-tensors.html#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 2x3</span></span>
<span id="cb31-4"><a href="simple-net-tensors.html#cb31-4" aria-hidden="true" tabindex="-1"></a>t<span class="sc">$</span><span class="fu">size</span>() </span>
<span id="cb31-5"><a href="simple-net-tensors.html#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="simple-net-tensors.html#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co"># just a single row: will be returned as a vector</span></span>
<span id="cb31-7"><a href="simple-net-tensors.html#cb31-7" aria-hidden="true" tabindex="-1"></a>t[<span class="dv">1</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]<span class="sc">$</span><span class="fu">size</span>() </span>
<span id="cb31-8"><a href="simple-net-tensors.html#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="simple-net-tensors.html#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co"># a single element</span></span>
<span id="cb31-10"><a href="simple-net-tensors.html#cb31-10" aria-hidden="true" tabindex="-1"></a>t[<span class="dv">1</span>, <span class="dv">1</span>]<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>[1] 2 3

[1] 2

integer(0)</code></pre>
<p>And just like in R, you can specify <code>drop = FALSE</code> to keep those dimensions:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="simple-net-tensors.html#cb33-1" aria-hidden="true" tabindex="-1"></a>t[<span class="dv">1</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, drop <span class="ot">=</span> <span class="cn">FALSE</span>]<span class="sc">$</span><span class="fu">size</span>()</span>
<span id="cb33-2"><a href="simple-net-tensors.html#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="simple-net-tensors.html#cb33-3" aria-hidden="true" tabindex="-1"></a>t[<span class="dv">1</span>, <span class="dv">1</span>, drop <span class="ot">=</span> <span class="cn">FALSE</span>]<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>[1] 1 2

[1] 1 1</code></pre>
</div>
<div id="indexing-and-slicing-what-to-look-out-for" class="section level4" number="3.1.3.2">
<h4><span class="header-section-number">3.1.3.2</span> Indexing and slicing: What to look out for</h4>
<p>Whereas R uses negative numbers to remove elements at specified positions, in <code>torch</code> negative values indicate that we start counting from the end of a tensor – with <code>-1</code> pointing to its last element:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="simple-net-tensors.html#cb35-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)))</span>
<span id="cb35-2"><a href="simple-net-tensors.html#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="simple-net-tensors.html#cb35-3" aria-hidden="true" tabindex="-1"></a>t[<span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb35-4"><a href="simple-net-tensors.html#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="simple-net-tensors.html#cb35-5" aria-hidden="true" tabindex="-1"></a>t[ , <span class="sc">-</span><span class="dv">2</span><span class="sc">:-</span><span class="dv">1</span>] </span></code></pre></div>
<pre><code>torch_tensor 
3
[ CPUFloatType{} ]

torch_tensor 
 2  3
 5  6
[ CPUFloatType{2,2} ]</code></pre>
<p>This is a feature you might know from NumPy. Same with the following.</p>
<p>When the slicing expression <code>m:n</code> is augmented by another colon and a third number – <code>m:n:o</code> –, we will take every <code>o</code>th item from the range specified by <code>m</code> and <code>n</code>:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="simple-net-tensors.html#cb37-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span>
<span id="cb37-2"><a href="simple-net-tensors.html#cb37-2" aria-hidden="true" tabindex="-1"></a>t[<span class="dv">2</span><span class="sc">:</span><span class="dv">10</span><span class="sc">:</span><span class="dv">2</span>]</span></code></pre></div>
<pre><code>torch_tensor 
  2
  4
  6
  8
 10
[ CPULongType{5} ]</code></pre>
<p>Sometimes we don’t know how many dimensions a tensor has, but we do know what to do with the final dimension, or the first one. To subsume all others, we can use <code>..</code>:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="simple-net-tensors.html#cb39-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_randint</span>(<span class="sc">-</span><span class="dv">7</span>, <span class="dv">7</span>, <span class="at">size =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb39-2"><a href="simple-net-tensors.html#cb39-2" aria-hidden="true" tabindex="-1"></a>t</span>
<span id="cb39-3"><a href="simple-net-tensors.html#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="simple-net-tensors.html#cb39-4" aria-hidden="true" tabindex="-1"></a>t[.., <span class="dv">1</span>]</span>
<span id="cb39-5"><a href="simple-net-tensors.html#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="simple-net-tensors.html#cb39-6" aria-hidden="true" tabindex="-1"></a>t[<span class="dv">2</span>, ..]</span></code></pre></div>
<pre><code>torch_tensor 
(1,.,.) = 
  2 -2
 -5  4

(2,.,.) = 
  0  4
 -3 -1
[ CPUFloatType{2,2,2} ]

torch_tensor 
 2 -5
 0 -3
[ CPUFloatType{2,2} ]

torch_tensor 
 0  4
-3 -1
[ CPUFloatType{2,2} ]</code></pre>
<p>Now we move on to a topic that, in practice, is just as indispensable as slicing: changing tensor <em>shapes</em>.</p>
</div>
</div>
<div id="reshaping-tensors" class="section level3" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Reshaping tensors</h3>
<p>Changes in shape can occur in two fundamentally different ways. Seeing how “reshape” really means: <em>keep the values but modify their layout</em>, we could either alter how they’re arranged physically, or keep the physical structure as-is and just change the “mapping” (a semantic change, as it were).</p>
<p>In the first case, storage will have to be allocated for two tensors, source and target, and elements will be copied from the latter to the former. In the second, physically there will be just a single tensor, referenced by two logical entities with distinct metadata.</p>
<p>Not surprisingly, for performance reasons, the second operation is preferred.</p>
<div id="zero-copy-reshaping" class="section level4" number="3.1.4.1">
<h4><span class="header-section-number">3.1.4.1</span> Zero-copy reshaping</h4>
<p>We start with zero-copy methods, as we’ll want to use them whenever we can.</p>
<p>A special case often seen in practice is adding or removing a singleton dimension.</p>
<p><code>unsqueeze()</code> adds a dimension of size <code>1</code> at a position specified by <code>dim</code>:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="simple-net-tensors.html#cb41-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_randint</span>(<span class="at">low =</span> <span class="dv">3</span>, <span class="at">high =</span> <span class="dv">7</span>, <span class="at">size =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb41-2"><a href="simple-net-tensors.html#cb41-2" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">size</span>()</span>
<span id="cb41-3"><a href="simple-net-tensors.html#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="simple-net-tensors.html#cb41-4" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> t1<span class="sc">$</span><span class="fu">unsqueeze</span>(<span class="at">dim =</span> <span class="dv">1</span>)</span>
<span id="cb41-5"><a href="simple-net-tensors.html#cb41-5" aria-hidden="true" tabindex="-1"></a>t2<span class="sc">$</span><span class="fu">size</span>()</span>
<span id="cb41-6"><a href="simple-net-tensors.html#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="simple-net-tensors.html#cb41-7" aria-hidden="true" tabindex="-1"></a>t3 <span class="ot">&lt;-</span> t1<span class="sc">$</span><span class="fu">unsqueeze</span>(<span class="at">dim =</span> <span class="dv">2</span>)</span>
<span id="cb41-8"><a href="simple-net-tensors.html#cb41-8" aria-hidden="true" tabindex="-1"></a>t3<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>[1] 3 3 3

[1] 1 3 3 3

[1] 3 1 3 3</code></pre>
<p>Conversely, <code>squeeze()</code> removes singleton dimensions:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="simple-net-tensors.html#cb43-1" aria-hidden="true" tabindex="-1"></a>t4 <span class="ot">&lt;-</span> t3<span class="sc">$</span><span class="fu">squeeze</span>()</span>
<span id="cb43-2"><a href="simple-net-tensors.html#cb43-2" aria-hidden="true" tabindex="-1"></a>t4<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>[1] 3 3 3</code></pre>
<p>The same could be accomplished with <code>view()</code>. <code>view()</code>, however, is much more general, in that it allows you to reshape the data to any valid dimensionality. (Valid meaning: The number of elements stays the same.)</p>
<p>Here we have a <code>3x2</code> tensor that is reshaped to size <code>2x3</code>:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="simple-net-tensors.html#cb45-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">6</span>)))</span>
<span id="cb45-2"><a href="simple-net-tensors.html#cb45-2" aria-hidden="true" tabindex="-1"></a>t1</span>
<span id="cb45-3"><a href="simple-net-tensors.html#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="simple-net-tensors.html#cb45-4" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> t1<span class="sc">$</span><span class="fu">view</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb45-5"><a href="simple-net-tensors.html#cb45-5" aria-hidden="true" tabindex="-1"></a>t2</span></code></pre></div>
<pre><code>torch_tensor 
 1  2
 3  4
 5  6
[ CPUFloatType{3,2} ]

torch_tensor 
 1  2  3
 4  5  6
[ CPUFloatType{2,3} ]</code></pre>
<p>(Note how this is different from matrix transposition.)</p>
<p>Instead of going from two to three dimensions, we can flatten the matrix to a vector.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="simple-net-tensors.html#cb47-1" aria-hidden="true" tabindex="-1"></a>t4 <span class="ot">&lt;-</span> t1<span class="sc">$</span><span class="fu">view</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">6</span>))</span>
<span id="cb47-2"><a href="simple-net-tensors.html#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="simple-net-tensors.html#cb47-3" aria-hidden="true" tabindex="-1"></a>t4<span class="sc">$</span><span class="fu">size</span>()</span>
<span id="cb47-4"><a href="simple-net-tensors.html#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="simple-net-tensors.html#cb47-5" aria-hidden="true" tabindex="-1"></a>t4</span></code></pre></div>
<pre><code>[1] 1 6

torch_tensor 
 1  2  3  4  5  6
[ CPUFloatType{1,6} ]</code></pre>
<p>In contrast to indexing operations, this does not drop dimensions.</p>
<p>Like we said above, operations like <code>squeeze()</code> or <code>view()</code> do not make copies. Or, put differently: The output tensor shares storage with the input tensor. We can in fact verify this ourselves:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="simple-net-tensors.html#cb49-1" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">storage</span>()<span class="sc">$</span><span class="fu">data_ptr</span>()</span>
<span id="cb49-2"><a href="simple-net-tensors.html#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="simple-net-tensors.html#cb49-3" aria-hidden="true" tabindex="-1"></a>t2<span class="sc">$</span><span class="fu">storage</span>()<span class="sc">$</span><span class="fu">data_ptr</span>()</span></code></pre></div>
<pre><code>[1] &quot;0x5648d02ac800&quot;

[1] &quot;0x5648d02ac800&quot;</code></pre>
<p>What’s different is the storage <em>metadata</em> <code>torch</code> keeps about both tensors. Here, the relevant information is the <em>stride</em>:</p>
<p>A tensor’s <code>stride()</code> method tracks, <em>for every dimension</em>, how many elements have to be traversed to arrive at its next element (row or column, in two dimensions). For <code>t1</code> above, of shape <code>3x2</code>, we have to skip over 2 items to arrive at the next row. To arrive at the next column though, in every row we just have to skip a single entry:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="simple-net-tensors.html#cb51-1" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">stride</span>()</span></code></pre></div>
<pre><code>[1] 2 1</code></pre>
<p>For <code>t2</code>, of shape <code>3x2</code>, the distance between column elements is the same, but the distance between rows is now 3:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="simple-net-tensors.html#cb53-1" aria-hidden="true" tabindex="-1"></a>t2<span class="sc">$</span><span class="fu">stride</span>()</span></code></pre></div>
<pre><code>[1] 3 1</code></pre>
<p>While zero-copy operations are optimal, there are cases where they won’t work.</p>
<p>With <code>view()</code>, this can happen when a tensor was obtained via an operation – other than <code>view()</code> itself – that itself has already modified the <em>stride</em>. One example would be <code>transpose()</code>:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="simple-net-tensors.html#cb55-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">6</span>)))</span>
<span id="cb55-2"><a href="simple-net-tensors.html#cb55-2" aria-hidden="true" tabindex="-1"></a>t1</span>
<span id="cb55-3"><a href="simple-net-tensors.html#cb55-3" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">stride</span>()</span>
<span id="cb55-4"><a href="simple-net-tensors.html#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="simple-net-tensors.html#cb55-5" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> t1<span class="sc">$</span><span class="fu">t</span>()</span>
<span id="cb55-6"><a href="simple-net-tensors.html#cb55-6" aria-hidden="true" tabindex="-1"></a>t2</span>
<span id="cb55-7"><a href="simple-net-tensors.html#cb55-7" aria-hidden="true" tabindex="-1"></a>t2<span class="sc">$</span><span class="fu">stride</span>()</span></code></pre></div>
<pre><code>torch_tensor 
 1  2
 3  4
 5  6
[ CPUFloatType{3,2} ]

[1] 2 1

torch_tensor 
 1  3  5
 2  4  6
[ CPUFloatType{2,3} ]

[1] 1 2</code></pre>
<p>In <code>torch</code> lingo, tensors – like <code>t2</code> – that re-use existing storage (and just read it differently), are said not to be “contiguous”<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. One way to reshape them is to use <code>contiguous()</code> on them before. We’ll see this in the next subsection.</p>
</div>
<div id="reshape-with-copy" class="section level4" number="3.1.4.2">
<h4><span class="header-section-number">3.1.4.2</span> Reshape with copy</h4>
<p>In the following snippet, trying to reshape <code>t2</code> using <code>view()</code> fails, as it already carries information indicating that the underlying data should not be read in physical order.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="simple-net-tensors.html#cb57-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">6</span>)))</span>
<span id="cb57-2"><a href="simple-net-tensors.html#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="simple-net-tensors.html#cb57-3" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> t1<span class="sc">$</span><span class="fu">t</span>()</span>
<span id="cb57-4"><a href="simple-net-tensors.html#cb57-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-5"><a href="simple-net-tensors.html#cb57-5" aria-hidden="true" tabindex="-1"></a>t2<span class="sc">$</span><span class="fu">view</span>(<span class="dv">6</span>) <span class="co"># error!</span></span></code></pre></div>
<pre><code>Error in (function (self, size)  : 
  view size is not compatible with input tensor&#39;s size and stride (at least one dimension spans across two contiguous subspaces).
  Use .reshape(...) instead. (view at ../aten/src/ATen/native/TensorShape.cpp:1364)</code></pre>
<p>However, if we first call <code>contiguous()</code> on it, a <em>new tensor</em> is created, which may then be (virtually) reshaped using <code>view()</code>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="simple-net-tensors.html#cb59-1" aria-hidden="true" tabindex="-1"></a>t3 <span class="ot">&lt;-</span> t2<span class="sc">$</span><span class="fu">contiguous</span>()</span>
<span id="cb59-2"><a href="simple-net-tensors.html#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="simple-net-tensors.html#cb59-3" aria-hidden="true" tabindex="-1"></a>t3<span class="sc">$</span><span class="fu">view</span>(<span class="dv">6</span>)</span></code></pre></div>
<pre><code>torch_tensor 
 1
 3
 5
 2
 4
 6
[ CPUFloatType{6} ]</code></pre>
<p>Alternatively, we can use <code>reshape()</code>. <code>reshape()</code> defaults to <code>view()</code>-like behavior if possible; otherwise it will create a physical copy.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="simple-net-tensors.html#cb61-1" aria-hidden="true" tabindex="-1"></a>t2<span class="sc">$</span><span class="fu">storage</span>()<span class="sc">$</span><span class="fu">data_ptr</span>()</span>
<span id="cb61-2"><a href="simple-net-tensors.html#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="simple-net-tensors.html#cb61-3" aria-hidden="true" tabindex="-1"></a>t4 <span class="ot">&lt;-</span> t2<span class="sc">$</span><span class="fu">reshape</span>(<span class="dv">6</span>)</span>
<span id="cb61-4"><a href="simple-net-tensors.html#cb61-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-5"><a href="simple-net-tensors.html#cb61-5" aria-hidden="true" tabindex="-1"></a>t4<span class="sc">$</span><span class="fu">storage</span>()<span class="sc">$</span><span class="fu">data_ptr</span>()</span></code></pre></div>
<pre><code>[1] &quot;0x5648d49b4f40&quot;

[1] &quot;0x5648d2752980&quot;</code></pre>
</div>
</div>
<div id="operations-on-tensors" class="section level3" number="3.1.5">
<h3><span class="header-section-number">3.1.5</span> Operations on tensors</h3>
<p>Unsurprisingly, <code>torch</code> provides a bunch of mathematical operations on tensors; we’ll see some of them in the network code below, and you’ll encounter lots more when you continue your <code>torch</code> journey. Here, we quickly take a look at the overall tensor method semantics.</p>
<p>Tensor methods normally return references to new objects. Here, we add to <code>t1</code> a clone of itself:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="simple-net-tensors.html#cb63-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">6</span>)))</span>
<span id="cb63-2"><a href="simple-net-tensors.html#cb63-2" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> t1<span class="sc">$</span><span class="fu">clone</span>()</span>
<span id="cb63-3"><a href="simple-net-tensors.html#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="simple-net-tensors.html#cb63-4" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">add</span>(t2)</span></code></pre></div>
<pre><code>torch_tensor 
  2   4
  6   8
 10  12
[ CPUFloatType{3,2} ]</code></pre>
<p>In this process, <code>t1</code> has not been modified:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="simple-net-tensors.html#cb65-1" aria-hidden="true" tabindex="-1"></a>t1</span></code></pre></div>
<pre><code>torch_tensor 
 1  2
 3  4
 5  6
[ CPUFloatType{3,2} ]</code></pre>
<p>Many tensor methods have variants for mutating operations. These all carry a trailing underscore:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="simple-net-tensors.html#cb67-1" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">add_</span>(t1)</span>
<span id="cb67-2"><a href="simple-net-tensors.html#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="simple-net-tensors.html#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="co"># now t1 has been modified</span></span>
<span id="cb67-4"><a href="simple-net-tensors.html#cb67-4" aria-hidden="true" tabindex="-1"></a>t1</span></code></pre></div>
<pre><code>torch_tensor 
  4   8
 12  16
 20  24
[ CPUFloatType{3,2} ]

torch_tensor 
  4   8
 12  16
 20  24
[ CPUFloatType{3,2} ]</code></pre>
<p>Alternatively, you can of course assign the new object to a new reference variable:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="simple-net-tensors.html#cb69-1" aria-hidden="true" tabindex="-1"></a>t3 <span class="ot">&lt;-</span> t1<span class="sc">$</span><span class="fu">add</span>(t1)</span>
<span id="cb69-2"><a href="simple-net-tensors.html#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="simple-net-tensors.html#cb69-3" aria-hidden="true" tabindex="-1"></a>t3</span></code></pre></div>
<pre><code>torch_tensor 
  8  16
 24  32
 40  48
[ CPUFloatType{3,2} ]</code></pre>
<p>There is one thing we need to discuss before we wrap up our introduction to tensors: How can we have all those operations executed on the GPU?</p>
</div>
</div>
<div id="running-on-gpu" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Running on GPU</h2>
<p>To check if your GPU(s) is/are visible to torch, run</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="simple-net-tensors.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cuda_is_available</span>()</span>
<span id="cb71-2"><a href="simple-net-tensors.html#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="simple-net-tensors.html#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cuda_device_count</span>()</span></code></pre></div>
<pre><code>[1] TRUE

[1] 1</code></pre>
<p>Tensors may be requested to live on the GPU right at creation:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="simple-net-tensors.html#cb73-1" aria-hidden="true" tabindex="-1"></a>device <span class="ot">&lt;-</span> <span class="fu">torch_device</span>(<span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb73-2"><a href="simple-net-tensors.html#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="simple-net-tensors.html#cb73-3" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">torch_ones</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="at">device =</span> device) </span></code></pre></div>
<p>Alternatively, they can be moved between devices at any time:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="simple-net-tensors.html#cb74-1" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> t<span class="sc">$</span><span class="fu">cuda</span>()</span>
<span id="cb74-2"><a href="simple-net-tensors.html#cb74-2" aria-hidden="true" tabindex="-1"></a>t2<span class="sc">$</span>device</span></code></pre></div>
<pre><code>torch_device(type=&#39;cuda&#39;, index=0)</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="simple-net-tensors.html#cb76-1" aria-hidden="true" tabindex="-1"></a>t3 <span class="ot">&lt;-</span> t2<span class="sc">$</span><span class="fu">cpu</span>()</span>
<span id="cb76-2"><a href="simple-net-tensors.html#cb76-2" aria-hidden="true" tabindex="-1"></a>t3<span class="sc">$</span>device</span></code></pre></div>
<pre><code>torch_device(type=&#39;cpu&#39;)</code></pre>
<p>That’s it for our discussion on tensors — almost. There is one <code>torch</code> feature that, although related to tensor operations, deserves special mention. It is called broadcasting, and “bilingual” (R + Python) users will know it from NumPy.</p>
</div>
<div id="broadcasting" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Broadcasting</h2>
<p>We often have to perform operations on tensors with shapes that don’t match exactly.</p>
<p>Unsurprisingly, we can add a scalar to a tensor:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="simple-net-tensors.html#cb78-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb78-2"><a href="simple-net-tensors.html#cb78-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-3"><a href="simple-net-tensors.html#cb78-3" aria-hidden="true" tabindex="-1"></a>t1 <span class="sc">+</span> <span class="dv">22</span></span></code></pre></div>
<pre><code>torch_tensor 
 23.1097  21.4425  22.7732  22.2973  21.4128
 22.6936  21.8829  21.1463  21.6781  21.0827
 22.5672  21.2210  21.2344  23.1154  20.5004
[ CPUFloatType{3,5} ]</code></pre>
<p>The same will work if we add tensor of size <code>1</code>:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="simple-net-tensors.html#cb80-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb80-2"><a href="simple-net-tensors.html#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="simple-net-tensors.html#cb80-3" aria-hidden="true" tabindex="-1"></a>t1 <span class="sc">+</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="dv">22</span>))</span></code></pre></div>
<p>Adding tensors of different sizes normally won’t work:</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="simple-net-tensors.html#cb81-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb81-2"><a href="simple-net-tensors.html#cb81-2" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb81-3"><a href="simple-net-tensors.html#cb81-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-4"><a href="simple-net-tensors.html#cb81-4" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">add</span>(t2) <span class="co"># error</span></span></code></pre></div>
<pre><code>Error in (function (self, other, alpha)  : 
  The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 1 (infer_size at ../aten/src/ATen/ExpandUtils.cpp:24)</code></pre>
<p>However, under certain conditions, one or both tensors may be virtually expanded so both tensors line up. This behavior is what is meant by <em>broadcasting</em>. The way it works in <code>torch</code> is not just inspired by, but actually identical to that of NumPy.</p>
<p>The rules are:</p>
<ol style="list-style-type: decimal">
<li><p>We align array shapes, <em>starting from the right</em>.</p>
<p>Say we have two tensors, one of size <code>8x1x6x1</code>, the other of size <code>7x1x5</code>.</p>
<p>Here they are, right-aligned:</p></li>
</ol>
<!-- -->
<pre><code># t1, shape:     8  1  6  1
# t2, shape:        7  1  5</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><p><em>Starting to look from the right</em>, the sizes along aligned axes either have to match exactly, or one of them has to be equal to <code>1</code>: in which case the latter is <em>broadcast</em> to the larger one.</p>
<p>In the above example, this is the case for the second-from-last dimension. This now gives</p></li>
</ol>
<!-- -->
<pre><code># t1, shape:     8  1  6  1
# t2, shape:        7  6  5</code></pre>
<p>, with broadcasting happening in <code>t2</code>.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>If on the left, one of the arrays has an additional axis (or more than one), the other is virtually expanded to have a size of <code>1</code> in that place, in which case broadcasting will happen as stated in (2).</p>
<p>This is the case with <code>t1</code>’s leftmost dimension. First, there is a virtual expansion</p></li>
</ol>
<!-- -->
<pre><code># t1, shape:     8  1  6  1
# t2, shape:     1  7  1  5</code></pre>
<p>and then, broadcasting happens:</p>
<pre><code># t1, shape:     8  1  6  1
# t2, shape:     8  7  1  5</code></pre>
<p>According to these rules, our above example</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="simple-net-tensors.html#cb87-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb87-2"><a href="simple-net-tensors.html#cb87-2" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb87-3"><a href="simple-net-tensors.html#cb87-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-4"><a href="simple-net-tensors.html#cb87-4" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">add</span>(t2)</span></code></pre></div>
<p>could be modified in various ways that would allow for adding two tensors.</p>
<p>For example, if <code>t2</code> were <code>1x5</code>, it would only need to get broadcast to size <code>3x5</code> before the addition operation:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="simple-net-tensors.html#cb88-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb88-2"><a href="simple-net-tensors.html#cb88-2" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">5</span>))</span>
<span id="cb88-3"><a href="simple-net-tensors.html#cb88-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-4"><a href="simple-net-tensors.html#cb88-4" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">add</span>(t2)</span></code></pre></div>
<pre><code>torch_tensor 
-1.0505  1.5811  1.1956 -0.0445  0.5373
 0.0779  2.4273  2.1518 -0.6136  2.6295
 0.1386 -0.6107 -1.2527 -1.3256 -0.1009
[ CPUFloatType{3,5} ]</code></pre>
<p>If it were of size <code>5</code>, a virtual leading dimension would be added, and then, the same broadcasting would take place as in the previous case.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="simple-net-tensors.html#cb90-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb90-2"><a href="simple-net-tensors.html#cb90-2" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">5</span>))</span>
<span id="cb90-3"><a href="simple-net-tensors.html#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="simple-net-tensors.html#cb90-4" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">add</span>(t2)</span></code></pre></div>
<pre><code>torch_tensor 
-1.4123  2.1392 -0.9891  1.1636 -1.4960
 0.8147  1.0368 -2.6144  0.6075 -2.0776
-2.3502  1.4165  0.4651 -0.8816 -1.0685
[ CPUFloatType{3,5} ]</code></pre>
<p>Here is a more complex example. Broadcasting how happens both in <code>t1</code> and in <code>t2</code>:</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="simple-net-tensors.html#cb92-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">5</span>))</span>
<span id="cb92-2"><a href="simple-net-tensors.html#cb92-2" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb92-3"><a href="simple-net-tensors.html#cb92-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-4"><a href="simple-net-tensors.html#cb92-4" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">add</span>(t2)</span></code></pre></div>
<pre><code>torch_tensor 
 1.2274  1.1880  0.8531  1.8511 -0.0627
 0.2639  0.2246 -0.1103  0.8877 -1.0262
-1.5951 -1.6344 -1.9693 -0.9713 -2.8852
[ CPUFloatType{3,5} ]</code></pre>
<p>As a nice concluding example, through broadcasting an outer product can be computed like so:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="simple-net-tensors.html#cb94-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>))</span>
<span id="cb94-2"><a href="simple-net-tensors.html#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="simple-net-tensors.html#cb94-3" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb94-4"><a href="simple-net-tensors.html#cb94-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-5"><a href="simple-net-tensors.html#cb94-5" aria-hidden="true" tabindex="-1"></a>t1<span class="sc">$</span><span class="fu">view</span>(<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">1</span>)) <span class="sc">*</span> t2</span></code></pre></div>
<pre><code>torch_tensor 
  0   0   0
 10  20  30
 20  40  60
 30  60  90
[ CPUFloatType{4,3} ]</code></pre>
<p>And now, we really get to implementing that neural network!</p>
</div>
<div id="simple-neural-network-using-torch-tensors" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Simple neural network using <code>torch</code> tensors</h2>
<p>We now use <code>torch</code> to simulate some data.</p>
<div id="toy-data" class="section level4" number="3.4.0.1">
<h4><span class="header-section-number">3.4.0.1</span> Toy data</h4>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="simple-net-tensors.html#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb96-2"><a href="simple-net-tensors.html#cb96-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-3"><a href="simple-net-tensors.html#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb96-4"><a href="simple-net-tensors.html#cb96-4" aria-hidden="true" tabindex="-1"></a>d_in <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb96-5"><a href="simple-net-tensors.html#cb96-5" aria-hidden="true" tabindex="-1"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb96-6"><a href="simple-net-tensors.html#cb96-6" aria-hidden="true" tabindex="-1"></a>d_out <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb96-7"><a href="simple-net-tensors.html#cb96-7" aria-hidden="true" tabindex="-1"></a><span class="co"># number of observations in training set</span></span>
<span id="cb96-8"><a href="simple-net-tensors.html#cb96-8" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb96-9"><a href="simple-net-tensors.html#cb96-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-10"><a href="simple-net-tensors.html#cb96-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-11"><a href="simple-net-tensors.html#cb96-11" aria-hidden="true" tabindex="-1"></a><span class="co"># create random data</span></span>
<span id="cb96-12"><a href="simple-net-tensors.html#cb96-12" aria-hidden="true" tabindex="-1"></a><span class="co"># input</span></span>
<span id="cb96-13"><a href="simple-net-tensors.html#cb96-13" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(n, d_in)</span>
<span id="cb96-14"><a href="simple-net-tensors.html#cb96-14" aria-hidden="true" tabindex="-1"></a><span class="co"># target</span></span>
<span id="cb96-15"><a href="simple-net-tensors.html#cb96-15" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> x[, <span class="dv">1</span>, drop <span class="ot">=</span> <span class="cn">FALSE</span>] <span class="sc">*</span> <span class="fl">0.2</span> <span class="sc">-</span></span>
<span id="cb96-16"><a href="simple-net-tensors.html#cb96-16" aria-hidden="true" tabindex="-1"></a>  x[, <span class="dv">2</span>, drop <span class="ot">=</span> <span class="cn">FALSE</span>] <span class="sc">*</span> <span class="fl">1.3</span> <span class="sc">-</span></span>
<span id="cb96-17"><a href="simple-net-tensors.html#cb96-17" aria-hidden="true" tabindex="-1"></a>  x[, <span class="dv">3</span>, drop <span class="ot">=</span> <span class="cn">FALSE</span>] <span class="sc">*</span> <span class="fl">0.5</span> <span class="sc">+</span></span>
<span id="cb96-18"><a href="simple-net-tensors.html#cb96-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">torch_randn</span>(n, <span class="dv">1</span>)</span></code></pre></div>
<p>The same goes for network initialization: We now make use of <code>torch_zeros()</code> and <code>torch_randn()</code>.</p>
</div>
<div id="initialize-weights" class="section level4" number="3.4.0.2">
<h4><span class="header-section-number">3.4.0.2</span> Initialize weights</h4>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="simple-net-tensors.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb97-2"><a href="simple-net-tensors.html#cb97-2" aria-hidden="true" tabindex="-1"></a>d_hidden <span class="ot">&lt;-</span> <span class="dv">32</span></span>
<span id="cb97-3"><a href="simple-net-tensors.html#cb97-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-4"><a href="simple-net-tensors.html#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="co"># weights connecting input to hidden layer</span></span>
<span id="cb97-5"><a href="simple-net-tensors.html#cb97-5" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(d_in, d_hidden)</span>
<span id="cb97-6"><a href="simple-net-tensors.html#cb97-6" aria-hidden="true" tabindex="-1"></a><span class="co"># weights connecting hidden to output layer</span></span>
<span id="cb97-7"><a href="simple-net-tensors.html#cb97-7" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(d_hidden, d_out)</span>
<span id="cb97-8"><a href="simple-net-tensors.html#cb97-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-9"><a href="simple-net-tensors.html#cb97-9" aria-hidden="true" tabindex="-1"></a><span class="co"># hidden layer bias</span></span>
<span id="cb97-10"><a href="simple-net-tensors.html#cb97-10" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="fu">torch_zeros</span>(<span class="dv">1</span>, d_hidden)</span>
<span id="cb97-11"><a href="simple-net-tensors.html#cb97-11" aria-hidden="true" tabindex="-1"></a><span class="co"># output layer bias</span></span>
<span id="cb97-12"><a href="simple-net-tensors.html#cb97-12" aria-hidden="true" tabindex="-1"></a>b2 <span class="ot">&lt;-</span> <span class="fu">torch_zeros</span>(<span class="dv">1</span>, d_out)</span></code></pre></div>
</div>
<div id="training-loop-1" class="section level4" number="3.4.0.3">
<h4><span class="header-section-number">3.4.0.3</span> Training loop</h4>
<p>Here are the four phases of the training loop – forward pass, determination of the loss, backward pass, and weight updates –, now with all operations being <code>torch</code> tensor methods. Firstly, the forward pass:</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="simple-net-tensors.html#cb98-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute pre-activations of hidden layers (dim: 100 x 32)</span></span>
<span id="cb98-2"><a href="simple-net-tensors.html#cb98-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># torch_mm does matrix multiplication</span></span>
<span id="cb98-3"><a href="simple-net-tensors.html#cb98-3" aria-hidden="true" tabindex="-1"></a>  h <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">mm</span>(w1) <span class="sc">+</span> b1</span>
<span id="cb98-4"><a href="simple-net-tensors.html#cb98-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb98-5"><a href="simple-net-tensors.html#cb98-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># apply activation function (dim: 100 x 32)</span></span>
<span id="cb98-6"><a href="simple-net-tensors.html#cb98-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># torch_clamp cuts off values below/above given thresholds</span></span>
<span id="cb98-7"><a href="simple-net-tensors.html#cb98-7" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="ot">&lt;-</span> h<span class="sc">$</span><span class="fu">clamp</span>(<span class="at">min =</span> <span class="dv">0</span>)</span>
<span id="cb98-8"><a href="simple-net-tensors.html#cb98-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb98-9"><a href="simple-net-tensors.html#cb98-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute output (dim: 100 x 1)</span></span>
<span id="cb98-10"><a href="simple-net-tensors.html#cb98-10" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">mm</span>(w2) <span class="sc">+</span> b2</span></code></pre></div>
<p>Loss computation:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="simple-net-tensors.html#cb99-1" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>((y_pred <span class="sc">-</span> y)<span class="sc">$</span><span class="fu">pow</span>(<span class="dv">2</span>)<span class="sc">$</span><span class="fu">sum</span>())</span></code></pre></div>
<p>Backprop:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="simple-net-tensors.html#cb100-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. prediction (dim: 100 x 1)</span></span>
<span id="cb100-2"><a href="simple-net-tensors.html#cb100-2" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> (y_pred <span class="sc">-</span> y)</span>
<span id="cb100-3"><a href="simple-net-tensors.html#cb100-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. w2 (dim: 32 x 1)</span></span>
<span id="cb100-4"><a href="simple-net-tensors.html#cb100-4" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_y_pred)</span>
<span id="cb100-5"><a href="simple-net-tensors.html#cb100-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. hidden activation (dim: 100 x 32)</span></span>
<span id="cb100-6"><a href="simple-net-tensors.html#cb100-6" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="ot">&lt;-</span> grad_y_pred<span class="sc">$</span><span class="fu">mm</span>(w2<span class="sc">$</span><span class="fu">t</span>())</span>
<span id="cb100-7"><a href="simple-net-tensors.html#cb100-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)</span></span>
<span id="cb100-8"><a href="simple-net-tensors.html#cb100-8" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="ot">&lt;-</span> grad_h_relu<span class="sc">$</span><span class="fu">clone</span>()</span>
<span id="cb100-9"><a href="simple-net-tensors.html#cb100-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb100-10"><a href="simple-net-tensors.html#cb100-10" aria-hidden="true" tabindex="-1"></a>  grad_h[h <span class="sc">&lt;</span> <span class="dv">0</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb100-11"><a href="simple-net-tensors.html#cb100-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb100-12"><a href="simple-net-tensors.html#cb100-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. b2 (shape: ())</span></span>
<span id="cb100-13"><a href="simple-net-tensors.html#cb100-13" aria-hidden="true" tabindex="-1"></a>  grad_b2 <span class="ot">&lt;-</span> grad_y_pred<span class="sc">$</span><span class="fu">sum</span>()</span>
<span id="cb100-14"><a href="simple-net-tensors.html#cb100-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb100-15"><a href="simple-net-tensors.html#cb100-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. w1 (dim: 3 x 32)</span></span>
<span id="cb100-16"><a href="simple-net-tensors.html#cb100-16" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_h)</span>
<span id="cb100-17"><a href="simple-net-tensors.html#cb100-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. b1 (shape: (32, ))</span></span>
<span id="cb100-18"><a href="simple-net-tensors.html#cb100-18" aria-hidden="true" tabindex="-1"></a>  grad_b1 <span class="ot">&lt;-</span> grad_h<span class="sc">$</span><span class="fu">sum</span>(<span class="at">dim =</span> <span class="dv">1</span>)</span></code></pre></div>
<p>And weight updates:</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="simple-net-tensors.html#cb101-1" aria-hidden="true" tabindex="-1"></a>  learning_rate <span class="ot">&lt;-</span> <span class="fl">1e-4</span></span>
<span id="cb101-2"><a href="simple-net-tensors.html#cb101-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb101-3"><a href="simple-net-tensors.html#cb101-3" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">&lt;-</span> w2 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w2</span>
<span id="cb101-4"><a href="simple-net-tensors.html#cb101-4" aria-hidden="true" tabindex="-1"></a>  b2 <span class="ot">&lt;-</span> b2 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_b2</span>
<span id="cb101-5"><a href="simple-net-tensors.html#cb101-5" aria-hidden="true" tabindex="-1"></a>  w1 <span class="ot">&lt;-</span> w1 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w1</span>
<span id="cb101-6"><a href="simple-net-tensors.html#cb101-6" aria-hidden="true" tabindex="-1"></a>  b1 <span class="ot">&lt;-</span> b1 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_b1</span></code></pre></div>
<p>Finally, let’s put the pieces together.</p>
</div>
<div id="complete-network-using-torch-tensors" class="section level4" number="3.4.0.4">
<h4><span class="header-section-number">3.4.0.4</span> Complete network using <code>torch</code> tensors</h4>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="simple-net-tensors.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb102-2"><a href="simple-net-tensors.html#cb102-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-3"><a href="simple-net-tensors.html#cb102-3" aria-hidden="true" tabindex="-1"></a><span class="do">### generate training data -----------------------------------------------------</span></span>
<span id="cb102-4"><a href="simple-net-tensors.html#cb102-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-5"><a href="simple-net-tensors.html#cb102-5" aria-hidden="true" tabindex="-1"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb102-6"><a href="simple-net-tensors.html#cb102-6" aria-hidden="true" tabindex="-1"></a>d_in <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb102-7"><a href="simple-net-tensors.html#cb102-7" aria-hidden="true" tabindex="-1"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb102-8"><a href="simple-net-tensors.html#cb102-8" aria-hidden="true" tabindex="-1"></a>d_out <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb102-9"><a href="simple-net-tensors.html#cb102-9" aria-hidden="true" tabindex="-1"></a><span class="co"># number of observations in training set</span></span>
<span id="cb102-10"><a href="simple-net-tensors.html#cb102-10" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb102-11"><a href="simple-net-tensors.html#cb102-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-12"><a href="simple-net-tensors.html#cb102-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-13"><a href="simple-net-tensors.html#cb102-13" aria-hidden="true" tabindex="-1"></a><span class="co"># create random data</span></span>
<span id="cb102-14"><a href="simple-net-tensors.html#cb102-14" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(n, d_in)</span>
<span id="cb102-15"><a href="simple-net-tensors.html#cb102-15" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span></span>
<span id="cb102-16"><a href="simple-net-tensors.html#cb102-16" aria-hidden="true" tabindex="-1"></a>  x[, <span class="dv">1</span>, <span class="cn">NULL</span>] <span class="sc">*</span> <span class="fl">0.2</span> <span class="sc">-</span> x[, <span class="dv">2</span>, <span class="cn">NULL</span>] <span class="sc">*</span> <span class="fl">1.3</span> <span class="sc">-</span> x[, <span class="dv">3</span>, <span class="cn">NULL</span>] <span class="sc">*</span> <span class="fl">0.5</span> <span class="sc">+</span> <span class="fu">torch_randn</span>(n, <span class="dv">1</span>)</span>
<span id="cb102-17"><a href="simple-net-tensors.html#cb102-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-18"><a href="simple-net-tensors.html#cb102-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-19"><a href="simple-net-tensors.html#cb102-19" aria-hidden="true" tabindex="-1"></a><span class="do">### initialize weights ---------------------------------------------------------</span></span>
<span id="cb102-20"><a href="simple-net-tensors.html#cb102-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-21"><a href="simple-net-tensors.html#cb102-21" aria-hidden="true" tabindex="-1"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb102-22"><a href="simple-net-tensors.html#cb102-22" aria-hidden="true" tabindex="-1"></a>d_hidden <span class="ot">&lt;-</span> <span class="dv">32</span></span>
<span id="cb102-23"><a href="simple-net-tensors.html#cb102-23" aria-hidden="true" tabindex="-1"></a><span class="co"># weights connecting input to hidden layer</span></span>
<span id="cb102-24"><a href="simple-net-tensors.html#cb102-24" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(d_in, d_hidden)</span>
<span id="cb102-25"><a href="simple-net-tensors.html#cb102-25" aria-hidden="true" tabindex="-1"></a><span class="co"># weights connecting hidden to output layer</span></span>
<span id="cb102-26"><a href="simple-net-tensors.html#cb102-26" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(d_hidden, d_out)</span>
<span id="cb102-27"><a href="simple-net-tensors.html#cb102-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-28"><a href="simple-net-tensors.html#cb102-28" aria-hidden="true" tabindex="-1"></a><span class="co"># hidden layer bias</span></span>
<span id="cb102-29"><a href="simple-net-tensors.html#cb102-29" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="fu">torch_zeros</span>(<span class="dv">1</span>, d_hidden)</span>
<span id="cb102-30"><a href="simple-net-tensors.html#cb102-30" aria-hidden="true" tabindex="-1"></a><span class="co"># output layer bias</span></span>
<span id="cb102-31"><a href="simple-net-tensors.html#cb102-31" aria-hidden="true" tabindex="-1"></a>b2 <span class="ot">&lt;-</span> <span class="fu">torch_zeros</span>(<span class="dv">1</span>, d_out)</span>
<span id="cb102-32"><a href="simple-net-tensors.html#cb102-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-33"><a href="simple-net-tensors.html#cb102-33" aria-hidden="true" tabindex="-1"></a><span class="do">### network parameters ---------------------------------------------------------</span></span>
<span id="cb102-34"><a href="simple-net-tensors.html#cb102-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-35"><a href="simple-net-tensors.html#cb102-35" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="ot">&lt;-</span> <span class="fl">1e-4</span></span>
<span id="cb102-36"><a href="simple-net-tensors.html#cb102-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-37"><a href="simple-net-tensors.html#cb102-37" aria-hidden="true" tabindex="-1"></a><span class="do">### training loop --------------------------------------------------------------</span></span>
<span id="cb102-38"><a href="simple-net-tensors.html#cb102-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-39"><a href="simple-net-tensors.html#cb102-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>) {</span>
<span id="cb102-40"><a href="simple-net-tensors.html#cb102-40" aria-hidden="true" tabindex="-1"></a>  <span class="do">### -------- Forward pass --------</span></span>
<span id="cb102-41"><a href="simple-net-tensors.html#cb102-41" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-42"><a href="simple-net-tensors.html#cb102-42" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute pre-activations of hidden layers (dim: 100 x 32)</span></span>
<span id="cb102-43"><a href="simple-net-tensors.html#cb102-43" aria-hidden="true" tabindex="-1"></a>  h <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">mm</span>(w1) <span class="sc">+</span> b1</span>
<span id="cb102-44"><a href="simple-net-tensors.html#cb102-44" aria-hidden="true" tabindex="-1"></a>  <span class="co"># apply activation function (dim: 100 x 32)</span></span>
<span id="cb102-45"><a href="simple-net-tensors.html#cb102-45" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="ot">&lt;-</span> h<span class="sc">$</span><span class="fu">clamp</span>(<span class="at">min =</span> <span class="dv">0</span>)</span>
<span id="cb102-46"><a href="simple-net-tensors.html#cb102-46" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute output (dim: 100 x 1)</span></span>
<span id="cb102-47"><a href="simple-net-tensors.html#cb102-47" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">mm</span>(w2) <span class="sc">+</span> b2</span>
<span id="cb102-48"><a href="simple-net-tensors.html#cb102-48" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-49"><a href="simple-net-tensors.html#cb102-49" aria-hidden="true" tabindex="-1"></a>  <span class="do">### -------- compute loss --------</span></span>
<span id="cb102-50"><a href="simple-net-tensors.html#cb102-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-51"><a href="simple-net-tensors.html#cb102-51" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>((y_pred <span class="sc">-</span> y)<span class="sc">$</span><span class="fu">pow</span>(<span class="dv">2</span>)<span class="sc">$</span><span class="fu">sum</span>())</span>
<span id="cb102-52"><a href="simple-net-tensors.html#cb102-52" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-53"><a href="simple-net-tensors.html#cb102-53" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (t <span class="sc">%%</span> <span class="dv">10</span> <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb102-54"><a href="simple-net-tensors.html#cb102-54" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">&quot;Epoch: &quot;</span>, t, <span class="st">&quot;   Loss: &quot;</span>, loss, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb102-55"><a href="simple-net-tensors.html#cb102-55" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-56"><a href="simple-net-tensors.html#cb102-56" aria-hidden="true" tabindex="-1"></a>  <span class="do">### -------- Backpropagation --------</span></span>
<span id="cb102-57"><a href="simple-net-tensors.html#cb102-57" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-58"><a href="simple-net-tensors.html#cb102-58" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. prediction (dim: 100 x 1)</span></span>
<span id="cb102-59"><a href="simple-net-tensors.html#cb102-59" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> (y_pred <span class="sc">-</span> y)</span>
<span id="cb102-60"><a href="simple-net-tensors.html#cb102-60" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. w2 (dim: 32 x 1)</span></span>
<span id="cb102-61"><a href="simple-net-tensors.html#cb102-61" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_y_pred)</span>
<span id="cb102-62"><a href="simple-net-tensors.html#cb102-62" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. hidden activation (dim: 100 x 32)</span></span>
<span id="cb102-63"><a href="simple-net-tensors.html#cb102-63" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="ot">&lt;-</span> grad_y_pred<span class="sc">$</span><span class="fu">mm</span>(</span>
<span id="cb102-64"><a href="simple-net-tensors.html#cb102-64" aria-hidden="true" tabindex="-1"></a>    w2<span class="sc">$</span><span class="fu">t</span>())</span>
<span id="cb102-65"><a href="simple-net-tensors.html#cb102-65" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)</span></span>
<span id="cb102-66"><a href="simple-net-tensors.html#cb102-66" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="ot">&lt;-</span> grad_h_relu<span class="sc">$</span><span class="fu">clone</span>()</span>
<span id="cb102-67"><a href="simple-net-tensors.html#cb102-67" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-68"><a href="simple-net-tensors.html#cb102-68" aria-hidden="true" tabindex="-1"></a>  grad_h[h <span class="sc">&lt;</span> <span class="dv">0</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb102-69"><a href="simple-net-tensors.html#cb102-69" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-70"><a href="simple-net-tensors.html#cb102-70" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. b2 (shape: ())</span></span>
<span id="cb102-71"><a href="simple-net-tensors.html#cb102-71" aria-hidden="true" tabindex="-1"></a>  grad_b2 <span class="ot">&lt;-</span> grad_y_pred<span class="sc">$</span><span class="fu">sum</span>()</span>
<span id="cb102-72"><a href="simple-net-tensors.html#cb102-72" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-73"><a href="simple-net-tensors.html#cb102-73" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. w1 (dim: 3 x 32)</span></span>
<span id="cb102-74"><a href="simple-net-tensors.html#cb102-74" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_h)</span>
<span id="cb102-75"><a href="simple-net-tensors.html#cb102-75" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient of loss w.r.t. b1 (shape: (32, ))</span></span>
<span id="cb102-76"><a href="simple-net-tensors.html#cb102-76" aria-hidden="true" tabindex="-1"></a>  grad_b1 <span class="ot">&lt;-</span> grad_h<span class="sc">$</span><span class="fu">sum</span>(<span class="at">dim =</span> <span class="dv">1</span>)</span>
<span id="cb102-77"><a href="simple-net-tensors.html#cb102-77" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-78"><a href="simple-net-tensors.html#cb102-78" aria-hidden="true" tabindex="-1"></a>  <span class="do">### -------- Update weights --------</span></span>
<span id="cb102-79"><a href="simple-net-tensors.html#cb102-79" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-80"><a href="simple-net-tensors.html#cb102-80" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">&lt;-</span> w2 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w2</span>
<span id="cb102-81"><a href="simple-net-tensors.html#cb102-81" aria-hidden="true" tabindex="-1"></a>  b2 <span class="ot">&lt;-</span> b2 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_b2</span>
<span id="cb102-82"><a href="simple-net-tensors.html#cb102-82" aria-hidden="true" tabindex="-1"></a>  w1 <span class="ot">&lt;-</span> w1 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w1</span>
<span id="cb102-83"><a href="simple-net-tensors.html#cb102-83" aria-hidden="true" tabindex="-1"></a>  b1 <span class="ot">&lt;-</span> b1 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_b1</span>
<span id="cb102-84"><a href="simple-net-tensors.html#cb102-84" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-85"><a href="simple-net-tensors.html#cb102-85" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>Epoch:  10     Loss:  352.3585 
Epoch:  20     Loss:  219.3624 
Epoch:  30     Loss:  155.2307 
Epoch:  40     Loss:  124.5716 
Epoch:  50     Loss:  109.2687 
Epoch:  60     Loss:  100.1543 
Epoch:  70     Loss:  94.77817 
Epoch:  80     Loss:  91.57003 
Epoch:  90     Loss:  89.37974 
Epoch:  100    Loss:  87.64617 
Epoch:  110    Loss:  86.3077 
Epoch:  120    Loss:  85.25118 
Epoch:  130    Loss:  84.37959 
Epoch:  140    Loss:  83.44133 
Epoch:  150    Loss:  82.60386 
Epoch:  160    Loss:  81.85324 
Epoch:  170    Loss:  81.23454 
Epoch:  180    Loss:  80.68679 
Epoch:  190    Loss:  80.16555 
Epoch:  200    Loss:  79.67953 </code></pre>
<p>In the next section, we’ll make an important change, freeing us from having to think in detail about the backward pass.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>Although the assumption may be tempting, “contiguous” does not correspond to what we’d call “contiguous in memory” in casual language.<a href="simple-net-tensors.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>For correctness’ sake, <code>contiguous()</code> will only make a copy if the tensor it is called on is <em>not contiguous already.</em><a href="simple-net-tensors.html#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-net-R.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple-net-autograd.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
