eq:linreg
eq:normaleqs
eq:mse
eq:gradlosswrtbeta
eq:euler
introduction
introduction-1
simple_net_R
whats-in-a-network
gradient-descent
from-linear-regression-to-a-simple-network
implications-of-having-multiple-layers
activation-functions
weights-and-biases
a-simple-network
simulate-data
initialize-weights-and-biases
training-loop
complete-code
simple_net_tensors
tensors
creation
conversion-to-built-in-r-data-types
indexing-and-slicing-tensors
indexing-and-slicing-the-r-like-part
indexing-and-slicing-what-to-look-out-for
reshaping-tensors
zero-copy-reshaping
reshape-with-copy
operations-on-tensors
running-on-gpu
broadcasting
simple-neural-network-using-torch-tensors
toy-data
initialize-weights
training-loop-1
complete-network-using-torch-tensors
simple_net_autograd
automatic-differentiation-with-autograd
autograd-basics
digging-deeper
the-simple-network-now-using-autograd
simple_net_modules
modules
base-modules-layers
container-modules-models
simple-network-using-modules
simple_net_optim
losses-and-loss-functions
optimizers
simple-network-final-version
image_classification
data-loading-and-preprocessing
image-preprocessing-pipeline
data-loaders
some-birds
model
training
finding-an-optimally-efficient-learning-rate
training-loop-2
test-set-accuracy
unet
u-net
brain-image-segmentation
data
dataset
model-1
optimization
training-1
evaluation
seq2seq_att
why-attention
preprocessing-with-torchtext
model-2
encoder
attention-module
decoder
seq2seq-module
training-and-evaluation
results
transformer
attention-is-all-you-need
self-attention-and-multi-head-attention
overall-architecture
implementation-building-blocks
a-transformer-for-natural-language-translation
load-data
encoder-1
decoder-1
overall-model
results-1
tabular
agenda
dataset-1
model-3
training-2
evaluation-1
making-the-task-harder
a-look-at-the-hidden-representations
gans
dataset-2
model-4
generator
discriminator
optimizers-and-loss-function
training-loop-3
artifacts
vaes
dataset-3
model-5
training-the-vae
latent-space
