eq:linreg
eq:normaleqs
eq:mse
eq:gradlosswrtbeta
eq:euler
introduction
introduction-1
simple_net_R
whats-in-a-network
gradient-descent
from-linear-regression-to-a-simple-network
implications-of-having-multiple-layers
activation-functions
weights-and-biases
a-simple-network
simulate-data
initialize-weights-and-biases
training-loop
complete-code
simple_net_tensors
tensors
creation
conversion-to-built-in-r-data-types
indexing-and-slicing-tensors
indexing-and-slicing-the-r-like-part
indexing-and-slicing-what-to-look-out-for
reshaping-tensors
zero-copy-reshaping
reshape-with-copy
operations-on-tensors
running-on-gpu
broadcasting
simple-neural-network-using-torch-tensors
toy-data
initialize-weights
training-loop-1
complete-network-using-torch-tensors
simple_net_autograd
automatic-differentiation-with-autograd
autograd-basics
digging-deeper
the-simple-network-now-using-autograd
simple_net_modules
modules
base-modules-layers
container-modules-models
simple-network-using-modules
simple_net_optim
losses-and-loss-functions
optimizers
simple-network-final-version
image_classification
data-loading-and-transformation
model
training
performance-on-the-test-set
unet
image-segmentation-in-a-nutshell
u-net
example-application-mri-images
preprocessing
image-preprocessing-and-transforms
brainsegmentationdataset
loading-the-data
u-net-model
loss
training-1
predictions
seq2seq_att
why-attention
preprocessing-with-torchtext
model-1
encoder
attention-module
decoder
seq2seq-module
training-and-evaluation
results
transformer
attention-is-all-you-need
self-attention-and-multi-head-attention
overall-architecture
implementation-building-blocks
a-transformer-for-natural-language-translation
load-data
encoder-1
decoder-1
overall-model
results-1
gans
dataset
model-2
generator
discriminator
optimizers-and-loss-function
training-loop-2
artifacts
vaes
dataset-1
model-3
training-the-vae
latent-space
