<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Using torch modules | Applied deep learning with torch from R</title>
  <meta name="description" content="tbd" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Using torch modules | Applied deep learning with torch from R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="tbd" />
  <meta property="og:image" content="tbdcover.png" />
  <meta property="og:description" content="tbd" />
  <meta name="github-repo" content="tbd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Using torch modules | Applied deep learning with torch from R" />
  
  <meta name="twitter:description" content="tbd" />
  <meta name="twitter:image" content="tbdcover.png" />

<meta name="author" content="tbd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-net-autograd.html"/>
<link rel="next" href="simple-net-optim.html"/>
<script src="libs/header-attrs-2.4.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
</ul></li>
<li class="part"><span><b>I Using Torch</b></span></li>
<li class="chapter" data-level="" data-path="using-torch-intro.html"><a href="using-torch-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="2" data-path="simple-net-R.html"><a href="simple-net-R.html"><i class="fa fa-check"></i><b>2</b> A simple neural network in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#whats-in-a-network"><i class="fa fa-check"></i><b>2.1</b> What’s in a network?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="simple-net-R.html"><a href="simple-net-R.html#gradient-descent"><i class="fa fa-check"></i><b>2.1.1</b> Gradient descent</a></li>
<li class="chapter" data-level="2.1.2" data-path="simple-net-R.html"><a href="simple-net-R.html#from-linear-regression-to-a-simple-network"><i class="fa fa-check"></i><b>2.1.2</b> From linear regression to a simple network</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#a-simple-network"><i class="fa fa-check"></i><b>2.2</b> A simple network</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#simulate-data"><i class="fa fa-check"></i><b>2.2.1</b> Simulate data</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#initialize-weights-and-biases"><i class="fa fa-check"></i><b>2.2.2</b> Initialize weights and biases</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>2.2.3</b> Training loop</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple-net-R.html"><a href="simple-net-R.html#complete-code"><i class="fa fa-check"></i><b>2.2.4</b> Complete code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html"><i class="fa fa-check"></i><b>3</b> Modifying the simple network to use torch tensors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#tensors"><i class="fa fa-check"></i><b>3.1</b> Tensors</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#creation"><i class="fa fa-check"></i><b>3.1.1</b> Creation</a></li>
<li class="chapter" data-level="3.1.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#conversion-to-built-in-r-data-types"><i class="fa fa-check"></i><b>3.1.2</b> Conversion to built-in R data types</a></li>
<li class="chapter" data-level="3.1.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#indexing-and-slicing-tensors"><i class="fa fa-check"></i><b>3.1.3</b> Indexing and slicing tensors</a></li>
<li class="chapter" data-level="3.1.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#reshaping-tensors"><i class="fa fa-check"></i><b>3.1.4</b> Reshaping tensors</a></li>
<li class="chapter" data-level="3.1.5" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#operations-on-tensors"><i class="fa fa-check"></i><b>3.1.5</b> Operations on tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#running-on-gpu"><i class="fa fa-check"></i><b>3.2</b> Running on GPU</a></li>
<li class="chapter" data-level="3.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#broadcasting"><i class="fa fa-check"></i><b>3.3</b> Broadcasting</a></li>
<li class="chapter" data-level="3.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#simple-neural-network-using-torch-tensors"><i class="fa fa-check"></i><b>3.4</b> Simple neural network using <code>torch</code> tensors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html"><i class="fa fa-check"></i><b>4</b> Using autograd</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#automatic-differentiation-with-autograd"><i class="fa fa-check"></i><b>4.1</b> Automatic differentiation with <em>autograd</em></a></li>
<li class="chapter" data-level="4.2" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#the-simple-network-now-using-autograd"><i class="fa fa-check"></i><b>4.2</b> The simple network, now using <em>autograd</em></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple-net-modules.html"><a href="simple-net-modules.html"><i class="fa fa-check"></i><b>5</b> Using torch modules</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#modules"><i class="fa fa-check"></i><b>5.1</b> Modules</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#base-modules-layers"><i class="fa fa-check"></i><b>5.1.1</b> Base modules (“layers”)</a></li>
<li class="chapter" data-level="5.1.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#container-modules-models"><i class="fa fa-check"></i><b>5.1.2</b> Container modules (“models”)</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#simple-network-using-modules"><i class="fa fa-check"></i><b>5.2</b> Simple network using modules</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="simple-net-optim.html"><a href="simple-net-optim.html"><i class="fa fa-check"></i><b>6</b> Using <code>torch</code> optimizers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="simple-net-optim.html"><a href="simple-net-optim.html#losses-and-loss-functions"><i class="fa fa-check"></i><b>6.1</b> Losses and loss functions</a></li>
<li class="chapter" data-level="6.2" data-path="simple-net-optim.html"><a href="simple-net-optim.html#optimizers"><i class="fa fa-check"></i><b>6.2</b> Optimizers</a></li>
<li class="chapter" data-level="6.3" data-path="simple-net-optim.html"><a href="simple-net-optim.html#simple-network-final-version"><i class="fa fa-check"></i><b>6.3</b> Simple network: final version</a></li>
</ul></li>
<li class="part"><span><b>II Image Recognition</b></span></li>
<li class="chapter" data-level="" data-path="image-recognition-intro.html"><a href="image-recognition-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="7" data-path="image-classification.html"><a href="image-classification.html"><i class="fa fa-check"></i><b>7</b> Classifying images</a>
<ul>
<li class="chapter" data-level="7.1" data-path="image-classification.html"><a href="image-classification.html#data-loading-and-transformation"><i class="fa fa-check"></i><b>7.1</b> Data loading and transformation</a></li>
<li class="chapter" data-level="7.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>7.2</b> Model</a></li>
<li class="chapter" data-level="7.3" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>7.3</b> Training</a></li>
<li class="chapter" data-level="7.4" data-path="image-classification.html"><a href="image-classification.html#performance-on-the-test-set"><i class="fa fa-check"></i><b>7.4</b> Performance on the test set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="unet.html"><a href="unet.html"><i class="fa fa-check"></i><b>8</b> Brain Image Segmentation with U-Net</a>
<ul>
<li class="chapter" data-level="8.1" data-path="unet.html"><a href="unet.html#image-segmentation-in-a-nutshell"><i class="fa fa-check"></i><b>8.1</b> Image segmentation in a nutshell</a></li>
<li class="chapter" data-level="8.2" data-path="unet.html"><a href="unet.html#u-net"><i class="fa fa-check"></i><b>8.2</b> U-Net</a></li>
<li class="chapter" data-level="8.3" data-path="unet.html"><a href="unet.html#example-application-mri-images"><i class="fa fa-check"></i><b>8.3</b> Example application: MRI images</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="unet.html"><a href="unet.html#preprocessing"><i class="fa fa-check"></i><b>8.3.1</b> Preprocessing</a></li>
<li class="chapter" data-level="8.3.2" data-path="unet.html"><a href="unet.html#u-net-model"><i class="fa fa-check"></i><b>8.3.2</b> U-Net model</a></li>
<li class="chapter" data-level="8.3.3" data-path="unet.html"><a href="unet.html#loss"><i class="fa fa-check"></i><b>8.3.3</b> Loss</a></li>
<li class="chapter" data-level="8.3.4" data-path="unet.html"><a href="unet.html#training-1"><i class="fa fa-check"></i><b>8.3.4</b> Training</a></li>
<li class="chapter" data-level="8.3.5" data-path="unet.html"><a href="unet.html#predictions"><i class="fa fa-check"></i><b>8.3.5</b> Predictions</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Natural language processing</b></span></li>
<li class="chapter" data-level="" data-path="NLP-intro.html"><a href="NLP-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="9" data-path="seq2seq-att.html"><a href="seq2seq-att.html"><i class="fa fa-check"></i><b>9</b> Sequence-to-sequence models with attention</a>
<ul>
<li class="chapter" data-level="9.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#why-attention"><i class="fa fa-check"></i><b>9.1</b> Why attention?</a></li>
<li class="chapter" data-level="9.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#preprocessing-with-torchtext"><i class="fa fa-check"></i><b>9.2</b> Preprocessing with <code>torchtext</code></a></li>
<li class="chapter" data-level="9.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#model-1"><i class="fa fa-check"></i><b>9.3</b> Model</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>9.3.1</b> Encoder</a></li>
<li class="chapter" data-level="9.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#attention-module"><i class="fa fa-check"></i><b>9.3.2</b> Attention module</a></li>
<li class="chapter" data-level="9.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>9.3.3</b> Decoder</a></li>
<li class="chapter" data-level="9.3.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#seq2seq-module"><i class="fa fa-check"></i><b>9.3.4</b> Seq2seq module</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#training-and-evaluation"><i class="fa fa-check"></i><b>9.4</b> Training and evaluation</a></li>
<li class="chapter" data-level="9.5" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>9.5</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="transformer.html"><a href="transformer.html"><i class="fa fa-check"></i><b>10</b> Torch transformer modules</a>
<ul>
<li class="chapter" data-level="10.1" data-path="transformer.html"><a href="transformer.html#attention-is-all-you-need"><i class="fa fa-check"></i><b>10.1</b> “Attention is all you need”</a></li>
<li class="chapter" data-level="10.2" data-path="transformer.html"><a href="transformer.html#implementation-building-blocks"><i class="fa fa-check"></i><b>10.2</b> Implementation: building blocks</a></li>
<li class="chapter" data-level="10.3" data-path="transformer.html"><a href="transformer.html#a-transformer-for-natural-language-translation"><i class="fa fa-check"></i><b>10.3</b> A Transformer for natural language translation</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="transformer.html"><a href="transformer.html#load-data"><i class="fa fa-check"></i><b>10.3.1</b> Load data</a></li>
<li class="chapter" data-level="10.3.2" data-path="transformer.html"><a href="transformer.html#encoder-1"><i class="fa fa-check"></i><b>10.3.2</b> Encoder</a></li>
<li class="chapter" data-level="10.3.3" data-path="transformer.html"><a href="transformer.html#decoder-1"><i class="fa fa-check"></i><b>10.3.3</b> Decoder</a></li>
<li class="chapter" data-level="10.3.4" data-path="transformer.html"><a href="transformer.html#overall-model"><i class="fa fa-check"></i><b>10.3.4</b> Overall model</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="transformer.html"><a href="transformer.html#results-1"><i class="fa fa-check"></i><b>10.4</b> Results</a></li>
</ul></li>
<li class="part"><span><b>IV Deep learning for tabular data</b></span></li>
<li class="chapter" data-level="" data-path="tabular-intro.html"><a href="tabular-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>V Time series</b></span></li>
<li class="chapter" data-level="" data-path="timeseries-intro.html"><a href="timeseries-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VI Generative deep learning</b></span></li>
<li class="chapter" data-level="" data-path="generative-intro.html"><a href="generative-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="11" data-path="gans.html"><a href="gans.html"><i class="fa fa-check"></i><b>11</b> Generative adversarial networks</a>
<ul>
<li class="chapter" data-level="11.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>11.1</b> Dataset</a></li>
<li class="chapter" data-level="11.2" data-path="gans.html"><a href="gans.html#model-2"><i class="fa fa-check"></i><b>11.2</b> Model</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="gans.html"><a href="gans.html#generator"><i class="fa fa-check"></i><b>11.2.1</b> Generator</a></li>
<li class="chapter" data-level="11.2.2" data-path="gans.html"><a href="gans.html#discriminator"><i class="fa fa-check"></i><b>11.2.2</b> Discriminator</a></li>
<li class="chapter" data-level="11.2.3" data-path="gans.html"><a href="gans.html#optimizers-and-loss-function"><i class="fa fa-check"></i><b>11.2.3</b> Optimizers and loss function</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="gans.html"><a href="gans.html#training-loop-2"><i class="fa fa-check"></i><b>11.3</b> Training loop</a></li>
<li class="chapter" data-level="11.4" data-path="gans.html"><a href="gans.html#artifacts"><i class="fa fa-check"></i><b>11.4</b> Artifacts</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="vaes.html"><a href="vaes.html"><i class="fa fa-check"></i><b>12</b> Variational autoencoders</a>
<ul>
<li class="chapter" data-level="12.1" data-path="vaes.html"><a href="vaes.html#dataset-1"><i class="fa fa-check"></i><b>12.1</b> Dataset</a></li>
<li class="chapter" data-level="12.2" data-path="vaes.html"><a href="vaes.html#model-3"><i class="fa fa-check"></i><b>12.2</b> Model</a></li>
<li class="chapter" data-level="12.3" data-path="vaes.html"><a href="vaes.html#training-the-vae"><i class="fa fa-check"></i><b>12.3</b> Training the VAE</a></li>
<li class="chapter" data-level="12.4" data-path="vaes.html"><a href="vaes.html#latent-space"><i class="fa fa-check"></i><b>12.4</b> Latent space</a></li>
</ul></li>
<li class="part"><span><b>VII Deep learning on graphs</b></span></li>
<li class="chapter" data-level="" data-path="graph-intro.html"><a href="graph-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VIII Probabilistic deep learning</b></span></li>
<li class="chapter" data-level="" data-path="probabilistic-intro.html"><a href="probabilistic-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>IX Private and secure deep learning</b></span></li>
<li class="chapter" data-level="" data-path="private-secure-intro.html"><a href="private-secure-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>X Research topics</b></span></li>
<li class="chapter" data-level="" data-path="research-intro.html"><a href="research-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied deep learning with torch from R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple_net_modules" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Using torch modules</h1>
<p>Initially, we started learning about <code>torch</code> basics by coding a simple
neural network from scratch, making use of just a single of <code>torch</code>’s
features: <em>tensors</em>. Then, we immensely simplified the task, replacing
manual backpropagation with <em>autograd</em>. In this section, we <em>modularize</em>
the network - in both the habitual and a very literal sense: Low-level
matrix operations are swapped out for <code>torch</code> <code>module</code>s.</p>
<div id="modules" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Modules</h2>
<p>From other frameworks (Keras, say), you may be used to distinguishing
between <em>models</em> and <em>layers</em>. In <code>torch</code>, both are instances of
<code>nn_Module()</code>, and thus, have some methods in common. For those thinking
in terms of “models” and “layers,” I’m artificially splitting up this
section into two parts. In reality though, there is no dichotomy: New
modules may be composed of existing ones up to arbitrary levels of
recursion.</p>
<div id="base-modules-layers" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Base modules (“layers”)</h3>
<p>Instead of writing out an affine operation by hand – <code>x$mm(w1) + b1</code>,
say –, as we’ve been doing so far, we can create a linear module. The
following snippet instantiates a linear layer that expects three-feature
inputs and returns a single output per observation:</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="simple-net-modules.html#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb129-2"><a href="simple-net-modules.html#cb129-2" aria-hidden="true" tabindex="-1"></a>l <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(<span class="dv">3</span>, <span class="dv">1</span>)</span></code></pre></div>
<p>The module has two parameters, “weight” and “bias.” Both now come
pre-initialized:</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="simple-net-modules.html#cb130-1" aria-hidden="true" tabindex="-1"></a>l<span class="sc">$</span>parameters</span></code></pre></div>
<pre><code>$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]</code></pre>
<p>Modules are callable; calling a module executes its <code>forward()</code> method,
which, for a linear layer, matrix-multiplies input and weights, and adds
the bias.</p>
<p>Let’s try this:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="simple-net-modules.html#cb132-1" aria-hidden="true" tabindex="-1"></a>data  <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">10</span>, <span class="dv">3</span>)</span>
<span id="cb132-2"><a href="simple-net-modules.html#cb132-2" aria-hidden="true" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">l</span>(data)</span></code></pre></div>
<p>Unsurprisingly, <code>out</code> now holds some data:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="simple-net-modules.html#cb133-1" aria-hidden="true" tabindex="-1"></a>out<span class="sc">$</span><span class="fu">data</span>()</span></code></pre></div>
<pre><code>torch_tensor 
 0.2711
-1.8151
-0.0073
 0.1876
-0.0930
 0.7498
-0.2332
-0.0428
 0.3849
-0.2618
[ CPUFloatType{10,1} ]</code></pre>
<p>In addition though, this tensor knows what will need to be done, should
ever it be asked to calculate gradients:</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="simple-net-modules.html#cb135-1" aria-hidden="true" tabindex="-1"></a>out<span class="sc">$</span>grad_fn</span></code></pre></div>
<pre><code>AddmmBackward</code></pre>
<p>Note the difference between tensors returned by modules and self-created
ones. When creating tensors ourselves, we need to pass
<code>requires_grad = TRUE</code> to trigger gradient calculation. With modules,
<code>torch</code> correctly assumes that we’ll want to perform backpropagation at
some point.</p>
<p>By now though, we haven’t called <code>backward()</code> yet. Thus, no gradients
have yet been computed:</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="simple-net-modules.html#cb137-1" aria-hidden="true" tabindex="-1"></a>l<span class="sc">$</span>weight<span class="sc">$</span>grad</span>
<span id="cb137-2"><a href="simple-net-modules.html#cb137-2" aria-hidden="true" tabindex="-1"></a>l<span class="sc">$</span>bias<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>torch_tensor 
[ Tensor (undefined) ]
torch_tensor 
[ Tensor (undefined) ]</code></pre>
<p>Let’s change this:</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="simple-net-modules.html#cb139-1" aria-hidden="true" tabindex="-1"></a>out<span class="sc">$</span><span class="fu">backward</span>()</span></code></pre></div>
<pre><code>Error in (function (self, gradient, keep_graph, create_graph)  : 
  grad can be implicitly created only for scalar outputs (_make_grads at ../torch/csrc/autograd/autograd.cpp:47)</code></pre>
<p>Why the error? <em>Autograd</em> expects the output tensor to be a scalar,
while in our example, we have a tensor of size <code>(10, 1)</code>. This error
won’t often occur in practice, where we work with <em>batches</em> of inputs
(sometimes, just a single batch). But still, it’s interesting to see how
to resolve this.</p>
<p>To make the example work, we introduce a – virtual – final aggregation
step – taking the mean, say. Let’s call it <code>avg</code>. If such a mean were
taken, its gradient with respect to <code>l$weight</code> would be obtained via the
chain rule:</p>
<p><span class="math display">\[
\begin{equation*} 
 \frac{\partial \ avg}{\partial w} = \frac{\partial \ avg}{\partial \ out}  \ \frac{\partial \ out}{\partial w}
\end{equation*}
\]</span></p>
<p>Of the quantities on the right side, we’re interested in the second. We
need to provide the first one, the way it would look <em>if really we were
taking the mean</em>:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="simple-net-modules.html#cb141-1" aria-hidden="true" tabindex="-1"></a>d_avg_d_out <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="dv">10</span>)<span class="sc">$</span><span class="st">`</span><span class="at">repeat</span><span class="st">`</span>(<span class="dv">10</span>)<span class="sc">$</span><span class="fu">unsqueeze</span>(<span class="dv">1</span>)<span class="sc">$</span><span class="fu">t</span>()</span>
<span id="cb141-2"><a href="simple-net-modules.html#cb141-2" aria-hidden="true" tabindex="-1"></a>out<span class="sc">$</span><span class="fu">backward</span>(<span class="at">gradient =</span> d_avg_d_out)</span></code></pre></div>
<p>Now, <code>l$weight$grad</code> and <code>l$bias$grad</code> <em>do</em> contain gradients:</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="simple-net-modules.html#cb142-1" aria-hidden="true" tabindex="-1"></a>l<span class="sc">$</span>weight<span class="sc">$</span>grad</span>
<span id="cb142-2"><a href="simple-net-modules.html#cb142-2" aria-hidden="true" tabindex="-1"></a>l<span class="sc">$</span>bias<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>torch_tensor 
 1.3410  6.4343 -30.7135
[ CPUFloatType{1,3} ]
torch_tensor 
 100
[ CPUFloatType{1} ]</code></pre>
<p>In addition to <code>nn_linear()</code> , <code>torch</code> provides pretty much all the
common layers you might hope for. But few tasks are solved by a single
layer. How do you combine them? Or, in the usual lingo: How do you build
<em>models</em>?</p>
</div>
<div id="container-modules-models" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Container modules (“models”)</h3>
<p>Now, <em>models</em> are just modules that contain other modules. For example,
if all inputs are supposed to flow through the same nodes and along the
same edges, then <code>nn_sequential()</code> can be used to build a simple graph.</p>
<p>For example:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="simple-net-modules.html#cb144-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb144-2"><a href="simple-net-modules.html#cb144-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nn_linear</span>(<span class="dv">3</span>, <span class="dv">16</span>),</span>
<span id="cb144-3"><a href="simple-net-modules.html#cb144-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nn_relu</span>(),</span>
<span id="cb144-4"><a href="simple-net-modules.html#cb144-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nn_linear</span>(<span class="dv">16</span>, <span class="dv">1</span>)</span>
<span id="cb144-5"><a href="simple-net-modules.html#cb144-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>We can use the same technique as above to get an overview of all model
parameters (two weight matrices and two bias vectors):</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="simple-net-modules.html#cb145-1" aria-hidden="true" tabindex="-1"></a>model<span class="sc">$</span>parameters</span></code></pre></div>
<pre><code>$`0.weight`
torch_tensor 
-0.1968 -0.1127 -0.0504
 0.0083  0.3125  0.0013
 0.4784 -0.2757  0.2535
-0.0898 -0.4706 -0.0733
-0.0654  0.5016  0.0242
 0.4855 -0.3980 -0.3434
-0.3609  0.1859 -0.4039
 0.2851  0.2809 -0.3114
-0.0542 -0.0754 -0.2252
-0.3175  0.2107 -0.2954
-0.3733  0.3931  0.3466
 0.5616 -0.3793 -0.4872
 0.0062  0.4168 -0.5580
 0.3174 -0.4867  0.0904
-0.0981 -0.0084  0.3580
 0.3187 -0.2954 -0.5181
[ CPUFloatType{16,3} ]

$`0.bias`
torch_tensor 
-0.3714
 0.5603
-0.3791
 0.4372
-0.1793
-0.3329
 0.5588
 0.1370
 0.4467
 0.2937
 0.1436
 0.1986
 0.4967
 0.1554
-0.3219
-0.0266
[ CPUFloatType{16} ]

$`2.weight`
torch_tensor 
Columns 1 to 10-0.0908 -0.1786  0.0812 -0.0414 -0.0251 -0.1961  0.2326  0.0943 -0.0246  0.0748

Columns 11 to 16 0.2111 -0.1801 -0.0102 -0.0244  0.1223 -0.1958
[ CPUFloatType{1,16} ]

$`2.bias`
torch_tensor 
 0.2470
[ CPUFloatType{1} ]</code></pre>
<p>To inspect an individual parameter, make use of its position in the
sequential model. For example:</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="simple-net-modules.html#cb147-1" aria-hidden="true" tabindex="-1"></a>model[[<span class="dv">1</span>]]<span class="sc">$</span>bias</span></code></pre></div>
<pre><code>torch_tensor 
-0.3714
 0.5603
-0.3791
 0.4372
-0.1793
-0.3329
 0.5588
 0.1370
 0.4467
 0.2937
 0.1436
 0.1986
 0.4967
 0.1554
-0.3219
-0.0266
[ CPUFloatType{16} ]</code></pre>
<p>And just like <code>nn_linear()</code> above, this module can be called directly on
data:</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="simple-net-modules.html#cb149-1" aria-hidden="true" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">model</span>(data)</span></code></pre></div>
<p>On a composite module like this one, calling <code>backward()</code> will
backpropagate through all the layers:</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="simple-net-modules.html#cb150-1" aria-hidden="true" tabindex="-1"></a>out<span class="sc">$</span><span class="fu">backward</span>(<span class="at">gradient =</span> <span class="fu">torch_tensor</span>(<span class="dv">10</span>)<span class="sc">$</span><span class="st">`</span><span class="at">repeat</span><span class="st">`</span>(<span class="dv">10</span>)<span class="sc">$</span><span class="fu">unsqueeze</span>(<span class="dv">1</span>)<span class="sc">$</span><span class="fu">t</span>())</span>
<span id="cb150-2"><a href="simple-net-modules.html#cb150-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-3"><a href="simple-net-modules.html#cb150-3" aria-hidden="true" tabindex="-1"></a><span class="co"># e.g.</span></span>
<span id="cb150-4"><a href="simple-net-modules.html#cb150-4" aria-hidden="true" tabindex="-1"></a>model[[<span class="dv">1</span>]]<span class="sc">$</span>bias<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>torch_tensor 
  0.0000
-17.8578
  1.6246
 -3.7258
 -0.2515
 -5.8825
 23.2624
  8.4903
 -2.4604
  6.7286
 14.7760
-14.4064
 -1.0206
 -1.7058
  0.0000
 -9.7897
[ CPUFloatType{16} ]</code></pre>
<p>And placing the composite module on the GPU will move all tensors there:</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="simple-net-modules.html#cb152-1" aria-hidden="true" tabindex="-1"></a>model<span class="sc">$</span><span class="fu">cuda</span>()</span>
<span id="cb152-2"><a href="simple-net-modules.html#cb152-2" aria-hidden="true" tabindex="-1"></a>model[[<span class="dv">1</span>]]<span class="sc">$</span>bias<span class="sc">$</span>grad</span></code></pre></div>
<pre><code>torch_tensor 
  0.0000
-17.8578
  1.6246
 -3.7258
 -0.2515
 -5.8825
 23.2624
  8.4903
 -2.4604
  6.7286
 14.7760
-14.4064
 -1.0206
 -1.7058
  0.0000
 -9.7897
[ CUDAFloatType{16} ]</code></pre>
<p>Now let’s see how using <code>nn_sequential()</code> can simplify our example
network.</p>
</div>
</div>
<div id="simple-network-using-modules" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Simple network using modules</h2>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="simple-net-modules.html#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="do">### generate training data -----------------------------------------------------</span></span>
<span id="cb154-2"><a href="simple-net-modules.html#cb154-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-3"><a href="simple-net-modules.html#cb154-3" aria-hidden="true" tabindex="-1"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb154-4"><a href="simple-net-modules.html#cb154-4" aria-hidden="true" tabindex="-1"></a>d_in <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb154-5"><a href="simple-net-modules.html#cb154-5" aria-hidden="true" tabindex="-1"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb154-6"><a href="simple-net-modules.html#cb154-6" aria-hidden="true" tabindex="-1"></a>d_out <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb154-7"><a href="simple-net-modules.html#cb154-7" aria-hidden="true" tabindex="-1"></a><span class="co"># number of observations in training set</span></span>
<span id="cb154-8"><a href="simple-net-modules.html#cb154-8" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb154-9"><a href="simple-net-modules.html#cb154-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-10"><a href="simple-net-modules.html#cb154-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-11"><a href="simple-net-modules.html#cb154-11" aria-hidden="true" tabindex="-1"></a><span class="co"># create random data</span></span>
<span id="cb154-12"><a href="simple-net-modules.html#cb154-12" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(n, d_in)</span>
<span id="cb154-13"><a href="simple-net-modules.html#cb154-13" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> x[, <span class="dv">1</span>, <span class="cn">NULL</span>] <span class="sc">*</span> <span class="fl">0.2</span> <span class="sc">-</span> x[, <span class="dv">2</span>, <span class="cn">NULL</span>] <span class="sc">*</span> <span class="fl">1.3</span> <span class="sc">-</span> x[, <span class="dv">3</span>, <span class="cn">NULL</span>] <span class="sc">*</span> <span class="fl">0.5</span> <span class="sc">+</span> <span class="fu">torch_randn</span>(n, <span class="dv">1</span>)</span>
<span id="cb154-14"><a href="simple-net-modules.html#cb154-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-15"><a href="simple-net-modules.html#cb154-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-16"><a href="simple-net-modules.html#cb154-16" aria-hidden="true" tabindex="-1"></a><span class="do">### define the network ---------------------------------------------------------</span></span>
<span id="cb154-17"><a href="simple-net-modules.html#cb154-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-18"><a href="simple-net-modules.html#cb154-18" aria-hidden="true" tabindex="-1"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb154-19"><a href="simple-net-modules.html#cb154-19" aria-hidden="true" tabindex="-1"></a>d_hidden <span class="ot">&lt;-</span> <span class="dv">32</span></span>
<span id="cb154-20"><a href="simple-net-modules.html#cb154-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-21"><a href="simple-net-modules.html#cb154-21" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb154-22"><a href="simple-net-modules.html#cb154-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(d_in, d_hidden),</span>
<span id="cb154-23"><a href="simple-net-modules.html#cb154-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(),</span>
<span id="cb154-24"><a href="simple-net-modules.html#cb154-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(d_hidden, d_out)</span>
<span id="cb154-25"><a href="simple-net-modules.html#cb154-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb154-26"><a href="simple-net-modules.html#cb154-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-27"><a href="simple-net-modules.html#cb154-27" aria-hidden="true" tabindex="-1"></a><span class="do">### network parameters ---------------------------------------------------------</span></span>
<span id="cb154-28"><a href="simple-net-modules.html#cb154-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-29"><a href="simple-net-modules.html#cb154-29" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="ot">&lt;-</span> <span class="fl">1e-4</span></span>
<span id="cb154-30"><a href="simple-net-modules.html#cb154-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-31"><a href="simple-net-modules.html#cb154-31" aria-hidden="true" tabindex="-1"></a><span class="do">### training loop --------------------------------------------------------------</span></span>
<span id="cb154-32"><a href="simple-net-modules.html#cb154-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-33"><a href="simple-net-modules.html#cb154-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>) {</span>
<span id="cb154-34"><a href="simple-net-modules.html#cb154-34" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb154-35"><a href="simple-net-modules.html#cb154-35" aria-hidden="true" tabindex="-1"></a>  <span class="do">### -------- Forward pass -------- </span></span>
<span id="cb154-36"><a href="simple-net-modules.html#cb154-36" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb154-37"><a href="simple-net-modules.html#cb154-37" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">&lt;-</span> <span class="fu">model</span>(x)</span>
<span id="cb154-38"><a href="simple-net-modules.html#cb154-38" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb154-39"><a href="simple-net-modules.html#cb154-39" aria-hidden="true" tabindex="-1"></a>  <span class="do">### -------- compute loss -------- </span></span>
<span id="cb154-40"><a href="simple-net-modules.html#cb154-40" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">&lt;-</span> (y_pred <span class="sc">-</span> y)<span class="sc">$</span><span class="fu">pow</span>(<span class="dv">2</span>)<span class="sc">$</span><span class="fu">sum</span>()</span>
<span id="cb154-41"><a href="simple-net-modules.html#cb154-41" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (t <span class="sc">%%</span> <span class="dv">10</span> <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb154-42"><a href="simple-net-modules.html#cb154-42" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">&quot;Epoch: &quot;</span>, t, <span class="st">&quot;   Loss: &quot;</span>, loss<span class="sc">$</span><span class="fu">item</span>(), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb154-43"><a href="simple-net-modules.html#cb154-43" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb154-44"><a href="simple-net-modules.html#cb154-44" aria-hidden="true" tabindex="-1"></a>  <span class="do">### -------- Backpropagation -------- </span></span>
<span id="cb154-45"><a href="simple-net-modules.html#cb154-45" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb154-46"><a href="simple-net-modules.html#cb154-46" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Zero the gradients before running the backward pass.</span></span>
<span id="cb154-47"><a href="simple-net-modules.html#cb154-47" aria-hidden="true" tabindex="-1"></a>  model<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb154-48"><a href="simple-net-modules.html#cb154-48" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb154-49"><a href="simple-net-modules.html#cb154-49" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute gradient of the loss w.r.t. all learnable parameters of the model</span></span>
<span id="cb154-50"><a href="simple-net-modules.html#cb154-50" aria-hidden="true" tabindex="-1"></a>  loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb154-51"><a href="simple-net-modules.html#cb154-51" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb154-52"><a href="simple-net-modules.html#cb154-52" aria-hidden="true" tabindex="-1"></a>  <span class="do">### -------- Update weights -------- </span></span>
<span id="cb154-53"><a href="simple-net-modules.html#cb154-53" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb154-54"><a href="simple-net-modules.html#cb154-54" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Wrap in with_no_grad() because this is a part we DON&#39;T want to record</span></span>
<span id="cb154-55"><a href="simple-net-modules.html#cb154-55" aria-hidden="true" tabindex="-1"></a>  <span class="co"># for automatic gradient computation</span></span>
<span id="cb154-56"><a href="simple-net-modules.html#cb154-56" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update each parameter by its `grad`</span></span>
<span id="cb154-57"><a href="simple-net-modules.html#cb154-57" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb154-58"><a href="simple-net-modules.html#cb154-58" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with_no_grad</span>({</span>
<span id="cb154-59"><a href="simple-net-modules.html#cb154-59" aria-hidden="true" tabindex="-1"></a>    model<span class="sc">$</span>parameters <span class="sc">%&gt;%</span> purrr<span class="sc">::</span><span class="fu">walk</span>(<span class="cf">function</span>(param) param<span class="sc">$</span><span class="fu">sub_</span>(learning_rate <span class="sc">*</span> param<span class="sc">$</span>grad))</span>
<span id="cb154-60"><a href="simple-net-modules.html#cb154-60" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb154-61"><a href="simple-net-modules.html#cb154-61" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb154-62"><a href="simple-net-modules.html#cb154-62" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The forward pass looks a lot better now; however, we still loop through
the model’s parameters and update each one by hand. Furthermore, you may
be already be suspecting that <code>torch</code> provides abstractions for common
loss functions. In the next section, we’ll address both points, making
use of <code>torch</code> losses and optimizers.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-net-autograd.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple-net-optim.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
