<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 A simple neural network in R | Applied deep learning with torch from R</title>
  <meta name="description" content="tbd" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2 A simple neural network in R | Applied deep learning with torch from R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="tbd" />
  <meta property="og:image" content="tbdcover.png" />
  <meta property="og:description" content="tbd" />
  <meta name="github-repo" content="tbd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 A simple neural network in R | Applied deep learning with torch from R" />
  
  <meta name="twitter:description" content="tbd" />
  <meta name="twitter:image" content="tbdcover.png" />

<meta name="author" content="tbd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="using-torch-intro.html"/>
<link rel="next" href="simple-net-tensors.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
</ul></li>
<li class="part"><span><b>I Using Torch</b></span></li>
<li class="chapter" data-level="" data-path="using-torch-intro.html"><a href="using-torch-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="2" data-path="simple-net-R.html"><a href="simple-net-R.html"><i class="fa fa-check"></i><b>2</b> A simple neural network in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#whats-in-a-network"><i class="fa fa-check"></i><b>2.1</b> What’s in a network?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="simple-net-R.html"><a href="simple-net-R.html#gradient-descent"><i class="fa fa-check"></i><b>2.1.1</b> Gradient descent</a></li>
<li class="chapter" data-level="2.1.2" data-path="simple-net-R.html"><a href="simple-net-R.html#from-linear-regression-to-a-simple-network"><i class="fa fa-check"></i><b>2.1.2</b> From linear regression to a simple network</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#a-simple-network"><i class="fa fa-check"></i><b>2.2</b> A simple network</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#simulate-data"><i class="fa fa-check"></i><b>2.2.1</b> Simulate data</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#initialize-weights-and-biases"><i class="fa fa-check"></i><b>2.2.2</b> Initialize weights and biases</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>2.2.3</b> Training loop</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple-net-R.html"><a href="simple-net-R.html#complete-code"><i class="fa fa-check"></i><b>2.2.4</b> Complete code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html"><i class="fa fa-check"></i><b>3</b> Modifying the simple network to use torch tensors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#tensors"><i class="fa fa-check"></i><b>3.1</b> Tensors</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#creation"><i class="fa fa-check"></i><b>3.1.1</b> Creation</a></li>
<li class="chapter" data-level="3.1.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#conversion-to-built-in-r-data-types"><i class="fa fa-check"></i><b>3.1.2</b> Conversion to built-in R data types</a></li>
<li class="chapter" data-level="3.1.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#indexing-and-slicing-tensors"><i class="fa fa-check"></i><b>3.1.3</b> Indexing and slicing tensors</a></li>
<li class="chapter" data-level="3.1.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#reshaping-tensors"><i class="fa fa-check"></i><b>3.1.4</b> Reshaping tensors</a></li>
<li class="chapter" data-level="3.1.5" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#operations-on-tensors"><i class="fa fa-check"></i><b>3.1.5</b> Operations on tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#running-on-gpu"><i class="fa fa-check"></i><b>3.2</b> Running on GPU</a></li>
<li class="chapter" data-level="3.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#broadcasting"><i class="fa fa-check"></i><b>3.3</b> Broadcasting</a></li>
<li class="chapter" data-level="3.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#simple-neural-network-using-torch-tensors"><i class="fa fa-check"></i><b>3.4</b> Simple neural network using <code>torch</code> tensors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html"><i class="fa fa-check"></i><b>4</b> Using autograd</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#automatic-differentiation-with-autograd"><i class="fa fa-check"></i><b>4.1</b> Automatic differentiation with <em>autograd</em></a></li>
<li class="chapter" data-level="4.2" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#the-simple-network-now-using-autograd"><i class="fa fa-check"></i><b>4.2</b> The simple network, now using <em>autograd</em></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple-net-modules.html"><a href="simple-net-modules.html"><i class="fa fa-check"></i><b>5</b> Using torch modules</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#modules"><i class="fa fa-check"></i><b>5.1</b> Modules</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#base-modules-layers"><i class="fa fa-check"></i><b>5.1.1</b> Base modules (“layers”)</a></li>
<li class="chapter" data-level="5.1.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#container-modules-models"><i class="fa fa-check"></i><b>5.1.2</b> Container modules (“models”)</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#simple-network-using-modules"><i class="fa fa-check"></i><b>5.2</b> Simple network using modules</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="simple-net-optim.html"><a href="simple-net-optim.html"><i class="fa fa-check"></i><b>6</b> Using <code>torch</code> optimizers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="simple-net-optim.html"><a href="simple-net-optim.html#losses-and-loss-functions"><i class="fa fa-check"></i><b>6.1</b> Losses and loss functions</a></li>
<li class="chapter" data-level="6.2" data-path="simple-net-optim.html"><a href="simple-net-optim.html#optimizers"><i class="fa fa-check"></i><b>6.2</b> Optimizers</a></li>
<li class="chapter" data-level="6.3" data-path="simple-net-optim.html"><a href="simple-net-optim.html#simple-network-final-version"><i class="fa fa-check"></i><b>6.3</b> Simple network: final version</a></li>
</ul></li>
<li class="part"><span><b>II Image Recognition</b></span></li>
<li class="chapter" data-level="" data-path="image-recognition-intro.html"><a href="image-recognition-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="7" data-path="image-classification.html"><a href="image-classification.html"><i class="fa fa-check"></i><b>7</b> Classifying images</a>
<ul>
<li class="chapter" data-level="7.1" data-path="image-classification.html"><a href="image-classification.html#data-loading-and-preprocessing"><i class="fa fa-check"></i><b>7.1</b> Data loading and preprocessing</a></li>
<li class="chapter" data-level="7.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>7.2</b> Model</a></li>
<li class="chapter" data-level="7.3" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>7.3</b> Training</a></li>
<li class="chapter" data-level="7.4" data-path="image-classification.html"><a href="image-classification.html#test-set-accuracy"><i class="fa fa-check"></i><b>7.4</b> Test set accuracy</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="unet.html"><a href="unet.html"><i class="fa fa-check"></i><b>8</b> Brain Image Segmentation with U-Net</a>
<ul>
<li class="chapter" data-level="8.1" data-path="unet.html"><a href="unet.html#u-net"><i class="fa fa-check"></i><b>8.1</b> U-Net</a></li>
<li class="chapter" data-level="8.2" data-path="unet.html"><a href="unet.html#brain-image-segmentation"><i class="fa fa-check"></i><b>8.2</b> Brain image segmentation</a></li>
<li class="chapter" data-level="8.3" data-path="unet.html"><a href="unet.html#data"><i class="fa fa-check"></i><b>8.3</b> Data</a></li>
<li class="chapter" data-level="8.4" data-path="unet.html"><a href="unet.html#dataset"><i class="fa fa-check"></i><b>8.4</b> Dataset</a></li>
<li class="chapter" data-level="8.5" data-path="unet.html"><a href="unet.html#model-1"><i class="fa fa-check"></i><b>8.5</b> Model</a></li>
<li class="chapter" data-level="8.6" data-path="unet.html"><a href="unet.html#optimization"><i class="fa fa-check"></i><b>8.6</b> Optimization</a></li>
<li class="chapter" data-level="8.7" data-path="unet.html"><a href="unet.html#training-1"><i class="fa fa-check"></i><b>8.7</b> Training</a></li>
<li class="chapter" data-level="8.8" data-path="unet.html"><a href="unet.html#evaluation"><i class="fa fa-check"></i><b>8.8</b> Evaluation</a></li>
</ul></li>
<li class="part"><span><b>III Time series</b></span></li>
<li class="chapter" data-level="" data-path="timeseries-intro.html"><a href="timeseries-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="9" data-path="RNN.html"><a href="RNN.html"><i class="fa fa-check"></i><b>9</b> Torch transformer modules</a></li>
<li class="chapter" data-level="10" data-path="seq2seq-att.html"><a href="seq2seq-att.html"><i class="fa fa-check"></i><b>10</b> Sequence-to-sequence models with attention</a></li>
<li class="chapter" data-level="11" data-path="transformer.html"><a href="transformer.html"><i class="fa fa-check"></i><b>11</b> Torch transformer modules</a></li>
<li class="part"><span><b>IV Natural language processing</b></span></li>
<li class="chapter" data-level="" data-path="NLP-intro.html"><a href="NLP-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="12" data-path="NLPseq2seq-att.html"><a href="NLPseq2seq-att.html"><i class="fa fa-check"></i><b>12</b> Sequence-to-sequence models with attention</a></li>
<li class="chapter" data-level="13" data-path="NLPtransformer.html"><a href="NLPtransformer.html"><i class="fa fa-check"></i><b>13</b> Torch transformer modules</a></li>
<li class="part"><span><b>V Tabular data</b></span></li>
<li class="chapter" data-level="" data-path="tabular-intro.html"><a href="tabular-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="14" data-path="categorical.html"><a href="categorical.html"><i class="fa fa-check"></i><b>14</b> Handling categorical data</a>
<ul>
<li class="chapter" data-level="14.1" data-path="categorical.html"><a href="categorical.html#agenda"><i class="fa fa-check"></i><b>14.1</b> Agenda</a></li>
<li class="chapter" data-level="14.2" data-path="categorical.html"><a href="categorical.html#dataset-1"><i class="fa fa-check"></i><b>14.2</b> Dataset</a></li>
<li class="chapter" data-level="14.3" data-path="categorical.html"><a href="categorical.html#model-2"><i class="fa fa-check"></i><b>14.3</b> Model</a></li>
<li class="chapter" data-level="14.4" data-path="categorical.html"><a href="categorical.html#training-2"><i class="fa fa-check"></i><b>14.4</b> Training</a></li>
<li class="chapter" data-level="14.5" data-path="categorical.html"><a href="categorical.html#evaluation-1"><i class="fa fa-check"></i><b>14.5</b> Evaluation</a></li>
<li class="chapter" data-level="14.6" data-path="categorical.html"><a href="categorical.html#making-the-task-harder"><i class="fa fa-check"></i><b>14.6</b> Making the task harder</a></li>
<li class="chapter" data-level="14.7" data-path="categorical.html"><a href="categorical.html#a-look-at-the-hidden-representations"><i class="fa fa-check"></i><b>14.7</b> A look at the hidden representations</a></li>
</ul></li>
<li class="part"><span><b>VI Generative deep learning</b></span></li>
<li class="chapter" data-level="" data-path="generative-intro.html"><a href="generative-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="15" data-path="gans.html"><a href="gans.html"><i class="fa fa-check"></i><b>15</b> Generative adversarial networks</a>
<ul>
<li class="chapter" data-level="15.1" data-path="gans.html"><a href="gans.html#dataset-2"><i class="fa fa-check"></i><b>15.1</b> Dataset</a></li>
<li class="chapter" data-level="15.2" data-path="gans.html"><a href="gans.html#model-3"><i class="fa fa-check"></i><b>15.2</b> Model</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="gans.html"><a href="gans.html#generator"><i class="fa fa-check"></i><b>15.2.1</b> Generator</a></li>
<li class="chapter" data-level="15.2.2" data-path="gans.html"><a href="gans.html#discriminator"><i class="fa fa-check"></i><b>15.2.2</b> Discriminator</a></li>
<li class="chapter" data-level="15.2.3" data-path="gans.html"><a href="gans.html#optimizers-and-loss-function"><i class="fa fa-check"></i><b>15.2.3</b> Optimizers and loss function</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="gans.html"><a href="gans.html#training-loop-3"><i class="fa fa-check"></i><b>15.3</b> Training loop</a></li>
<li class="chapter" data-level="15.4" data-path="gans.html"><a href="gans.html#artifacts"><i class="fa fa-check"></i><b>15.4</b> Artifacts</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="vaes.html"><a href="vaes.html"><i class="fa fa-check"></i><b>16</b> Variational autoencoders</a>
<ul>
<li class="chapter" data-level="16.1" data-path="vaes.html"><a href="vaes.html#dataset-3"><i class="fa fa-check"></i><b>16.1</b> Dataset</a></li>
<li class="chapter" data-level="16.2" data-path="vaes.html"><a href="vaes.html#model-4"><i class="fa fa-check"></i><b>16.2</b> Model</a></li>
<li class="chapter" data-level="16.3" data-path="vaes.html"><a href="vaes.html#training-the-vae"><i class="fa fa-check"></i><b>16.3</b> Training the VAE</a></li>
<li class="chapter" data-level="16.4" data-path="vaes.html"><a href="vaes.html#latent-space"><i class="fa fa-check"></i><b>16.4</b> Latent space</a></li>
</ul></li>
<li class="part"><span><b>VII Deep learning on graphs</b></span></li>
<li class="chapter" data-level="" data-path="graph-intro.html"><a href="graph-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VIII Probabilistic deep learning</b></span></li>
<li class="chapter" data-level="" data-path="probabilistic-intro.html"><a href="probabilistic-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>IX Private and secure deep learning</b></span></li>
<li class="chapter" data-level="" data-path="private-secure-intro.html"><a href="private-secure-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>X Research topics</b></span></li>
<li class="chapter" data-level="" data-path="research-intro.html"><a href="research-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied deep learning with torch from R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple_net_R" class="section level1" number="2">
<h1><span class="header-section-number">2</span> A simple neural network in R</h1>
<p>Let’s think about what we need for a neural network.</p>
<div id="whats-in-a-network" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> What’s in a network?</h2>
<p>Our toy network will perform a simple regression task, and to explain its building blocks, we start by explaining what it has in common with standard linear regression (ordinary least squares, OLS).</p>
<div id="gradient-descent" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Gradient descent</h3>
<p>If we had to, we could do linear regression</p>
<span class="math display" id="eq:linreg">\[\begin{equation*}
\mathbf{X} \boldsymbol{\beta} = \mathbf{y}
\tag{2.1}
\end{equation*}\]</span>
<p>from scratch in R. Not necessarily using the <em>normal equations</em> (as those imply invertibility of the covariance matrix):</p>
<span class="math display" id="eq:normaleqs">\[\begin{equation*}
\hat{\boldsymbol{\beta}} = \mathbf{{(X^t X)}^{-1} X^t y}
\tag{2.2}
\end{equation*}\]</span>
<p>but iteratively, doing <em>gradient descent</em>. We start with a guess of the weight vector <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>. Then in each iteration, we compute how far we are from our objective, that is, from correctly solving equation <a href="simple-net-R.html#eq:linreg">(2.1)</a>. Most often in regression, for this we’d calculate <em>the sum of squared errors</em>:</p>
<span class="math display" id="eq:mse">\[\begin{equation*}
\mathcal{L} = \sum{{(\mathbf{X} \hat{\boldsymbol{\beta}} - \mathbf{\ y})}^2}
\tag{2.3}
\end{equation*}\]</span>
<p>This is our <em>loss function</em>. For that loss to go down, we need to update <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> in the right direction. How the loss changes with <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is given by its <em>gradient</em> with respect to the same:</p>
<span class="math display" id="eq:gradlosswrtbeta">\[\begin{equation*}
\nabla_{\hat{\boldsymbol{\beta}}} \mathcal{L} = 2 \mathbf{X}^t (\mathbf{X} \hat{\boldsymbol{\beta}} - \mathbf{y})
\tag{2.4}
\end{equation*}\]</span>
<p>Substracting a fraction of that gradient from <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> – “descending” the gradient of the loss – will make it go down. This can be seen by looking at the first-order Taylor approximation of a function <span class="math inline">\(f\)</span> (choosing a single-variable function for simplicity):</p>
<span class="math display" id="eq:euler">\[\begin{equation*}
f(x + \delta x) \approx f(x) + f&#39;(x) \delta x
\tag{2.5}
\end{equation*}\]</span>
<p>If we set <span class="math inline">\(\delta x\)</span> to a multiple of the derivative of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span>, <span class="math inline">\(\delta = \eta f&#39;(x)\)</span>, we get</p>
<span class="math display">\[\begin{equation*}
f(x - \eta f&#39;(x)) \approx f(x) - \eta f&#39;(x) f&#39;(x))
\end{equation*}\]</span>
<span class="math display" id="eq:euler">\[\begin{equation*}
f(x - \eta f&#39;(x)) \approx f(x) - \eta (f&#39;(x))^2
\tag{2.5}
\end{equation*}\]</span>
<p>This new value <span class="math inline">\(f(x - \eta f&#39;(x))\)</span> is smaller than <span class="math inline">\(f(x)\)</span> because on the right side, a positive value is subtracted.</p>
<p>Ported to our task of loss minimization, where loss <span class="math inline">\(\mathcal(L)\)</span> depends on a vector parameter <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, this would be</p>
<span class="math display" id="eq:euler">\[\begin{equation*}
\mathcal{L}(\hat{\boldsymbol{\beta}} + \Delta \hat{\boldsymbol{\beta}}) \approx \mathcal{L}(\hat{\boldsymbol{\beta}}) + {\Delta \hat{\boldsymbol{\beta}}}^t \nabla \hat{\boldsymbol{\beta}}
\tag{2.5}
\end{equation*}\]</span>
<p>Now, again, subtracting a fraction of the gradient, <span class="math inline">\(- \eta \nabla \hat{\boldsymbol{\beta}}\)</span>, we have</p>
<span class="math display">\[\begin{equation*}
\mathcal{L}(\hat{\boldsymbol{\beta}} - \eta \nabla \hat{\boldsymbol{\beta}}) \approx \mathcal{L}(\hat{\boldsymbol{\beta}}) - \eta {\nabla \hat{\boldsymbol{\beta}}}^t \nabla \hat{\boldsymbol{\beta}}
\end{equation*}\]</span>
<p>where again the new loss value is lower than the old one.</p>
<p>Iterating this process, we successively approach better estimates of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>. The scale parameter, <span class="math inline">\(\eta\)</span>, used to multiply the gradient is called the <em>learning rate</em>.</p>
<p>The process is analogous if we have a simple network. The main difference is that instead of one weight vector <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, we have several layers, each with their own weights that have to be updated.</p>
<p>Before going there, a quick summary of the concepts and building blocks we’ve now seen:</p>
<ul>
<li>Better weights are determined iteratively.</li>
<li>On each iteration, with the current weight estimates, we calculate a new prediction, the current <em>loss</em>, and the gradient of the loss with respect to the weights.</li>
<li>We update the weights, subtracting <em>learning_rate</em> times the gradient, and proceed to the next iteration.</li>
</ul>
<p>The program below will follow this blueprint. We’ll fill out the sections soon:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="simple-net-R.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>) {</span>
<span id="cb2-2"><a href="simple-net-R.html#cb2-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-3"><a href="simple-net-R.html#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- Forward pass -------- </span></span>
<span id="cb2-4"><a href="simple-net-R.html#cb2-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-5"><a href="simple-net-R.html#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># here we&#39;ll compute the prediction</span></span>
<span id="cb2-6"><a href="simple-net-R.html#cb2-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-7"><a href="simple-net-R.html#cb2-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-8"><a href="simple-net-R.html#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- compute loss -------- </span></span>
<span id="cb2-9"><a href="simple-net-R.html#cb2-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-10"><a href="simple-net-R.html#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># here we&#39;ll compute the sum of squared errors</span></span>
<span id="cb2-11"><a href="simple-net-R.html#cb2-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-12"><a href="simple-net-R.html#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="simple-net-R.html#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- Backpropagation -------- </span></span>
<span id="cb2-14"><a href="simple-net-R.html#cb2-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-15"><a href="simple-net-R.html#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># here we&#39;ll pass through the network, calculating the required gradients</span></span>
<span id="cb2-16"><a href="simple-net-R.html#cb2-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-17"><a href="simple-net-R.html#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="simple-net-R.html#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- Update weights -------- </span></span>
<span id="cb2-19"><a href="simple-net-R.html#cb2-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-20"><a href="simple-net-R.html#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># here we&#39;ll update the weights, subtracting portion of the gradients </span></span>
<span id="cb2-21"><a href="simple-net-R.html#cb2-21" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="from-linear-regression-to-a-simple-network" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> From linear regression to a simple network</h3>
<p>Let’s see how our simple network will be different from that process.</p>
<div id="implications-of-having-multiple-layers" class="section level4" number="2.1.2.1">
<h4><span class="header-section-number">2.1.2.1</span> Implications of having multiple layers</h4>
<p>The simple network will have two layers, the output layer (corresponding to the predictions above) and an intermediate (<em>hidden</em>) layer. Both layers have their corresponding weight matrices, <code>w1</code> and <code>w2</code>, and intermediate values computed at the hidden layer – called <em>activations</em> – are passed to the output layer for multiplication with <em>its</em> weight matrix.</p>
<p>Mirroring that multi-step forward pass, losses have to be <em>propagated back</em> through the network, such that both weight matrices may be updated. <em>Backpropagation</em>, understood in a conceptual<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> way, means that gradients are computed via the chain rule of calculus; for example, the gradient of the loss with respect to <code>w1</code> in our example will be<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<ul>
<li>the gradient of the loss w.r.t. the predictions; times</li>
<li>the gradient of output layer activation w.r.t. hidden layer activation (<code>w2</code>)</li>
<li>the gradient of hidden layer activation w.r.t. <code>w1</code> (<code>X</code>, the matrix of input data)</li>
</ul>
</div>
<div id="activation-functions" class="section level4" number="2.1.2.2">
<h4><span class="header-section-number">2.1.2.2</span> Activation functions</h4>
<p>In the above paragraph, we simplified slightly, making it look as though layer weights were applied with no action “in between.” In fact, usually a layer’s output, before being passed to the next layer, is transformed by an <em>activation function</em>, operating pointwise. Different activation functions exist; they all have in common that they introduce non-linearity into the computation.</p>
<p>Our example will use <em>ReLU</em> (“Rectified Linear Unit”) activation for the intermediate layer. <em>ReLU</em> sets negative input to 0 while leaving positive input as is. Activation functions add a further step to the backward pass, as well.</p>
</div>
<div id="weights-and-biases" class="section level4" number="2.1.2.3">
<h4><span class="header-section-number">2.1.2.3</span> Weights and biases</h4>
<p>In the linear regression example, we had a weight vector  – a vector, with one element for each predictor.</p>
<p>In neural networks, layers normally consist of several “neurons” (or units), the exception being the output layer – sometimes, namely, when there is a single prediction per observation.</p>
<p>Apart from that exception though, instead of weight vectors here we have weight <em>matrices</em>, connecting multiple “source” units to multiple “target” units.</p>
<p>Moreover, every unit has a so-called <em>bias</em> that is added to the output of the multiplication of inputs and weights. Thus, the biases <em>are</em> in fact vectors.</p>
<p>We now have all building blocks we need to define a training loop.</p>
</div>
</div>
</div>
<div id="a-simple-network" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> A simple network</h2>
<p>Our blueprint for a simple network does not employ any deep learning libraries; however, for speed, predictability and intuitiveness (in the sense of comparability to Python’s NumPy) we make use of <a href="https://github.com/r-lib/rray">rray</a> to manipulate array data.</p>
<p>Before getting to the network proper, we simulate some data for a typical regression problem.</p>
<div id="simulate-data" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Simulate data</h3>
<p>Our data has three input columns and a single target column.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="simple-net-R.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rray)</span>
<span id="cb3-2"><a href="simple-net-R.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb3-3"><a href="simple-net-R.html#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="simple-net-R.html#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb3-5"><a href="simple-net-R.html#cb3-5" aria-hidden="true" tabindex="-1"></a>d_in <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb3-6"><a href="simple-net-R.html#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb3-7"><a href="simple-net-R.html#cb3-7" aria-hidden="true" tabindex="-1"></a>d_out <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb3-8"><a href="simple-net-R.html#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># number of observations in training set</span></span>
<span id="cb3-9"><a href="simple-net-R.html#cb3-9" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb3-10"><a href="simple-net-R.html#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="simple-net-R.html#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># create random data</span></span>
<span id="cb3-12"><a href="simple-net-R.html#cb3-12" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rray</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> d_in), <span class="at">dim =</span> <span class="fu">c</span>(n, d_in))</span>
<span id="cb3-13"><a href="simple-net-R.html#cb3-13" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> x[ , <span class="dv">1</span>] <span class="sc">*</span> <span class="fl">0.2</span> <span class="sc">-</span> x[ , <span class="dv">2</span>] <span class="sc">*</span> <span class="fl">1.3</span> <span class="sc">-</span> x[ , <span class="dv">3</span>] <span class="sc">*</span> <span class="fl">0.5</span> <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span></code></pre></div>
<p>With <code>x</code> and <code>y</code> being instances of <code>rray</code> - provided classes,</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="simple-net-R.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(x)</span></code></pre></div>
<p>we can use operations like <code>rray_dot</code>, <code>rray_add</code> or <code>rray_transpose</code> on them. If you’ve used Python NumPy before, these will look familiar, – there is one point of caution though: Although <code>rray</code> explicitly provides <a href="https://blogs.rstudio.com/tensorflow/posts/2020-01-24-numpy-broadcasting/">broadcasting</a>, it lines up array dimensions <a href="https://github.com/r-lib/rray/blob/master/vignettes/broadcasting.Rmd">from the left, not from the right side</a>, in line with R’s column-major storage format.</p>
<p>Also reflecting column-major layout, <code>rray</code> prints array dimension data differently from base R – e.g. for two-dimensional arrays, the number of columns goes first:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="simple-net-R.html#cb5-1" aria-hidden="true" tabindex="-1"></a>first_ten_rows <span class="ot">=</span> x[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, ]</span>
<span id="cb5-2"><a href="simple-net-R.html#cb5-2" aria-hidden="true" tabindex="-1"></a>first_ten_rows</span></code></pre></div>
<p>We also need the weight matrices <span class="math inline">\(w1\)</span> and <span class="math inline">\(w2\)</span>, as well as the biases <span class="math inline">\(b1\)</span> and <span class="math inline">\(b2\)</span>.</p>
</div>
<div id="initialize-weights-and-biases" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Initialize weights and biases</h3>
<p>Again, we use <code>rray</code>, initializing the weights from a standard normal distribution, and the biases to <span class="math inline">\(0\)</span>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="simple-net-R.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">### initialize weights ---------------------------------------------------------</span></span>
<span id="cb6-2"><a href="simple-net-R.html#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="simple-net-R.html#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb6-4"><a href="simple-net-R.html#cb6-4" aria-hidden="true" tabindex="-1"></a>d_hidden <span class="ot">&lt;-</span> <span class="dv">32</span></span>
<span id="cb6-5"><a href="simple-net-R.html#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># weights connecting input to hidden layer</span></span>
<span id="cb6-6"><a href="simple-net-R.html#cb6-6" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> <span class="fu">rray</span>(<span class="fu">rnorm</span>(d_in <span class="sc">*</span> d_hidden), <span class="at">dim =</span> <span class="fu">c</span>(d_in, d_hidden))</span>
<span id="cb6-7"><a href="simple-net-R.html#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># weights connecting hidden to output layer</span></span>
<span id="cb6-8"><a href="simple-net-R.html#cb6-8" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> <span class="fu">rray</span>(<span class="fu">rnorm</span>(d_hidden <span class="sc">*</span> d_out), <span class="at">dim =</span> <span class="fu">c</span>(d_hidden, d_out))</span>
<span id="cb6-9"><a href="simple-net-R.html#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="simple-net-R.html#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># hidden layer bias</span></span>
<span id="cb6-11"><a href="simple-net-R.html#cb6-11" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="fu">rray</span>(<span class="fu">rep</span>(<span class="dv">0</span>, d_hidden), <span class="at">dim =</span> <span class="fu">c</span>(<span class="dv">1</span>, d_hidden))</span>
<span id="cb6-12"><a href="simple-net-R.html#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># output layer bias</span></span>
<span id="cb6-13"><a href="simple-net-R.html#cb6-13" aria-hidden="true" tabindex="-1"></a>b2 <span class="ot">&lt;-</span> <span class="fu">rray</span>(<span class="fu">rep</span>(<span class="dv">0</span>, d_out), <span class="at">dim =</span> <span class="fu">c</span>(d_out, <span class="dv">1</span>))</span></code></pre></div>
</div>
<div id="training-loop" class="section level3" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Training loop</h3>
<p>Now for the training loop proper. The training loop here <em>is</em> the network.</p>
<p>The forward pass computes intermediate activations (also applying <em>ReLU</em> activation), actual predictions, and the loss.</p>
<p>The backward pass starts from the output and, making use of the chain rule, calculates the gradients of the loss with respect to <span class="math inline">\(w2\)</span>, <span class="math inline">\(b2\)</span>, <span class="math inline">\(w1\)</span> und <span class="math inline">\(b1\)</span>. It then uses the gradients to update the parameters.</p>
<p>If you’re just starting out with neural networks, don’t worry too much about the details of matrix shapes and operations – all this will become <em>a lot</em> easier when we use full-flegded torch. Just try to develop an understanding of what this code does overall.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="simple-net-R.html#cb7-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="ot">&lt;-</span> <span class="fl">1e-4</span></span>
<span id="cb7-2"><a href="simple-net-R.html#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="simple-net-R.html#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="do">### training loop --------------------------------------------------------------</span></span>
<span id="cb7-4"><a href="simple-net-R.html#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>) {</span>
<span id="cb7-5"><a href="simple-net-R.html#cb7-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-6"><a href="simple-net-R.html#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- Forward pass -------- </span></span>
<span id="cb7-7"><a href="simple-net-R.html#cb7-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-8"><a href="simple-net-R.html#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute pre-activations of hidden layers (dim: 100 x 32)</span></span>
<span id="cb7-9"><a href="simple-net-R.html#cb7-9" aria-hidden="true" tabindex="-1"></a>    h <span class="ot">&lt;-</span> <span class="fu">rray_dot</span>(x, w1) <span class="sc">+</span> b1</span>
<span id="cb7-10"><a href="simple-net-R.html#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># apply activation function (dim: 100 x 32)</span></span>
<span id="cb7-11"><a href="simple-net-R.html#cb7-11" aria-hidden="true" tabindex="-1"></a>    h_relu <span class="ot">&lt;-</span> <span class="fu">rray_maximum</span>(h, <span class="dv">0</span>)</span>
<span id="cb7-12"><a href="simple-net-R.html#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute output (dim: 100 x 1)</span></span>
<span id="cb7-13"><a href="simple-net-R.html#cb7-13" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="ot">&lt;-</span> <span class="fu">rray_dot</span>(h_relu, w2) <span class="sc">+</span> b2</span>
<span id="cb7-14"><a href="simple-net-R.html#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="simple-net-R.html#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- compute loss -------- </span></span>
<span id="cb7-16"><a href="simple-net-R.html#cb7-16" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">&lt;-</span> <span class="fu">rray_pow</span>(y_pred <span class="sc">-</span> y, <span class="dv">2</span>) <span class="sc">%&gt;%</span> <span class="fu">rray_sum</span>()</span>
<span id="cb7-17"><a href="simple-net-R.html#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (t <span class="sc">%%</span> <span class="dv">10</span> <span class="sc">==</span> <span class="dv">0</span>) <span class="fu">cat</span>(<span class="st">&quot;Epoch:&quot;</span>, t, <span class="st">&quot;, loss:&quot;</span>, loss, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb7-18"><a href="simple-net-R.html#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="simple-net-R.html#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- Backpropagation -------- </span></span>
<span id="cb7-20"><a href="simple-net-R.html#cb7-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-21"><a href="simple-net-R.html#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. prediction (dim: 100 x 1)</span></span>
<span id="cb7-22"><a href="simple-net-R.html#cb7-22" aria-hidden="true" tabindex="-1"></a>    grad_y_pred <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> (y_pred <span class="sc">-</span> y)</span>
<span id="cb7-23"><a href="simple-net-R.html#cb7-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-24"><a href="simple-net-R.html#cb7-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. w2 (dim: 32 x 1)</span></span>
<span id="cb7-25"><a href="simple-net-R.html#cb7-25" aria-hidden="true" tabindex="-1"></a>    grad_w2 <span class="ot">&lt;-</span> <span class="fu">rray_transpose</span>(h_relu) <span class="sc">%&gt;%</span> <span class="fu">rray_dot</span>(grad_y_pred)</span>
<span id="cb7-26"><a href="simple-net-R.html#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. hidden activation (dim: 100 x 32)</span></span>
<span id="cb7-27"><a href="simple-net-R.html#cb7-27" aria-hidden="true" tabindex="-1"></a>    grad_h_relu <span class="ot">&lt;-</span> <span class="fu">rray_dot</span>(grad_y_pred, <span class="fu">rray_transpose</span>(w2))</span>
<span id="cb7-28"><a href="simple-net-R.html#cb7-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)</span></span>
<span id="cb7-29"><a href="simple-net-R.html#cb7-29" aria-hidden="true" tabindex="-1"></a>    grad_h <span class="ot">&lt;-</span> <span class="fu">rray_if_else</span>(h <span class="sc">&gt;</span> <span class="dv">0</span>, grad_h_relu, <span class="dv">0</span>)</span>
<span id="cb7-30"><a href="simple-net-R.html#cb7-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. b2 (dim: 1 x 1)</span></span>
<span id="cb7-31"><a href="simple-net-R.html#cb7-31" aria-hidden="true" tabindex="-1"></a>    grad_b2 <span class="ot">&lt;-</span> <span class="fu">rray_sum</span>(grad_y_pred)</span>
<span id="cb7-32"><a href="simple-net-R.html#cb7-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-33"><a href="simple-net-R.html#cb7-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. w1 (dim: 3 x 32)</span></span>
<span id="cb7-34"><a href="simple-net-R.html#cb7-34" aria-hidden="true" tabindex="-1"></a>    grad_w1 <span class="ot">&lt;-</span> <span class="fu">rray_transpose</span>(x) <span class="sc">%&gt;%</span> <span class="fu">rray_dot</span>(grad_h)</span>
<span id="cb7-35"><a href="simple-net-R.html#cb7-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. b1 (dim: 3 x 32)</span></span>
<span id="cb7-36"><a href="simple-net-R.html#cb7-36" aria-hidden="true" tabindex="-1"></a>    grad_b1 <span class="ot">&lt;-</span> <span class="fu">rray_sum</span>(grad_h, <span class="at">axes =</span> <span class="dv">1</span>)</span>
<span id="cb7-37"><a href="simple-net-R.html#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="simple-net-R.html#cb7-38" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- Update weights -------- </span></span>
<span id="cb7-39"><a href="simple-net-R.html#cb7-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-40"><a href="simple-net-R.html#cb7-40" aria-hidden="true" tabindex="-1"></a>    w2 <span class="ot">&lt;-</span> w2 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w2</span>
<span id="cb7-41"><a href="simple-net-R.html#cb7-41" aria-hidden="true" tabindex="-1"></a>    b2 <span class="ot">&lt;-</span> b2 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_b2</span>
<span id="cb7-42"><a href="simple-net-R.html#cb7-42" aria-hidden="true" tabindex="-1"></a>    w1 <span class="ot">&lt;-</span> w1 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w1</span>
<span id="cb7-43"><a href="simple-net-R.html#cb7-43" aria-hidden="true" tabindex="-1"></a>    b1 <span class="ot">&lt;-</span> b1 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_b1</span>
<span id="cb7-44"><a href="simple-net-R.html#cb7-44" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>In the next chapter, we start introducing torch. Optimization will still be performed manually, but instead of <code>rray</code> we are going to use torch <em>tensors</em>.</p>
</div>
<div id="complete-code" class="section level3" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Complete code</h3>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="simple-net-R.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rray)</span>
<span id="cb8-2"><a href="simple-net-R.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb8-3"><a href="simple-net-R.html#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="do">### generate training data -----------------------------------------------------</span></span>
<span id="cb8-4"><a href="simple-net-R.html#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="simple-net-R.html#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb8-6"><a href="simple-net-R.html#cb8-6" aria-hidden="true" tabindex="-1"></a>d_in <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb8-7"><a href="simple-net-R.html#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb8-8"><a href="simple-net-R.html#cb8-8" aria-hidden="true" tabindex="-1"></a>d_out <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb8-9"><a href="simple-net-R.html#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># number of observations in training set</span></span>
<span id="cb8-10"><a href="simple-net-R.html#cb8-10" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb8-11"><a href="simple-net-R.html#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="simple-net-R.html#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co"># create random data</span></span>
<span id="cb8-13"><a href="simple-net-R.html#cb8-13" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rray</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> d_in), <span class="at">dim =</span> <span class="fu">c</span>(n, d_in))</span>
<span id="cb8-14"><a href="simple-net-R.html#cb8-14" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> x[ , <span class="dv">1</span>] <span class="sc">*</span> <span class="fl">0.2</span> <span class="sc">-</span> x[ , <span class="dv">2</span>] <span class="sc">*</span> <span class="fl">1.3</span> <span class="sc">-</span> x[ , <span class="dv">3</span>] <span class="sc">*</span> <span class="fl">0.5</span> <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb8-15"><a href="simple-net-R.html#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co"># lm(as.matrix(y) ~ as.matrix(x)) %&gt;% summary()</span></span>
<span id="cb8-16"><a href="simple-net-R.html#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="simple-net-R.html#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="simple-net-R.html#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="do">### initialize weights ---------------------------------------------------------</span></span>
<span id="cb8-19"><a href="simple-net-R.html#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="simple-net-R.html#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb8-21"><a href="simple-net-R.html#cb8-21" aria-hidden="true" tabindex="-1"></a>d_hidden <span class="ot">&lt;-</span> <span class="dv">32</span></span>
<span id="cb8-22"><a href="simple-net-R.html#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co"># weights connecting input to hidden layer</span></span>
<span id="cb8-23"><a href="simple-net-R.html#cb8-23" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> <span class="fu">rray</span>(<span class="fu">rnorm</span>(d_in <span class="sc">*</span> d_hidden), <span class="at">dim =</span> <span class="fu">c</span>(d_in, d_hidden))</span>
<span id="cb8-24"><a href="simple-net-R.html#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="co"># weights connecting hidden to output layer</span></span>
<span id="cb8-25"><a href="simple-net-R.html#cb8-25" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> <span class="fu">rray</span>(<span class="fu">rnorm</span>(d_hidden <span class="sc">*</span> d_out), <span class="at">dim =</span> <span class="fu">c</span>(d_hidden, d_out))</span>
<span id="cb8-26"><a href="simple-net-R.html#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="simple-net-R.html#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co"># hidden layer bias</span></span>
<span id="cb8-28"><a href="simple-net-R.html#cb8-28" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="fu">rray</span>(<span class="fu">rep</span>(<span class="dv">0</span>, d_hidden), <span class="at">dim =</span> <span class="fu">c</span>(<span class="dv">1</span>, d_hidden))</span>
<span id="cb8-29"><a href="simple-net-R.html#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co"># output layer bias</span></span>
<span id="cb8-30"><a href="simple-net-R.html#cb8-30" aria-hidden="true" tabindex="-1"></a>b2 <span class="ot">&lt;-</span> <span class="fu">rray</span>(<span class="fu">rep</span>(<span class="dv">0</span>, d_out), <span class="at">dim =</span> <span class="fu">c</span>(d_out, <span class="dv">1</span>))</span>
<span id="cb8-31"><a href="simple-net-R.html#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="simple-net-R.html#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="do">### network parameters ---------------------------------------------------------</span></span>
<span id="cb8-33"><a href="simple-net-R.html#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="simple-net-R.html#cb8-34" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="ot">&lt;-</span> <span class="fl">1e-4</span></span>
<span id="cb8-35"><a href="simple-net-R.html#cb8-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-36"><a href="simple-net-R.html#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="do">### training loop --------------------------------------------------------------</span></span>
<span id="cb8-37"><a href="simple-net-R.html#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>) {</span>
<span id="cb8-38"><a href="simple-net-R.html#cb8-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-39"><a href="simple-net-R.html#cb8-39" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- Forward pass -------- </span></span>
<span id="cb8-40"><a href="simple-net-R.html#cb8-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-41"><a href="simple-net-R.html#cb8-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute pre-activations of hidden layers (dim: 100 x 32)</span></span>
<span id="cb8-42"><a href="simple-net-R.html#cb8-42" aria-hidden="true" tabindex="-1"></a>    h <span class="ot">&lt;-</span> <span class="fu">rray_dot</span>(x, w1) <span class="sc">+</span> b1</span>
<span id="cb8-43"><a href="simple-net-R.html#cb8-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># apply activation function (dim: 100 x 32)</span></span>
<span id="cb8-44"><a href="simple-net-R.html#cb8-44" aria-hidden="true" tabindex="-1"></a>    h_relu <span class="ot">&lt;-</span> <span class="fu">rray_maximum</span>(h, <span class="dv">0</span>)</span>
<span id="cb8-45"><a href="simple-net-R.html#cb8-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute output (dim: 100 x 1)</span></span>
<span id="cb8-46"><a href="simple-net-R.html#cb8-46" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="ot">&lt;-</span> <span class="fu">rray_dot</span>(h_relu, w2) <span class="sc">+</span> b2</span>
<span id="cb8-47"><a href="simple-net-R.html#cb8-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-48"><a href="simple-net-R.html#cb8-48" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- compute loss -------- </span></span>
<span id="cb8-49"><a href="simple-net-R.html#cb8-49" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">&lt;-</span> <span class="fu">rray_pow</span>(y_pred <span class="sc">-</span> y, <span class="dv">2</span>) <span class="sc">%&gt;%</span> <span class="fu">rray_sum</span>()</span>
<span id="cb8-50"><a href="simple-net-R.html#cb8-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (t <span class="sc">%%</span> <span class="dv">10</span> <span class="sc">==</span> <span class="dv">0</span>) <span class="fu">cat</span>(<span class="st">&quot;Epoch:&quot;</span>, t, <span class="st">&quot;, loss:&quot;</span>, loss, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb8-51"><a href="simple-net-R.html#cb8-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-52"><a href="simple-net-R.html#cb8-52" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- Backpropagation -------- </span></span>
<span id="cb8-53"><a href="simple-net-R.html#cb8-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-54"><a href="simple-net-R.html#cb8-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. prediction (dim: 100 x 1)</span></span>
<span id="cb8-55"><a href="simple-net-R.html#cb8-55" aria-hidden="true" tabindex="-1"></a>    grad_y_pred <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> (y_pred <span class="sc">-</span> y)</span>
<span id="cb8-56"><a href="simple-net-R.html#cb8-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-57"><a href="simple-net-R.html#cb8-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. w2 (dim: 32 x 1)</span></span>
<span id="cb8-58"><a href="simple-net-R.html#cb8-58" aria-hidden="true" tabindex="-1"></a>    grad_w2 <span class="ot">&lt;-</span> <span class="fu">rray_transpose</span>(h_relu) <span class="sc">%&gt;%</span> <span class="fu">rray_dot</span>(grad_y_pred)</span>
<span id="cb8-59"><a href="simple-net-R.html#cb8-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. hidden activation (dim: 100 x 32)</span></span>
<span id="cb8-60"><a href="simple-net-R.html#cb8-60" aria-hidden="true" tabindex="-1"></a>    grad_h_relu <span class="ot">&lt;-</span> <span class="fu">rray_dot</span>(grad_y_pred, <span class="fu">rray_transpose</span>(w2))</span>
<span id="cb8-61"><a href="simple-net-R.html#cb8-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)</span></span>
<span id="cb8-62"><a href="simple-net-R.html#cb8-62" aria-hidden="true" tabindex="-1"></a>    grad_h <span class="ot">&lt;-</span> <span class="fu">rray_if_else</span>(h <span class="sc">&gt;</span> <span class="dv">0</span>, grad_h_relu, <span class="dv">0</span>)</span>
<span id="cb8-63"><a href="simple-net-R.html#cb8-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. b2 (dim: 1 x 1)</span></span>
<span id="cb8-64"><a href="simple-net-R.html#cb8-64" aria-hidden="true" tabindex="-1"></a>    grad_b2 <span class="ot">&lt;-</span> <span class="fu">rray_sum</span>(grad_y_pred)</span>
<span id="cb8-65"><a href="simple-net-R.html#cb8-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-66"><a href="simple-net-R.html#cb8-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. w1 (dim: 3 x 32)</span></span>
<span id="cb8-67"><a href="simple-net-R.html#cb8-67" aria-hidden="true" tabindex="-1"></a>    grad_w1 <span class="ot">&lt;-</span> <span class="fu">rray_transpose</span>(x) <span class="sc">%&gt;%</span> <span class="fu">rray_dot</span>(grad_h)</span>
<span id="cb8-68"><a href="simple-net-R.html#cb8-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient of loss w.r.t. b1 (dim: 3 x 32)</span></span>
<span id="cb8-69"><a href="simple-net-R.html#cb8-69" aria-hidden="true" tabindex="-1"></a>    grad_b1 <span class="ot">&lt;-</span> <span class="fu">rray_sum</span>(grad_h, <span class="at">axes =</span> <span class="dv">1</span>)</span>
<span id="cb8-70"><a href="simple-net-R.html#cb8-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-71"><a href="simple-net-R.html#cb8-71" aria-hidden="true" tabindex="-1"></a>    <span class="do">### -------- Update weights -------- </span></span>
<span id="cb8-72"><a href="simple-net-R.html#cb8-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-73"><a href="simple-net-R.html#cb8-73" aria-hidden="true" tabindex="-1"></a>    w2 <span class="ot">&lt;-</span> w2 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w2</span>
<span id="cb8-74"><a href="simple-net-R.html#cb8-74" aria-hidden="true" tabindex="-1"></a>    b2 <span class="ot">&lt;-</span> b2 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_b2</span>
<span id="cb8-75"><a href="simple-net-R.html#cb8-75" aria-hidden="true" tabindex="-1"></a>    w1 <span class="ot">&lt;-</span> w1 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w1</span>
<span id="cb8-76"><a href="simple-net-R.html#cb8-76" aria-hidden="true" tabindex="-1"></a>    b1 <span class="ot">&lt;-</span> b1 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_b1</span>
<span id="cb8-77"><a href="simple-net-R.html#cb8-77" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>as opposed to implementation-related<a href="simple-net-R.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>simplifying slightly here; we’ll correct that shortly<a href="simple-net-R.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="using-torch-intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple-net-tensors.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
