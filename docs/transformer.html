<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Torch transformer modules | Applied deep learning with torch from R</title>
  <meta name="description" content="tbd" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Torch transformer modules | Applied deep learning with torch from R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="tbd" />
  <meta property="og:image" content="tbdcover.png" />
  <meta property="og:description" content="tbd" />
  <meta name="github-repo" content="tbd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Torch transformer modules | Applied deep learning with torch from R" />
  
  <meta name="twitter:description" content="tbd" />
  <meta name="twitter:image" content="tbdcover.png" />

<meta name="author" content="tbd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="seq2seq-att.html"/>
<link rel="next" href="tabular-intro.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
</ul></li>
<li class="part"><span><b>I Using Torch</b></span></li>
<li class="chapter" data-level="" data-path="using-torch-intro.html"><a href="using-torch-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="2" data-path="simple-net-R.html"><a href="simple-net-R.html"><i class="fa fa-check"></i><b>2</b> A simple neural network in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#whats-in-a-network"><i class="fa fa-check"></i><b>2.1</b> What’s in a network?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="simple-net-R.html"><a href="simple-net-R.html#gradient-descent"><i class="fa fa-check"></i><b>2.1.1</b> Gradient descent</a></li>
<li class="chapter" data-level="2.1.2" data-path="simple-net-R.html"><a href="simple-net-R.html#from-linear-regression-to-a-simple-network"><i class="fa fa-check"></i><b>2.1.2</b> From linear regression to a simple network</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#a-simple-network"><i class="fa fa-check"></i><b>2.2</b> A simple network</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#simulate-data"><i class="fa fa-check"></i><b>2.2.1</b> Simulate data</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#initialize-weights-and-biases"><i class="fa fa-check"></i><b>2.2.2</b> Initialize weights and biases</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>2.2.3</b> Training loop</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple-net-R.html"><a href="simple-net-R.html#complete-code"><i class="fa fa-check"></i><b>2.2.4</b> Complete code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>2.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html"><i class="fa fa-check"></i><b>3</b> Modifying the simple network to use torch tensors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#simple-network-torchified-step-1"><i class="fa fa-check"></i><b>3.1</b> Simple network torchified, step 1</a></li>
<li class="chapter" data-level="3.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#more-on-tensors"><i class="fa fa-check"></i><b>3.2</b> More on tensors</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#creating-tensors"><i class="fa fa-check"></i><b>3.2.1</b> Creating tensors</a></li>
<li class="chapter" data-level="3.2.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#conversion-between-torch-tensors-and-r-values"><i class="fa fa-check"></i><b>3.2.2</b> Conversion between <code>torch</code> tensors and R values</a></li>
<li class="chapter" data-level="3.2.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#indexing-and-slicing-tensors"><i class="fa fa-check"></i><b>3.2.3</b> Indexing and slicing tensors</a></li>
<li class="chapter" data-level="3.2.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#reshaping-tensors"><i class="fa fa-check"></i><b>3.2.4</b> Reshaping tensors</a></li>
<li class="chapter" data-level="3.2.5" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#operations-on-tensors"><i class="fa fa-check"></i><b>3.2.5</b> Operations on tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#broadcasting"><i class="fa fa-check"></i><b>3.3</b> Broadcasting</a></li>
<li class="chapter" data-level="3.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#running-on-gpu"><i class="fa fa-check"></i><b>3.4</b> Running on GPU</a></li>
<li class="chapter" data-level="3.5" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>3.5</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html"><i class="fa fa-check"></i><b>4</b> Using autograd</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#automatic-differentiation-with-autograd"><i class="fa fa-check"></i><b>4.1</b> Automatic differentiation with autograd</a></li>
<li class="chapter" data-level="4.2" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#the-simple-network-now-using-autograd"><i class="fa fa-check"></i><b>4.2</b> The simple network, now using autograd</a></li>
<li class="chapter" data-level="4.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>4.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple-net-modules.html"><a href="simple-net-modules.html"><i class="fa fa-check"></i><b>5</b> Using torch modules</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#modules"><i class="fa fa-check"></i><b>5.1</b> Modules</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#layers-as-modules"><i class="fa fa-check"></i><b>5.1.1</b> Layers as modules</a></li>
<li class="chapter" data-level="5.1.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#models-as-modules"><i class="fa fa-check"></i><b>5.1.2</b> Models as modules</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#simple-network-using-modules"><i class="fa fa-check"></i><b>5.2</b> Simple network using modules</a></li>
<li class="chapter" data-level="5.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>5.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="simple-net-optim.html"><a href="simple-net-optim.html"><i class="fa fa-check"></i><b>6</b> Using <code>torch</code> optimizers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="simple-net-optim.html"><a href="simple-net-optim.html#torch-optimizers"><i class="fa fa-check"></i><b>6.1</b> Torch optimizers</a></li>
<li class="chapter" data-level="6.2" data-path="simple-net-optim.html"><a href="simple-net-optim.html#simple-net-with-optim"><i class="fa fa-check"></i><b>6.2</b> Simple net with <code>optim</code></a></li>
<li class="chapter" data-level="6.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>6.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="part"><span><b>II Image Recognition</b></span></li>
<li class="chapter" data-level="" data-path="image-recognition-intro.html"><a href="image-recognition-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="7" data-path="image-classification.html"><a href="image-classification.html"><i class="fa fa-check"></i><b>7</b> Classifying images</a>
<ul>
<li class="chapter" data-level="7.1" data-path="image-classification.html"><a href="image-classification.html#data-loading-and-transformation"><i class="fa fa-check"></i><b>7.1</b> Data loading and transformation</a></li>
<li class="chapter" data-level="7.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>7.2</b> Model</a></li>
<li class="chapter" data-level="7.3" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>7.3</b> Training</a></li>
<li class="chapter" data-level="7.4" data-path="image-classification.html"><a href="image-classification.html#performance-on-the-test-set"><i class="fa fa-check"></i><b>7.4</b> Performance on the test set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="unet.html"><a href="unet.html"><i class="fa fa-check"></i><b>8</b> Brain Image Segmentation with U-Net</a>
<ul>
<li class="chapter" data-level="8.1" data-path="unet.html"><a href="unet.html#image-segmentation-in-a-nutshell"><i class="fa fa-check"></i><b>8.1</b> Image segmentation in a nutshell</a></li>
<li class="chapter" data-level="8.2" data-path="unet.html"><a href="unet.html#u-net"><i class="fa fa-check"></i><b>8.2</b> U-Net</a></li>
<li class="chapter" data-level="8.3" data-path="unet.html"><a href="unet.html#example-application-mri-images"><i class="fa fa-check"></i><b>8.3</b> Example application: MRI images</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="unet.html"><a href="unet.html#preprocessing"><i class="fa fa-check"></i><b>8.3.1</b> Preprocessing</a></li>
<li class="chapter" data-level="8.3.2" data-path="unet.html"><a href="unet.html#u-net-model"><i class="fa fa-check"></i><b>8.3.2</b> U-Net model</a></li>
<li class="chapter" data-level="8.3.3" data-path="unet.html"><a href="unet.html#loss"><i class="fa fa-check"></i><b>8.3.3</b> Loss</a></li>
<li class="chapter" data-level="8.3.4" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>8.3.4</b> Training</a></li>
<li class="chapter" data-level="8.3.5" data-path="unet.html"><a href="unet.html#predictions"><i class="fa fa-check"></i><b>8.3.5</b> Predictions</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Natural language processing</b></span></li>
<li class="chapter" data-level="" data-path="NLP-intro.html"><a href="NLP-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="9" data-path="seq2seq-att.html"><a href="seq2seq-att.html"><i class="fa fa-check"></i><b>9</b> Sequence-to-sequence models with attention</a>
<ul>
<li class="chapter" data-level="9.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#why-attention"><i class="fa fa-check"></i><b>9.1</b> Why attention?</a></li>
<li class="chapter" data-level="9.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#preprocessing-with-torchtext"><i class="fa fa-check"></i><b>9.2</b> Preprocessing with <code>torchtext</code></a></li>
<li class="chapter" data-level="9.3" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>9.3</b> Model</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>9.3.1</b> Encoder</a></li>
<li class="chapter" data-level="9.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#attention-module"><i class="fa fa-check"></i><b>9.3.2</b> Attention module</a></li>
<li class="chapter" data-level="9.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>9.3.3</b> Decoder</a></li>
<li class="chapter" data-level="9.3.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#seq2seq-module"><i class="fa fa-check"></i><b>9.3.4</b> Seq2seq module</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#training-and-evaluation"><i class="fa fa-check"></i><b>9.4</b> Training and evaluation</a></li>
<li class="chapter" data-level="9.5" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>9.5</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="transformer.html"><a href="transformer.html"><i class="fa fa-check"></i><b>10</b> Torch transformer modules</a>
<ul>
<li class="chapter" data-level="10.1" data-path="transformer.html"><a href="transformer.html#attention-is-all-you-need"><i class="fa fa-check"></i><b>10.1</b> “Attention is all you need”</a></li>
<li class="chapter" data-level="10.2" data-path="transformer.html"><a href="transformer.html#implementation-building-blocks"><i class="fa fa-check"></i><b>10.2</b> Implementation: building blocks</a></li>
<li class="chapter" data-level="10.3" data-path="transformer.html"><a href="transformer.html#a-transformer-for-natural-language-translation"><i class="fa fa-check"></i><b>10.3</b> A Transformer for natural language translation</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="transformer.html"><a href="transformer.html#load-data"><i class="fa fa-check"></i><b>10.3.1</b> Load data</a></li>
<li class="chapter" data-level="10.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>10.3.2</b> Encoder</a></li>
<li class="chapter" data-level="10.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>10.3.3</b> Decoder</a></li>
<li class="chapter" data-level="10.3.4" data-path="transformer.html"><a href="transformer.html#overall-model"><i class="fa fa-check"></i><b>10.3.4</b> Overall model</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>10.4</b> Results</a></li>
</ul></li>
<li class="part"><span><b>IV Deep learning for tabular data</b></span></li>
<li class="chapter" data-level="" data-path="tabular-intro.html"><a href="tabular-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>V Time series</b></span></li>
<li class="chapter" data-level="" data-path="timeseries-intro.html"><a href="timeseries-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VI Generative deep learning</b></span></li>
<li class="chapter" data-level="" data-path="generative-intro.html"><a href="generative-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="11" data-path="gans.html"><a href="gans.html"><i class="fa fa-check"></i><b>11</b> Generative adversarial networks</a>
<ul>
<li class="chapter" data-level="11.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>11.1</b> Dataset</a></li>
<li class="chapter" data-level="11.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>11.2</b> Model</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="gans.html"><a href="gans.html#generator"><i class="fa fa-check"></i><b>11.2.1</b> Generator</a></li>
<li class="chapter" data-level="11.2.2" data-path="gans.html"><a href="gans.html#discriminator"><i class="fa fa-check"></i><b>11.2.2</b> Discriminator</a></li>
<li class="chapter" data-level="11.2.3" data-path="gans.html"><a href="gans.html#optimizers-and-loss-function"><i class="fa fa-check"></i><b>11.2.3</b> Optimizers and loss function</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>11.3</b> Training loop</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="vaes.html"><a href="vaes.html"><i class="fa fa-check"></i><b>12</b> Variational autoencoders</a>
<ul>
<li class="chapter" data-level="12.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>12.1</b> Dataset</a></li>
<li class="chapter" data-level="12.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>12.2</b> Model</a></li>
<li class="chapter" data-level="12.3" data-path="vaes.html"><a href="vaes.html#training-the-vae"><i class="fa fa-check"></i><b>12.3</b> Training the VAE</a></li>
<li class="chapter" data-level="12.4" data-path="vaes.html"><a href="vaes.html#latent-space"><i class="fa fa-check"></i><b>12.4</b> Latent space</a></li>
</ul></li>
<li class="part"><span><b>VII Deep learning on graphs</b></span></li>
<li class="chapter" data-level="" data-path="graph-intro.html"><a href="graph-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VIII Probabilistic deep learning</b></span></li>
<li class="chapter" data-level="" data-path="probabilistic-intro.html"><a href="probabilistic-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>IX Private and secure deep learning</b></span></li>
<li class="chapter" data-level="" data-path="private-secure-intro.html"><a href="private-secure-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>X Research topics</b></span></li>
<li class="chapter" data-level="" data-path="research-intro.html"><a href="research-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied deep learning with torch from R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="transformer" class="section level1" number="10">
<h1><span class="header-section-number">10</span> Torch transformer modules</h1>
<div id="attention-is-all-you-need" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> “Attention is all you need”</h2>
<p>When the original Transformer paper <span class="citation">(Vaswani et al. <a href="#ref-VaswaniSPUJGKP17" role="doc-biblioref">2017</a>)</span> appeared, its provocative title can only have speeded up its road to
fame. Why would it be provocative? At that time, sequential data were firmly thought to be the realm of RNNs (albeit extended
by encoder-decoder attention): If one input token’s probability depends on the previous input(s) (as in time series, language,
or music), it seems we need to keep some form of <em>state</em> to preserve sequential relationships over the whole calculation.</p>
<p>In fact, Transformer did not have RNNs, but compensated for lacking state in two ways: adding <em>positional encoding</em>, thus in
some way keeping track of where in the phrase a token is located, and most importantly, <em>self-attention</em>: making use of
context (surrounding tokens) when encoding each input token. With self-attention, <em>no token is an island</em>; instead, it only
gains its meaning through how it relates to its neighbors.</p>
<p>Seeing how excellent architectural explanations abund, ranging from code-oriented [@rush-2018-annotated] to
<a href="https://jalammar.github.io/illustrated-transformer/">visual</a>, we just give a brief conceptual characterization.</p>
<div id="self-attention-and-multi-head-attention" class="section level4" number="10.1.0.1">
<h4><span class="header-section-number">10.1.0.1</span> Self-attention and “multi-head attention”</h4>
<p>Each input word (after the usual embedding) plays three roles, designated by terms coming from <em>information retrieval</em>: query,
key, and value. In this chapter, we won’t be coding attention from scratch, but relying on <code>torch</code> modules; so strictly, we
don’t need to go there. But as query, key and value vectors have become part of the official transformer lingo, it’s good to
have heard those terms.</p>
<p>In a nutshell, self-attention does not encode every word separately, but at every position, works with a conglomerate of
semantic and syntactic information that is made up, in some way, of the complete input phrase. The basic operation, like in
the encoder-decoder setup, is a <em>dot product</em> used to determine some form of similarity/promixity/relevance in semantic space.</p>
<p>This dot product occurs between the word that is being encoded – appearing in its role as <em>query vector</em> – and every other
word, each wearing their <em>key vector</em> hats. Essentially, these measures of affinity are normalized and used to weight the
<em>value vectors</em> corresponding to every <em>key</em> that was used in the comparison. Finally, for every <em>query</em> we aggregate the
weighted value vectors into a composite result, which is passed on to the next layer.</p>
<p>Why does each token have to wear three different hats? If it didn’t, these affinity relationships would be symmetric (the dot
product per se being commutative), thus badly conforming with semantic and (especially!) syntactic reality. Thus, technically,
a word’s query, key and value vectors are not the same; instead, each is obtained as the output of a different feedforward
layer.</p>
<p>So that is self-attention - now what does “multi-head attention” refer to? This simply is a “bag of attention modules”, all
operating in parallel. Like that, multi-head attention is said to take care of taking into account multiple “representation
subspaces”.</p>
</div>
<div id="overall-architecture" class="section level4" number="10.1.0.2">
<h4><span class="header-section-number">10.1.0.2</span> Overall architecture</h4>
<p>Overall, transformer is an encoder stack, followed by a decoder stack. Both stacks are composed of several, identical
submodules combining multi-head attention, layer normalization <span class="citation">(Lei Ba, Kiros, and Hinton <a href="#ref-2016arXiv160706450L" role="doc-biblioref">2016</a>)</span>, residual connections, and feedforward
neural networks applied pointwise to each input.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> In addition to the self-attention mechanism (conceptually) shared with
encoder layers, the decoder layers exercise a second form of attention: <em>encoder-decoder</em> attention allows them to
differentially pay attention to the output passed by the encoder.</p>
</div>
</div>
<div id="implementation-building-blocks" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Implementation: building blocks</h2>
<p>While it is certainly possible, and instructive, to build a transformer network from scratch, we won’t reinvent the wheel but
instead, make use of <code>torch</code> layers that simplify the process significantly. <code>TransformerEncoderLayer</code> and
<code>TransformerDecoderLayer</code> are the basic modules that make up encoder and decoder stacks, respectively.</p>
<p>Here is a single encoder submodule comprising multi-head self-attention, layer normalization and feedforward networks:</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb168-1"><a href="transformer.html#cb168-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> TransformerEncoderLayer</span>
<span id="cb168-2"><a href="transformer.html#cb168-2"></a></span>
<span id="cb168-3"><a href="transformer.html#cb168-3"></a>e <span class="op">=</span> TransformerEncoderLayer(d_model <span class="op">=</span> <span class="dv">256</span>, nhead <span class="op">=</span> <span class="dv">2</span>, dim_feedforward <span class="op">=</span> <span class="dv">256</span>, dropout <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb168-4"><a href="transformer.html#cb168-4"></a>e</span></code></pre></div>
<pre><code>TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): Linear(in_features=256, out_features=256, bias=True)
  )
  (linear1): Linear(in_features=256, out_features=256, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
  (linear2): Linear(in_features=256, out_features=256, bias=True)
  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.2, inplace=False)
  (dropout2): Dropout(p=0.2, inplace=False)
)</code></pre>
<p>A decoder submodule looks similar, apart from the fact that it has an additional <code>MultiHeadAttention</code> (sub-)submodule. One is
for self-attention, the other, for attending to encoder input:</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb170-1"><a href="transformer.html#cb170-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> TransformerDecoderLayer</span>
<span id="cb170-2"><a href="transformer.html#cb170-2"></a></span>
<span id="cb170-3"><a href="transformer.html#cb170-3"></a>d <span class="op">=</span> TransformerDecoderLayer(d_model <span class="op">=</span> <span class="dv">256</span>, nhead <span class="op">=</span> <span class="dv">2</span>, dim_feedforward <span class="op">=</span> <span class="dv">256</span>, dropout <span class="op">=</span> <span class="fl">0.2</span>)</span></code></pre></div>
<p>Next up in the hierarchy are <code>TransformerEncoder</code> and <code>TransformerDecoder</code>. These are just containers, making up the
respective stacks:</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb171-1"><a href="transformer.html#cb171-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> TransformerEncoder</span>
<span id="cb171-2"><a href="transformer.html#cb171-2"></a></span>
<span id="cb171-3"><a href="transformer.html#cb171-3"></a>TransformerEncoder(encoder_layer <span class="op">=</span> e, num_layers <span class="op">=</span> <span class="dv">6</span>)</span></code></pre></div>
<div class="sourceCode" id="cb172"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb172-1"><a href="transformer.html#cb172-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> TransformerDecoder</span>
<span id="cb172-2"><a href="transformer.html#cb172-2"></a></span>
<span id="cb172-3"><a href="transformer.html#cb172-3"></a>TransformerDecoder(decoder_layer <span class="op">=</span> d, num_layers <span class="op">=</span> <span class="dv">6</span>)</span></code></pre></div>
<p>There is even a <code>Transformer</code> module that takes in parameters for the sublayers
(<code>TransformerEncoderLayer</code>/<code>TransformerDecoderLayer</code>) as well as the containers (<code>TransformerEncoder</code>/<code>TransformerDecoder</code>).
We won’t make use of that one though, as the overall architecture is more transparent when encoder and decoder stacks remain
clearly separated.</p>
</div>
<div id="a-transformer-for-natural-language-translation" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> A Transformer for natural language translation</h2>
<p>We use the same dataset as in the previous chapter, but this time, another language to translate to: Czech.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> You are very
welcome to compare architectures on identical splits, of course.</p>
<div id="load-data" class="section level3" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> Load data</h3>
<p>As you see, the training set now is considerably smaller, resulting in significantly lower training time.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb173-1"><a href="transformer.html#cb173-1"></a><span class="im">import</span> torch</span>
<span id="cb173-2"><a href="transformer.html#cb173-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb173-3"><a href="transformer.html#cb173-3"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb173-4"><a href="transformer.html#cb173-4"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb173-5"><a href="transformer.html#cb173-5"></a></span>
<span id="cb173-6"><a href="transformer.html#cb173-6"></a><span class="im">from</span> torchtext.data <span class="im">import</span> Field, BucketIterator</span>
<span id="cb173-7"><a href="transformer.html#cb173-7"></a><span class="im">from</span> torchtext.datasets <span class="im">import</span> IWSLT</span>
<span id="cb173-8"><a href="transformer.html#cb173-8"></a></span>
<span id="cb173-9"><a href="transformer.html#cb173-9"></a><span class="im">import</span> random</span>
<span id="cb173-10"><a href="transformer.html#cb173-10"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb173-11"><a href="transformer.html#cb173-11"></a><span class="im">import</span> math</span>
<span id="cb173-12"><a href="transformer.html#cb173-12"></a></span>
<span id="cb173-13"><a href="transformer.html#cb173-13"></a><span class="co"># this time the model will expect to get the batch dimension first</span></span>
<span id="cb173-14"><a href="transformer.html#cb173-14"></a>src_spec <span class="op">=</span> Field(</span>
<span id="cb173-15"><a href="transformer.html#cb173-15"></a>    tokenize <span class="op">=</span> <span class="st">&quot;spacy&quot;</span>,</span>
<span id="cb173-16"><a href="transformer.html#cb173-16"></a>    tokenizer_language<span class="op">=</span><span class="st">&quot;en&quot;</span>,</span>
<span id="cb173-17"><a href="transformer.html#cb173-17"></a>    init_token <span class="op">=</span> <span class="st">&#39;&lt;sos&gt;&#39;</span>,</span>
<span id="cb173-18"><a href="transformer.html#cb173-18"></a>    eos_token <span class="op">=</span> <span class="st">&#39;&lt;eos&gt;&#39;</span>,</span>
<span id="cb173-19"><a href="transformer.html#cb173-19"></a>    lower <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb173-20"><a href="transformer.html#cb173-20"></a>    batch_first <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb173-21"><a href="transformer.html#cb173-21"></a>    fix_length<span class="op">=</span><span class="dv">100</span></span>
<span id="cb173-22"><a href="transformer.html#cb173-22"></a>    )</span>
<span id="cb173-23"><a href="transformer.html#cb173-23"></a></span>
<span id="cb173-24"><a href="transformer.html#cb173-24"></a>trg_spec <span class="op">=</span> Field(</span>
<span id="cb173-25"><a href="transformer.html#cb173-25"></a>    tokenize <span class="op">=</span> <span class="st">&quot;spacy&quot;</span>,</span>
<span id="cb173-26"><a href="transformer.html#cb173-26"></a>    <span class="co"># no language-specific tokenizer available for cz</span></span>
<span id="cb173-27"><a href="transformer.html#cb173-27"></a>    tokenizer_language<span class="op">=</span><span class="st">&quot;xx&quot;</span>, </span>
<span id="cb173-28"><a href="transformer.html#cb173-28"></a>    init_token <span class="op">=</span> <span class="st">&#39;&lt;sos&gt;&#39;</span>,</span>
<span id="cb173-29"><a href="transformer.html#cb173-29"></a>    eos_token <span class="op">=</span> <span class="st">&#39;&lt;eos&gt;&#39;</span>,</span>
<span id="cb173-30"><a href="transformer.html#cb173-30"></a>    lower <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb173-31"><a href="transformer.html#cb173-31"></a>    batch_first <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb173-32"><a href="transformer.html#cb173-32"></a>    fix_length<span class="op">=</span><span class="dv">100</span></span>
<span id="cb173-33"><a href="transformer.html#cb173-33"></a>    )</span>
<span id="cb173-34"><a href="transformer.html#cb173-34"></a>            </span>
<span id="cb173-35"><a href="transformer.html#cb173-35"></a>train_data, valid_data, test_data <span class="op">=</span> IWSLT.splits(</span>
<span id="cb173-36"><a href="transformer.html#cb173-36"></a>  exts <span class="op">=</span> (<span class="st">&#39;.en&#39;</span>, <span class="st">&#39;.cs&#39;</span>),</span>
<span id="cb173-37"><a href="transformer.html#cb173-37"></a>  fields <span class="op">=</span> (src_spec, trg_spec),</span>
<span id="cb173-38"><a href="transformer.html#cb173-38"></a>  test<span class="op">=</span><span class="st">&#39;IWSLT16.TED.tst2013&#39;</span>) <span class="co"># 2014 does not exist</span></span>
<span id="cb173-39"><a href="transformer.html#cb173-39"></a></span>
<span id="cb173-40"><a href="transformer.html#cb173-40"></a><span class="bu">len</span>(train_data.examples), <span class="bu">len</span>(valid_data.examples), <span class="bu">len</span>(test_data.examples)</span></code></pre></div>
<pre><code>(114390, 1327, 1327)</code></pre>
<p>This time too, let’s see a few examples:</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb175-1"><a href="transformer.html#cb175-1"></a><span class="bu">vars</span>(train_data.examples[<span class="dv">111</span>])</span>
<span id="cb175-2"><a href="transformer.html#cb175-2"></a><span class="bu">vars</span>(train_data.examples[<span class="dv">11111</span>])</span>
<span id="cb175-3"><a href="transformer.html#cb175-3"></a><span class="bu">vars</span>(train_data.examples[<span class="dv">111111</span>])</span></code></pre></div>
<pre><code>{&#39;src&#39;: [&#39;here&#39;, &#39;they&#39;, &#39;go&#39;, &#39;.&#39;], &#39;trg&#39;: [&#39;a&#39;, &#39;je&#39;, &#39;to&#39;, &#39;tady&#39;, &#39;.&#39;]}

{&#39;src&#39;: [&#39;or&#39;, &#39;you&#39;, &#39;could&#39;, &#39;see&#39;, &#39;the&#39;, &#39;first&#39;, &#39;time&#39;, &#39;the&#39;, &#39;two&#39;, &#39;curves&#39;, &#39;diverged&#39;, &#39;,&#39;, &#39;as&#39;, &#39;shown&#39;, &#39;on&#39;, &#39;the&#39;, &#39;left&#39;, &#39;.&#39;], &#39;trg&#39;: [&#39;nalevo&#39;, &#39;můžete&#39;, &#39;vidět&#39;, &#39;,&#39;, &#39;kdy&#39;, &#39;se&#39;, &#39;ty&#39;, &#39;dvě&#39;, &#39;křivky&#39;, &#39;poprvé&#39;, &#39;rozchází&#39;, &#39;.&#39;]}

{&#39;src&#39;: [&#39;i&#39;, &#39;hope&#39;, &#39;you&#39;, &quot;&#39;ll&quot;, &#39;each&#39;, &#39;take&#39;, &#39;a&#39;, &#39;moment&#39;, &#39;to&#39;, &#39;think&#39;, &#39;about&#39;, &#39;how&#39;, &#39;you&#39;, &#39;could&#39;, &#39;use&#39;, &#39;something&#39;, &#39;like&#39;, &#39;this&#39;, &#39;to&#39;, &#39;give&#39;, &#39;yourself&#39;, &#39;more&#39;, &#39;access&#39;, &#39;to&#39;, &#39;your&#39;, &#39;own&#39;, &#39;world&#39;, &#39;,&#39;, &#39;and&#39;, &#39;to&#39;, &#39;make&#39;, &#39;your&#39;, &#39;own&#39;, &#39;travel&#39;, &#39;more&#39;, &#39;convenient&#39;, &#39;and&#39;, &#39;more&#39;, &#39;fun&#39;, &#39;.&#39;], &#39;trg&#39;: [&#39;doufám&#39;, &#39;,&#39;, &#39;že&#39;, &#39;se&#39;, &#39;všichni&#39;, &#39;alespoň&#39;, &#39;na&#39;, &#39;chvíli&#39;, &#39;zamyslíte&#39;, &#39;,&#39;, &#39;jak&#39;, &#39;by&#39;, &#39;vám&#39;, &#39;mohlo&#39;, &#39;použití&#39;, &#39;tohoto&#39;, &#39;prostředku&#39;, &#39;pomoci&#39;, &#39;,&#39;, &#39;abyste&#39;, &#39;získali&#39;, &#39;lepší&#39;, &#39;přístup&#39;, &#39;k&#39;, &#39;vlastnímu&#39;, &#39;světu&#39;, &#39;,&#39;, &#39;a&#39;, &#39;abyste&#39;, &#39;své&#39;, &#39;cestování&#39;, &#39;učinili&#39;, &#39;pohodlnějším&#39;, &#39;a&#39;, &#39;zábavnějším&#39;, &#39;.&#39;]}</code></pre>
<p>We construct the vocabularies and save away the indices used for the pad tokens, as we’ll want to exempt those locations from
both attention mechanism and loss calculation.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb177-1"><a href="transformer.html#cb177-1"></a>src_spec.build_vocab(train_data, min_freq <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb177-2"><a href="transformer.html#cb177-2"></a>trg_spec.build_vocab(train_data, min_freq <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb177-3"><a href="transformer.html#cb177-3"></a></span>
<span id="cb177-4"><a href="transformer.html#cb177-4"></a>src_pad_idx <span class="op">=</span> src_spec.vocab.stoi[<span class="st">&quot;&lt;pad&gt;&quot;</span>]</span>
<span id="cb177-5"><a href="transformer.html#cb177-5"></a>trg_pad_idx <span class="op">=</span> trg_spec.vocab.stoi[<span class="st">&quot;&lt;pad&gt;&quot;</span>]</span></code></pre></div>
<p>You might have noticed, in the <code>Field</code> specifications, that this time, we load the data batch-dimension first. Let’s verify:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb178-1"><a href="transformer.html#cb178-1"></a>device <span class="op">=</span> torch.device(<span class="st">&#39;cuda&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb178-2"><a href="transformer.html#cb178-2"></a></span>
<span id="cb178-3"><a href="transformer.html#cb178-3"></a>batch_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb178-4"><a href="transformer.html#cb178-4"></a></span>
<span id="cb178-5"><a href="transformer.html#cb178-5"></a>train_iterator, valid_iterator, test_iterator <span class="op">=</span> BucketIterator.splits(</span>
<span id="cb178-6"><a href="transformer.html#cb178-6"></a>    (train_data, valid_data, test_data),</span>
<span id="cb178-7"><a href="transformer.html#cb178-7"></a>    batch_size <span class="op">=</span> batch_size,</span>
<span id="cb178-8"><a href="transformer.html#cb178-8"></a>    device <span class="op">=</span> device)</span>
<span id="cb178-9"><a href="transformer.html#cb178-9"></a>    </span>
<span id="cb178-10"><a href="transformer.html#cb178-10"></a>batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_iterator))</span>
<span id="cb178-11"><a href="transformer.html#cb178-11"></a>batch.src.shape, batch.trg.shape</span></code></pre></div>
<pre><code>(torch.Size([8, 100]), torch.Size([8, 100]))</code></pre>
</div>
<div id="encoder" class="section level3" number="10.3.2">
<h3><span class="header-section-number">10.3.2</span> Encoder</h3>
<p>Again, we start with the encoder. Some points of interest:</p>
<ul>
<li>This time, we have two embedding layers: the habitual one, used to embed the input, and a second one that’s supposed to
learn the position codes. Originally, fixed rules were used to construct position encodings; this approach, however, seems
to have gotten superseded<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> by the usual deep learning maxim: let the network learn the features.</li>
<li>Before calling the encoder stack, both embeddings are combined (scaling down the token embeddings, a heuristic said to
help with training stability – you may want to experiment with this).</li>
<li>The encoder stack is called with a <em>mask</em>, whose creation we’ll witness soon. Its function is to mask the <code>pad</code> tokens so
no energy is wasted paying attention to them.</li>
</ul>
<div class="sourceCode" id="cb180"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb180-1"><a href="transformer.html#cb180-1"></a>num_input_features <span class="op">=</span> <span class="bu">len</span>(src_spec.vocab)</span>
<span id="cb180-2"><a href="transformer.html#cb180-2"></a></span>
<span id="cb180-3"><a href="transformer.html#cb180-3"></a>embedding_dim <span class="op">=</span> <span class="dv">256</span> </span>
<span id="cb180-4"><a href="transformer.html#cb180-4"></a></span>
<span id="cb180-5"><a href="transformer.html#cb180-5"></a><span class="co"># max number of positions to encode</span></span>
<span id="cb180-6"><a href="transformer.html#cb180-6"></a>max_length <span class="op">=</span> <span class="dv">100</span> </span>
<span id="cb180-7"><a href="transformer.html#cb180-7"></a></span>
<span id="cb180-8"><a href="transformer.html#cb180-8"></a><span class="co"># the dimension of the feedforward network model in nn.TransformerEncoder</span></span>
<span id="cb180-9"><a href="transformer.html#cb180-9"></a>hidden_dim <span class="op">=</span> <span class="dv">256</span> </span>
<span id="cb180-10"><a href="transformer.html#cb180-10"></a></span>
<span id="cb180-11"><a href="transformer.html#cb180-11"></a><span class="co"># number of nn.TransformerEncoderLayer in nn.TransformerEncoder</span></span>
<span id="cb180-12"><a href="transformer.html#cb180-12"></a>n_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb180-13"><a href="transformer.html#cb180-13"></a></span>
<span id="cb180-14"><a href="transformer.html#cb180-14"></a><span class="co"># number of heads in the MultiheadAttention modules</span></span>
<span id="cb180-15"><a href="transformer.html#cb180-15"></a>n_heads <span class="op">=</span> <span class="dv">2</span> </span>
<span id="cb180-16"><a href="transformer.html#cb180-16"></a></span>
<span id="cb180-17"><a href="transformer.html#cb180-17"></a>dropout <span class="op">=</span> <span class="fl">0.2</span> </span>
<span id="cb180-18"><a href="transformer.html#cb180-18"></a></span>
<span id="cb180-19"><a href="transformer.html#cb180-19"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb180-20"><a href="transformer.html#cb180-20"></a>  </span>
<span id="cb180-21"><a href="transformer.html#cb180-21"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_input_features, embedding_dim, n_heads, hidden_dim, n_layers, max_length, dropout):</span>
<span id="cb180-22"><a href="transformer.html#cb180-22"></a>        <span class="bu">super</span>(Encoder, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb180-23"><a href="transformer.html#cb180-23"></a>        <span class="im">from</span> torch.nn <span class="im">import</span> TransformerEncoder, TransformerEncoderLayer</span>
<span id="cb180-24"><a href="transformer.html#cb180-24"></a>        <span class="va">self</span>.embedding_dim <span class="op">=</span> embedding_dim</span>
<span id="cb180-25"><a href="transformer.html#cb180-25"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(num_input_features, embedding_dim)</span>
<span id="cb180-26"><a href="transformer.html#cb180-26"></a>        <span class="va">self</span>.pos_embedding <span class="op">=</span> nn.Embedding(max_length, embedding_dim)</span>
<span id="cb180-27"><a href="transformer.html#cb180-27"></a>        encoder_layers <span class="op">=</span> TransformerEncoderLayer(embedding_dim, n_heads, hidden_dim, dropout)</span>
<span id="cb180-28"><a href="transformer.html#cb180-28"></a>        <span class="va">self</span>.transformer_encoder <span class="op">=</span> TransformerEncoder(encoder_layers, n_layers)</span>
<span id="cb180-29"><a href="transformer.html#cb180-29"></a>        <span class="va">self</span>.init_weights()</span>
<span id="cb180-30"><a href="transformer.html#cb180-30"></a>        </span>
<span id="cb180-31"><a href="transformer.html#cb180-31"></a>    <span class="kw">def</span> init_weights(<span class="va">self</span>):</span>
<span id="cb180-32"><a href="transformer.html#cb180-32"></a>        initrange <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb180-33"><a href="transformer.html#cb180-33"></a>        <span class="va">self</span>.embedding.weight.data.uniform_(<span class="op">-</span>initrange, initrange)</span>
<span id="cb180-34"><a href="transformer.html#cb180-34"></a>        <span class="va">self</span>.pos_embedding.weight.data.uniform_(<span class="op">-</span>initrange, initrange)</span>
<span id="cb180-35"><a href="transformer.html#cb180-35"></a>        </span>
<span id="cb180-36"><a href="transformer.html#cb180-36"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, src_key_padding_mask):</span>
<span id="cb180-37"><a href="transformer.html#cb180-37"></a>        batch_size <span class="op">=</span> src.shape[<span class="dv">0</span>]</span>
<span id="cb180-38"><a href="transformer.html#cb180-38"></a>        src_len <span class="op">=</span> src.shape[<span class="dv">1</span>]</span>
<span id="cb180-39"><a href="transformer.html#cb180-39"></a>        <span class="co"># bs * src len</span></span>
<span id="cb180-40"><a href="transformer.html#cb180-40"></a>        <span class="co"># repeat vector 0 ... 35 once for every batch item</span></span>
<span id="cb180-41"><a href="transformer.html#cb180-41"></a>        <span class="co"># input for pos_embedding</span></span>
<span id="cb180-42"><a href="transformer.html#cb180-42"></a>        pos <span class="op">=</span> torch.arange(<span class="dv">0</span>, src_len).unsqueeze(<span class="dv">0</span>).repeat(batch_size, <span class="dv">1</span>).to(device)</span>
<span id="cb180-43"><a href="transformer.html#cb180-43"></a>        <span class="co"># bs * src len * hidden dim</span></span>
<span id="cb180-44"><a href="transformer.html#cb180-44"></a>        src <span class="op">=</span> (<span class="va">self</span>.embedding(src) <span class="op">*</span> math.sqrt(<span class="va">self</span>.embedding_dim)) <span class="op">+</span> <span class="va">self</span>.pos_embedding(pos)</span>
<span id="cb180-45"><a href="transformer.html#cb180-45"></a>        <span class="co"># apply transformer stack</span></span>
<span id="cb180-46"><a href="transformer.html#cb180-46"></a>        src <span class="op">=</span> torch.transpose(src, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb180-47"><a href="transformer.html#cb180-47"></a>        output <span class="op">=</span> <span class="va">self</span>.transformer_encoder(src, src_key_padding_mask <span class="op">=</span> src_key_padding_mask)</span>
<span id="cb180-48"><a href="transformer.html#cb180-48"></a>        <span class="co"># bs * src len * hidden dim</span></span>
<span id="cb180-49"><a href="transformer.html#cb180-49"></a>        <span class="cf">return</span> output</span>
<span id="cb180-50"><a href="transformer.html#cb180-50"></a></span>
<span id="cb180-51"><a href="transformer.html#cb180-51"></a>encoder <span class="op">=</span> Encoder(num_input_features, embedding_dim, n_heads, hidden_dim, n_layers, max_length, dropout).to(device)</span></code></pre></div>
</div>
<div id="decoder" class="section level3" number="10.3.3">
<h3><span class="header-section-number">10.3.3</span> Decoder</h3>
<p>The decoder also uses position embeddings, and its stack of <code>TransformerDecoderLayer</code> s looks a lot like a mirror of the
<code>TransformerEncoderLayer</code> s doing the work on the encoder side. Just keep in mind that in decoding, we have multihead
attention operating twice in every layer: firstly, to attend selectively to encoder output; and secondly, to attend
selectively to what we already generated.</p>
<p>Notice how the stack of <code>TransformerDecoderLayer</code>s is called with two masks: One, again, hides the artificial <em>pad</em> tokens;
the other is destined to modulate decoder-encoder attention, making sure that the decoder only ever looks at current or past,
but not <em>future</em> encoder input.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb181-1"><a href="transformer.html#cb181-1"></a>num_output_features <span class="op">=</span> <span class="bu">len</span>(trg_spec.vocab)</span>
<span id="cb181-2"><a href="transformer.html#cb181-2"></a></span>
<span id="cb181-3"><a href="transformer.html#cb181-3"></a><span class="kw">class</span> Decoder(nn.Module):</span>
<span id="cb181-4"><a href="transformer.html#cb181-4"></a>  </span>
<span id="cb181-5"><a href="transformer.html#cb181-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_output_features, embedding_dim, n_heads, hidden_dim, n_layers, max_length, dropout):</span>
<span id="cb181-6"><a href="transformer.html#cb181-6"></a>        <span class="bu">super</span>(Decoder, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb181-7"><a href="transformer.html#cb181-7"></a>        <span class="im">from</span> torch.nn <span class="im">import</span> TransformerDecoder, TransformerDecoderLayer</span>
<span id="cb181-8"><a href="transformer.html#cb181-8"></a>        <span class="va">self</span>.embedding_dim <span class="op">=</span> embedding_dim</span>
<span id="cb181-9"><a href="transformer.html#cb181-9"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(num_output_features, embedding_dim)</span>
<span id="cb181-10"><a href="transformer.html#cb181-10"></a>        <span class="co"># learn positional encoding</span></span>
<span id="cb181-11"><a href="transformer.html#cb181-11"></a>        <span class="va">self</span>.pos_embedding <span class="op">=</span> nn.Embedding(max_length, embedding_dim)</span>
<span id="cb181-12"><a href="transformer.html#cb181-12"></a>        decoder_layers <span class="op">=</span> TransformerDecoderLayer(embedding_dim, n_heads, hidden_dim, dropout)</span>
<span id="cb181-13"><a href="transformer.html#cb181-13"></a>        <span class="va">self</span>.transformer_decoder <span class="op">=</span> TransformerDecoder(decoder_layers, n_layers)</span>
<span id="cb181-14"><a href="transformer.html#cb181-14"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(hidden_dim, num_output_features)</span>
<span id="cb181-15"><a href="transformer.html#cb181-15"></a>        <span class="va">self</span>.init_weights()</span>
<span id="cb181-16"><a href="transformer.html#cb181-16"></a>        </span>
<span id="cb181-17"><a href="transformer.html#cb181-17"></a>    <span class="kw">def</span> init_weights(<span class="va">self</span>):</span>
<span id="cb181-18"><a href="transformer.html#cb181-18"></a>        initrange <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb181-19"><a href="transformer.html#cb181-19"></a>        <span class="va">self</span>.embedding.weight.data.uniform_(<span class="op">-</span>initrange, initrange)</span>
<span id="cb181-20"><a href="transformer.html#cb181-20"></a>        <span class="va">self</span>.pos_embedding.weight.data.uniform_(<span class="op">-</span>initrange, initrange)</span>
<span id="cb181-21"><a href="transformer.html#cb181-21"></a>        </span>
<span id="cb181-22"><a href="transformer.html#cb181-22"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, trg, encoder_outputs, tgt_mask, tgt_key_padding_mask):</span>
<span id="cb181-23"><a href="transformer.html#cb181-23"></a>        batch_size <span class="op">=</span> trg.shape[<span class="dv">0</span>]</span>
<span id="cb181-24"><a href="transformer.html#cb181-24"></a>        trg_len <span class="op">=</span> trg.shape[<span class="dv">1</span>]</span>
<span id="cb181-25"><a href="transformer.html#cb181-25"></a>        <span class="co"># bs * trg len</span></span>
<span id="cb181-26"><a href="transformer.html#cb181-26"></a>        <span class="co"># input for pos_embedding</span></span>
<span id="cb181-27"><a href="transformer.html#cb181-27"></a>        pos <span class="op">=</span> torch.arange(<span class="dv">0</span>, trg_len).unsqueeze(<span class="dv">0</span>).repeat(batch_size, <span class="dv">1</span>).to(device)</span>
<span id="cb181-28"><a href="transformer.html#cb181-28"></a>        <span class="co"># bs * trg len * hidden dim</span></span>
<span id="cb181-29"><a href="transformer.html#cb181-29"></a>        trg <span class="op">=</span> (<span class="va">self</span>.embedding(trg) <span class="op">*</span> math.sqrt(<span class="va">self</span>.embedding_dim)) <span class="op">+</span> <span class="va">self</span>.pos_embedding(pos)</span>
<span id="cb181-30"><a href="transformer.html#cb181-30"></a>        <span class="co"># apply transformer stack</span></span>
<span id="cb181-31"><a href="transformer.html#cb181-31"></a>        <span class="co"># bs * trg len * hidden dim</span></span>
<span id="cb181-32"><a href="transformer.html#cb181-32"></a>        trg <span class="op">=</span> torch.transpose(trg, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb181-33"><a href="transformer.html#cb181-33"></a>        output <span class="op">=</span> <span class="va">self</span>.transformer_decoder(trg, encoder_outputs,</span>
<span id="cb181-34"><a href="transformer.html#cb181-34"></a>          tgt_mask <span class="op">=</span> tgt_mask, tgt_key_padding_mask <span class="op">=</span> tgt_key_padding_mask)</span>
<span id="cb181-35"><a href="transformer.html#cb181-35"></a>        output <span class="op">=</span> <span class="va">self</span>.fc(output)</span>
<span id="cb181-36"><a href="transformer.html#cb181-36"></a>        <span class="cf">return</span> output</span>
<span id="cb181-37"><a href="transformer.html#cb181-37"></a></span>
<span id="cb181-38"><a href="transformer.html#cb181-38"></a>decoder <span class="op">=</span> Decoder(num_output_features, embedding_dim, n_heads, hidden_dim, n_layers, max_length, dropout).to(device)</span></code></pre></div>
</div>
<div id="overall-model" class="section level3" number="10.3.4">
<h3><span class="header-section-number">10.3.4</span> Overall model</h3>
<p>As before, we organize encoder and decoder in a <code>Seq2Seq</code> module. Apart from being a convenient container, this one also makes
sure that at each step, its submodules get called with up-to-date masks.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb182-1"><a href="transformer.html#cb182-1"></a><span class="kw">class</span> Seq2Seq(nn.Module):</span>
<span id="cb182-2"><a href="transformer.html#cb182-2"></a>  </span>
<span id="cb182-3"><a href="transformer.html#cb182-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, decoder, device):</span>
<span id="cb182-4"><a href="transformer.html#cb182-4"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb182-5"><a href="transformer.html#cb182-5"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb182-6"><a href="transformer.html#cb182-6"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb182-7"><a href="transformer.html#cb182-7"></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb182-8"><a href="transformer.html#cb182-8"></a>        </span>
<span id="cb182-9"><a href="transformer.html#cb182-9"></a>    <span class="kw">def</span> make_src_key_padding_mask(<span class="va">self</span>, src):</span>
<span id="cb182-10"><a href="transformer.html#cb182-10"></a>        <span class="co"># bs * src_len</span></span>
<span id="cb182-11"><a href="transformer.html#cb182-11"></a>        src_mask <span class="op">=</span> src <span class="op">==</span> src_pad_idx</span>
<span id="cb182-12"><a href="transformer.html#cb182-12"></a>        <span class="cf">return</span> src_mask</span>
<span id="cb182-13"><a href="transformer.html#cb182-13"></a>      </span>
<span id="cb182-14"><a href="transformer.html#cb182-14"></a>    <span class="kw">def</span> make_trg_key_padding_mask(<span class="va">self</span>, trg):</span>
<span id="cb182-15"><a href="transformer.html#cb182-15"></a>        <span class="co"># bs * trg_len</span></span>
<span id="cb182-16"><a href="transformer.html#cb182-16"></a>        trg_mask <span class="op">=</span> trg <span class="op">==</span> trg_pad_idx</span>
<span id="cb182-17"><a href="transformer.html#cb182-17"></a>        <span class="cf">return</span> trg_mask</span>
<span id="cb182-18"><a href="transformer.html#cb182-18"></a>      </span>
<span id="cb182-19"><a href="transformer.html#cb182-19"></a>    <span class="kw">def</span> make_trg_mask(<span class="va">self</span>, trg):</span>
<span id="cb182-20"><a href="transformer.html#cb182-20"></a>        trg_len <span class="op">=</span> trg.shape[<span class="dv">1</span>]</span>
<span id="cb182-21"><a href="transformer.html#cb182-21"></a>        mask <span class="op">=</span> (torch.triu(torch.ones(trg_len, trg_len)) <span class="op">==</span> <span class="dv">1</span>).transpose(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb182-22"><a href="transformer.html#cb182-22"></a>        mask <span class="op">=</span> mask.<span class="bu">float</span>().masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">&#39;-inf&#39;</span>)).masked_fill(mask <span class="op">==</span> <span class="dv">1</span>, <span class="bu">float</span>(<span class="fl">0.0</span>))</span>
<span id="cb182-23"><a href="transformer.html#cb182-23"></a>        <span class="cf">return</span> mask.to(device)</span>
<span id="cb182-24"><a href="transformer.html#cb182-24"></a>      </span>
<span id="cb182-25"><a href="transformer.html#cb182-25"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, trg):</span>
<span id="cb182-26"><a href="transformer.html#cb182-26"></a>        encoded <span class="op">=</span> <span class="va">self</span>.encoder(src, <span class="va">self</span>.make_src_key_padding_mask(src))</span>
<span id="cb182-27"><a href="transformer.html#cb182-27"></a>        output <span class="op">=</span> <span class="va">self</span>.decoder(trg, encoded,  <span class="va">self</span>.make_trg_mask(trg), <span class="va">self</span>.make_trg_key_padding_mask(trg))</span>
<span id="cb182-28"><a href="transformer.html#cb182-28"></a>        <span class="cf">return</span> output</span>
<span id="cb182-29"><a href="transformer.html#cb182-29"></a></span>
<span id="cb182-30"><a href="transformer.html#cb182-30"></a>model <span class="op">=</span> Seq2Seq(encoder, decoder, device).to(device)</span>
<span id="cb182-31"><a href="transformer.html#cb182-31"></a>model(src, trg)</span></code></pre></div>
<p>Training and evaluation look pretty much like in the previous model, apart from operations on tensor shapes.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb183-1"><a href="transformer.html#cb183-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.0005</span></span>
<span id="cb183-2"><a href="transformer.html#cb183-2"></a></span>
<span id="cb183-3"><a href="transformer.html#cb183-3"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb183-4"><a href="transformer.html#cb183-4"></a>pad_idx <span class="op">=</span> trg_spec.vocab.stoi[<span class="st">&#39;&lt;pad&gt;&#39;</span>]</span>
<span id="cb183-5"><a href="transformer.html#cb183-5"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss(ignore_index <span class="op">=</span> pad_idx)</span>
<span id="cb183-6"><a href="transformer.html#cb183-6"></a></span>
<span id="cb183-7"><a href="transformer.html#cb183-7"></a></span>
<span id="cb183-8"><a href="transformer.html#cb183-8"></a><span class="kw">def</span> train(model, iterator, optimizer, criterion, clip):</span>
<span id="cb183-9"><a href="transformer.html#cb183-9"></a>    model.train()</span>
<span id="cb183-10"><a href="transformer.html#cb183-10"></a>    epoch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb183-11"><a href="transformer.html#cb183-11"></a>    <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(iterator):</span>
<span id="cb183-12"><a href="transformer.html#cb183-12"></a>        src <span class="op">=</span> batch.src</span>
<span id="cb183-13"><a href="transformer.html#cb183-13"></a>        trg <span class="op">=</span> batch.trg</span>
<span id="cb183-14"><a href="transformer.html#cb183-14"></a>        optimizer.zero_grad()</span>
<span id="cb183-15"><a href="transformer.html#cb183-15"></a>        output <span class="op">=</span> model(src, trg[:,:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb183-16"><a href="transformer.html#cb183-16"></a>        <span class="co"># bs * (trg len - 1) * num_output_features</span></span>
<span id="cb183-17"><a href="transformer.html#cb183-17"></a>        output <span class="op">=</span> torch.transpose(output, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb183-18"><a href="transformer.html#cb183-18"></a>        output_dim <span class="op">=</span> output.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb183-19"><a href="transformer.html#cb183-19"></a>        <span class="co"># (bs * (trg len - 1)) * num_output_features</span></span>
<span id="cb183-20"><a href="transformer.html#cb183-20"></a>        output <span class="op">=</span> output.contiguous().view(<span class="op">-</span><span class="dv">1</span>, output_dim)</span>
<span id="cb183-21"><a href="transformer.html#cb183-21"></a>        <span class="co"># (bs * trg len)</span></span>
<span id="cb183-22"><a href="transformer.html#cb183-22"></a>        trg <span class="op">=</span> trg[:,<span class="dv">1</span>:].contiguous().view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb183-23"><a href="transformer.html#cb183-23"></a>        loss <span class="op">=</span> criterion(output, trg)</span>
<span id="cb183-24"><a href="transformer.html#cb183-24"></a>        <span class="bu">print</span>(loss)</span>
<span id="cb183-25"><a href="transformer.html#cb183-25"></a>        loss.backward()</span>
<span id="cb183-26"><a href="transformer.html#cb183-26"></a>        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)</span>
<span id="cb183-27"><a href="transformer.html#cb183-27"></a>        optimizer.step()</span>
<span id="cb183-28"><a href="transformer.html#cb183-28"></a>        epoch_loss <span class="op">+=</span> loss.item()</span>
<span id="cb183-29"><a href="transformer.html#cb183-29"></a>    <span class="cf">return</span> epoch_loss <span class="op">/</span> <span class="bu">len</span>(iterator)</span>
<span id="cb183-30"><a href="transformer.html#cb183-30"></a></span>
<span id="cb183-31"><a href="transformer.html#cb183-31"></a><span class="kw">def</span> evaluate(model, iterator, criterion):</span>
<span id="cb183-32"><a href="transformer.html#cb183-32"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb183-33"><a href="transformer.html#cb183-33"></a>    epoch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb183-34"><a href="transformer.html#cb183-34"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb183-35"><a href="transformer.html#cb183-35"></a>        <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(iterator):</span>
<span id="cb183-36"><a href="transformer.html#cb183-36"></a>            src <span class="op">=</span> batch.src</span>
<span id="cb183-37"><a href="transformer.html#cb183-37"></a>            trg <span class="op">=</span> batch.trg</span>
<span id="cb183-38"><a href="transformer.html#cb183-38"></a>            output <span class="op">=</span> model(src, trg[:,:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb183-39"><a href="transformer.html#cb183-39"></a>            output <span class="op">=</span> torch.transpose(output, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb183-40"><a href="transformer.html#cb183-40"></a>            output_dim <span class="op">=</span> output.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb183-41"><a href="transformer.html#cb183-41"></a>            output <span class="op">=</span> output.contiguous().view(<span class="op">-</span><span class="dv">1</span>, output_dim)</span>
<span id="cb183-42"><a href="transformer.html#cb183-42"></a>            trg <span class="op">=</span> trg[:,<span class="dv">1</span>:].contiguous().view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb183-43"><a href="transformer.html#cb183-43"></a>            loss <span class="op">=</span> criterion(output, trg)</span>
<span id="cb183-44"><a href="transformer.html#cb183-44"></a>            epoch_loss <span class="op">+=</span> loss.item()</span>
<span id="cb183-45"><a href="transformer.html#cb183-45"></a>    <span class="cf">return</span> epoch_loss <span class="op">/</span> <span class="bu">len</span>(iterator)</span></code></pre></div>
<p>Again, we look at how translations evolve during training.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb184-1"><a href="transformer.html#cb184-1"></a><span class="kw">def</span> translate_sentence(sentence, src_field, trg_field, model, device, max_len <span class="op">=</span> <span class="dv">50</span>):</span>
<span id="cb184-2"><a href="transformer.html#cb184-2"></a>  </span>
<span id="cb184-3"><a href="transformer.html#cb184-3"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb184-4"><a href="transformer.html#cb184-4"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(sentence, <span class="bu">str</span>):</span>
<span id="cb184-5"><a href="transformer.html#cb184-5"></a>        nlp <span class="op">=</span> spacy.load(<span class="st">&#39;en&#39;</span>)</span>
<span id="cb184-6"><a href="transformer.html#cb184-6"></a>        tokens <span class="op">=</span> [token.text.lower() <span class="cf">for</span> token <span class="kw">in</span> nlp(sentence)]</span>
<span id="cb184-7"><a href="transformer.html#cb184-7"></a>    <span class="cf">else</span>:</span>
<span id="cb184-8"><a href="transformer.html#cb184-8"></a>        tokens <span class="op">=</span> [token.lower() <span class="cf">for</span> token <span class="kw">in</span> sentence]</span>
<span id="cb184-9"><a href="transformer.html#cb184-9"></a>    tokens <span class="op">=</span> [src_field.init_token] <span class="op">+</span> tokens <span class="op">+</span> [src_field.eos_token]</span>
<span id="cb184-10"><a href="transformer.html#cb184-10"></a>    src_indexes <span class="op">=</span> [src_field.vocab.stoi[token] <span class="cf">for</span> token <span class="kw">in</span> tokens]</span>
<span id="cb184-11"><a href="transformer.html#cb184-11"></a>    src_tensor <span class="op">=</span> torch.LongTensor(src_indexes).unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="cb184-12"><a href="transformer.html#cb184-12"></a>    src_mask <span class="op">=</span> model.make_src_key_padding_mask(src_tensor)</span>
<span id="cb184-13"><a href="transformer.html#cb184-13"></a>    </span>
<span id="cb184-14"><a href="transformer.html#cb184-14"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb184-15"><a href="transformer.html#cb184-15"></a>        enc_src <span class="op">=</span> model.encoder(src_tensor, src_mask)</span>
<span id="cb184-16"><a href="transformer.html#cb184-16"></a>    trg_indexes <span class="op">=</span> [trg_field.vocab.stoi[trg_field.init_token]]</span>
<span id="cb184-17"><a href="transformer.html#cb184-17"></a>    </span>
<span id="cb184-18"><a href="transformer.html#cb184-18"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_len):</span>
<span id="cb184-19"><a href="transformer.html#cb184-19"></a>        trg_tensor <span class="op">=</span> torch.LongTensor(trg_indexes).unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="cb184-20"><a href="transformer.html#cb184-20"></a>        trg_key_padding_mask <span class="op">=</span> model.make_trg_key_padding_mask(trg_tensor)</span>
<span id="cb184-21"><a href="transformer.html#cb184-21"></a>        trg_mask <span class="op">=</span> model.make_trg_mask(trg_tensor)</span>
<span id="cb184-22"><a href="transformer.html#cb184-22"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb184-23"><a href="transformer.html#cb184-23"></a>            output <span class="op">=</span> model.decoder(trg_tensor, enc_src, trg_mask, trg_key_padding_mask)</span>
<span id="cb184-24"><a href="transformer.html#cb184-24"></a>            output <span class="op">=</span> torch.transpose(output, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb184-25"><a href="transformer.html#cb184-25"></a>            pred_token <span class="op">=</span> output.argmax(<span class="dv">2</span>)[:,<span class="op">-</span><span class="dv">1</span>].item()</span>
<span id="cb184-26"><a href="transformer.html#cb184-26"></a>            trg_indexes.append(pred_token)</span>
<span id="cb184-27"><a href="transformer.html#cb184-27"></a>            <span class="cf">if</span> pred_token <span class="op">==</span> trg_field.vocab.stoi[trg_field.eos_token]: <span class="cf">break</span></span>
<span id="cb184-28"><a href="transformer.html#cb184-28"></a>    trg_tokens <span class="op">=</span> [trg_field.vocab.itos[i] <span class="cf">for</span> i <span class="kw">in</span> trg_indexes]</span>
<span id="cb184-29"><a href="transformer.html#cb184-29"></a>    <span class="cf">return</span> trg_tokens[<span class="dv">1</span>:<span class="op">-</span><span class="dv">2</span>]</span></code></pre></div>
<p>Here, finally, the main loop:</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb185-1"><a href="transformer.html#cb185-1"></a>n_epochs <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb185-2"><a href="transformer.html#cb185-2"></a>clip <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb185-3"><a href="transformer.html#cb185-3"></a></span>
<span id="cb185-4"><a href="transformer.html#cb185-4"></a>example_idx <span class="op">=</span> [<span class="dv">11</span>, <span class="dv">77</span>, <span class="dv">133</span>, <span class="dv">241</span>, <span class="dv">333</span>, <span class="dv">477</span>, <span class="dv">555</span>, <span class="dv">777</span>]</span>
<span id="cb185-5"><a href="transformer.html#cb185-5"></a></span>
<span id="cb185-6"><a href="transformer.html#cb185-6"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb185-7"><a href="transformer.html#cb185-7"></a>    train_loss <span class="op">=</span> train(model, train_iterator, optimizer, criterion, clip)</span>
<span id="cb185-8"><a href="transformer.html#cb185-8"></a>    valid_loss <span class="op">=</span> evaluate(model, valid_iterator, criterion)</span>
<span id="cb185-9"><a href="transformer.html#cb185-9"></a>    test_loss <span class="op">=</span> evaluate(model, test_iterator, criterion)</span>
<span id="cb185-10"><a href="transformer.html#cb185-10"></a>    <span class="bu">print</span>(<span class="ss">f&#39;Epoch: </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:02}</span><span class="ss">&#39;</span>)</span>
<span id="cb185-11"><a href="transformer.html#cb185-11"></a>    <span class="bu">print</span>(<span class="ss">f&#39;</span><span class="ch">\t</span><span class="ss">Train Loss: </span><span class="sc">{</span>train_loss<span class="sc">:.3f}</span><span class="ss"> | Train PPL: </span><span class="sc">{</span>math<span class="sc">.</span>exp(train_loss)<span class="sc">:7.3f}</span><span class="ss">&#39;</span>)</span>
<span id="cb185-12"><a href="transformer.html#cb185-12"></a>    <span class="bu">print</span>(<span class="ss">f&#39;</span><span class="ch">\t</span><span class="ss"> Val. Loss: </span><span class="sc">{</span>valid_loss<span class="sc">:.3f}</span><span class="ss"> |  Val. PPL: </span><span class="sc">{</span>math<span class="sc">.</span>exp(valid_loss)<span class="sc">:7.3f}</span><span class="ss">&#39;</span>)</span>
<span id="cb185-13"><a href="transformer.html#cb185-13"></a>    <span class="bu">print</span>(<span class="ss">f&#39;</span><span class="ch">\t</span><span class="ss">Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">:.3f}</span><span class="ss"> | Test PPL: </span><span class="sc">{</span>math<span class="sc">.</span>exp(test_loss)<span class="sc">:7.3f}</span><span class="ss"> |&#39;</span>)</span>
<span id="cb185-14"><a href="transformer.html#cb185-14"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb185-15"><a href="transformer.html#cb185-15"></a>        example_src <span class="op">=</span> <span class="bu">vars</span>(train_data.examples[example_idx[i]])[<span class="st">&#39;src&#39;</span>]</span>
<span id="cb185-16"><a href="transformer.html#cb185-16"></a>        example_trg <span class="op">=</span> <span class="bu">vars</span>(train_data.examples[example_idx[i]])[<span class="st">&#39;trg&#39;</span>]</span>
<span id="cb185-17"><a href="transformer.html#cb185-17"></a>        translation <span class="op">=</span> translate_sentence(example_src, src_spec, trg_spec, model, device)</span>
<span id="cb185-18"><a href="transformer.html#cb185-18"></a>        src_sentence <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(i <span class="cf">for</span> i <span class="kw">in</span> example_src)</span>
<span id="cb185-19"><a href="transformer.html#cb185-19"></a>        target_sentence <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(i <span class="cf">for</span> i <span class="kw">in</span> example_trg)</span>
<span id="cb185-20"><a href="transformer.html#cb185-20"></a>        translated_sentence <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(i <span class="cf">for</span> i <span class="kw">in</span> translation)</span>
<span id="cb185-21"><a href="transformer.html#cb185-21"></a>        <span class="bu">print</span>(<span class="st">&quot;Source: &quot;</span> <span class="op">+</span> src_sentence)</span>
<span id="cb185-22"><a href="transformer.html#cb185-22"></a>        <span class="bu">print</span>(<span class="st">&quot;Target: &quot;</span> <span class="op">+</span> target_sentence)</span>
<span id="cb185-23"><a href="transformer.html#cb185-23"></a>        <span class="bu">print</span>(<span class="st">&quot;Predicted: &quot;</span> <span class="op">+</span> translated_sentence <span class="op">+</span> <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<p>And now, let’s see! How well did that work?</p>
</div>
</div>
<div id="results" class="section level2" number="10.4">
<h2><span class="header-section-number">10.4</span> Results</h2>
<p>Like in the previous chapter, we show losses and translations after epochs 1, 5, and 9.</p>
<p>[…]</p>
<pre><code>Epoch: 01 
    Train Loss: 5.286 | Train PPL: 197.526
      Val. Loss: 3.937 |  Val. PPL:  51.272
    Test Loss: 3.937 | Test PPL:  51.272 |


Epoch: 05
    Train Loss: 2.844 | Train PPL:  17.187
      Val. Loss: 3.296 |  Val. PPL:  27.016
      Test Loss: 3.296 | Test PPL:  27.016 |
    
Epoch: 09
    Train Loss: 2.318 | Train PPL:  10.152
      Val. Loss: 3.391 |  Val. PPL:  29.701
      Test Loss: 3.391 | Test PPL:  29.701 |</code></pre>
<table>
<colgroup>
<col width="9%" />
<col width="90%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>most of the earthquakes and volcanoes are in the sea , at the bottom of the sea .</td>
</tr>
<tr class="even">
<td>Target</td>
<td>většina zemětřesení a vulkánů je v moři - na mořském dně .</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>většina z moře a &lt;unk v moře .</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>většina zemětřesení a &lt;unk jsou v moři , na mořském dně .</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>většina zemětřesení a &lt;unk jsou v moře a dole na dně moře z moře .</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>and we knew it was volcanic back in the ’ 60s , ’ 70s .</td>
</tr>
<tr class="even">
<td>Target</td>
<td>víme , že tam byl vulkanismus , v 60 . a 70 . letech .</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>a věděl jsme , že v 60 . letech , 70 . letech .</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>věděli jsme , že jsme , že jsme to bylo v 70 . 70 . letech .</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>věděli jsme , že je ropa , v 60 . léta .</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="3%" />
<col width="96%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>so here , you ’ve got this valley with this incredible alien landscape of pillars and hot springs and volcanic eruptions and earthquakes , inhabited by these very strange animals that live only on chemical energy coming out of the ground .</td>
</tr>
<tr class="even">
<td>Target</td>
<td>takže tady máme to údolí s mimořádně nepřátelskou krajinou sloupů , horkých pramenů , vulkanických erupcí a zemětřesení , obydlených těmito velmi zvláštními živočichy , co žijí pouze z chemické energie vycházející ze země .</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>tady máme údolí , s touto údolí &lt;unk &lt;unk a &lt;unk &lt;unk &lt;unk &lt;unk &lt;unk &lt;unk a &lt;unk &lt;unk tyto &lt;unk &lt;unk &lt;unk &lt;unk , které žijí jen na povrch .</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>takže tady máte tento údolí &lt;unk &lt;unk &lt;unk &lt;unk &lt;unk &lt;unk &lt;unk a zemětřesení a zemětřesení , &lt;unk &lt;unk &lt;unk tyto velmi neobvyklé živočichy , které žijí na tomto místě , které žijí na zemi .</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>tady máte tady s touto neuvěřitelnou čistotu .</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>and instead , what do we value ?</td>
</tr>
<tr class="even">
<td>Target</td>
<td>a místo toho , čeho si vážíme ?</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>a místo , co děláme hodnotu ?</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>a místo toho , co děláme ?</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>a místo toho , co děláme ?</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>these guys are facts .</td>
</tr>
<tr class="even">
<td>Target</td>
<td>tohle jsou fakta , lidi .</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>tito jsou fakta .</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>tihle chlapíci jsou fakta .</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>tito chlapíci jsou fakta .</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="7%" />
<col width="92%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>we see in other countries that it matters much less into which social context you ’re born .</td>
</tr>
<tr class="even">
<td>Target</td>
<td>v jiných zemích naopak vidíme , že záleží mnohem méně na tom , do jaké sociální vrstvy se kdo narodí .</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>vidíme , že v jiných zemích záleží na sociální kontextu , které se &lt;unk&gt; .</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>vidíme v jiných zemích , které záleží na tom , že záleží na tom , že se rodíme .</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>vidíme v jiných zemích , které záleží na tom , že záleží mnohem méně sociální kontext , ve kterém se dostanete .</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>how do the media talk about schools and teachers ?</td>
</tr>
<tr class="even">
<td>Target</td>
<td>jak reflektují školy a učitele média ?</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>jak se média o školách a učitelé ?</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>jak se média mluví o školách a učitelé ?</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>jak se média mluví o školách a učitelé ?</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="6%" />
<col width="93%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>when peter moves his arm , that yellow spot you see there is the interface to the functioning of peter ’s mind taking place .</td>
</tr>
<tr class="even">
<td>Target</td>
<td>když petr pohne svojí paží , ta žlutá tečka , kterou vidíte tady je rozhraní petrovi mysli k této aktivitě .</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>když petr pohne svojí paží , ta žlutá tečka , kterou vidíte tady je rozhraní petrovi mysli k této aktivitě .</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>když peter diamandis svého paže , která je zde rozhraní mezi tímhle rozhraním , které se peter diamandis .</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>když se peter paži , ta žlutá tečka , kterou vidíte je rozhraní petrovi mysli k fungující peter &lt;unk&gt; si místo .</td>
</tr>
</tbody>
</table>
<p>From the author’s very limited – and rusty – knowledge of Czech, this does not look so bad – but up to you to form on
opinion on a dataset of your choice!</p>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-2016arXiv160706450L">
<p>Lei Ba, Jimmy, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. “Layer Normalization.” <em>arXiv E-Prints</em>, July, arXiv:1607.06450. <a href="http://arxiv.org/abs/1607.06450">http://arxiv.org/abs/1607.06450</a>.</p>
</div>
<div id="ref-VaswaniSPUJGKP17">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” <em>CoRR</em> abs/1706.03762. <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>Here “pointwise” implies that the weights are the same for every input fed into the layer. These feedforward neural
networks would therefore be analogous to convolutional layers with kernel siez 1.<a href="transformer.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>There are four different language pairings in the dataset overall, each involving English.<a href="transformer.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>e.g., in <span class="citation">(Devlin et al. <a href="#ref-abs-1810-04805" role="doc-biblioref">2018</a>)</span><a href="transformer.html#fnref11" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="seq2seq-att.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tabular-intro.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
