<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 Torch transformer modules | Torch book</title>
  <meta name="description" content="tbd" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="11 Torch transformer modules | Torch book" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="tbd" />
  <meta property="og:image" content="tbdcover.png" />
  <meta property="og:description" content="tbd" />
  <meta name="github-repo" content="tbd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Torch transformer modules | Torch book" />
  
  <meta name="twitter:description" content="tbd" />
  <meta name="twitter:image" content="tbdcover.png" />

<meta name="author" content="tbd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="seq2seq-att.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs-2.1/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
</ul></li>
<li class="part"><span><b>I Using Torch</b></span></li>
<li class="chapter" data-level="" data-path="using-torch-intro.html"><a href="using-torch-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="2" data-path="simple-net-R.html"><a href="simple-net-R.html"><i class="fa fa-check"></i><b>2</b> A simple neural network in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#whats-in-a-network"><i class="fa fa-check"></i><b>2.1</b> What’s in a network?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="simple-net-R.html"><a href="simple-net-R.html#gradient-descent"><i class="fa fa-check"></i><b>2.1.1</b> Gradient descent</a></li>
<li class="chapter" data-level="2.1.2" data-path="simple-net-R.html"><a href="simple-net-R.html#from-linear-regression-to-a-simple-network"><i class="fa fa-check"></i><b>2.1.2</b> From linear regression to a simple network</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#a-simple-network"><i class="fa fa-check"></i><b>2.2</b> A simple network</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#simulate-data"><i class="fa fa-check"></i><b>2.2.1</b> Simulate data</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#initialize-weights-and-biases"><i class="fa fa-check"></i><b>2.2.2</b> Initialize weights and biases</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>2.2.3</b> Training loop</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple-net-R.html"><a href="simple-net-R.html#complete-code"><i class="fa fa-check"></i><b>2.2.4</b> Complete code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>2.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html"><i class="fa fa-check"></i><b>3</b> Modifying the simple network to use torch tensors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#simple-network-torchified-step-1"><i class="fa fa-check"></i><b>3.1</b> Simple network torchified, step 1</a></li>
<li class="chapter" data-level="3.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#more-on-tensors"><i class="fa fa-check"></i><b>3.2</b> More on tensors</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#creating-tensors"><i class="fa fa-check"></i><b>3.2.1</b> Creating tensors</a></li>
<li class="chapter" data-level="3.2.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#conversion-from-and-to-r"><i class="fa fa-check"></i><b>3.2.2</b> Conversion from and to R</a></li>
<li class="chapter" data-level="3.2.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#indexing-and-slicing-tensors"><i class="fa fa-check"></i><b>3.2.3</b> Indexing and slicing tensors</a></li>
<li class="chapter" data-level="3.2.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#reshaping-tensors"><i class="fa fa-check"></i><b>3.2.4</b> Reshaping tensors</a></li>
<li class="chapter" data-level="3.2.5" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#operations-on-tensors"><i class="fa fa-check"></i><b>3.2.5</b> Operations on tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#broadcasting"><i class="fa fa-check"></i><b>3.3</b> Broadcasting</a></li>
<li class="chapter" data-level="3.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#running-on-gpu"><i class="fa fa-check"></i><b>3.4</b> Running on GPU</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html"><i class="fa fa-check"></i><b>4</b> Using autograd</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#automatic-differentiation-with-autograd"><i class="fa fa-check"></i><b>4.1</b> Automatic differentiation with autograd</a></li>
<li class="chapter" data-level="4.2" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#the-simple-network-now-using-autograd"><i class="fa fa-check"></i><b>4.2</b> The simple network, now using autograd</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple-net-modules.html"><a href="simple-net-modules.html"><i class="fa fa-check"></i><b>5</b> Using torch modules</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#modules"><i class="fa fa-check"></i><b>5.1</b> Modules</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#layers-as-modules"><i class="fa fa-check"></i><b>5.1.1</b> Layers as modules</a></li>
<li class="chapter" data-level="5.1.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#models-as-modules"><i class="fa fa-check"></i><b>5.1.2</b> Models as modules</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#simple-network-using-modules"><i class="fa fa-check"></i><b>5.2</b> Simple network using modules</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="simple-net-optim.html"><a href="simple-net-optim.html"><i class="fa fa-check"></i><b>6</b> Using torch optimizers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="simple-net-optim.html"><a href="simple-net-optim.html#torch-optimizers"><i class="fa fa-check"></i><b>6.1</b> Torch optimizers</a></li>
<li class="chapter" data-level="6.2" data-path="simple-net-optim.html"><a href="simple-net-optim.html#simple-net-with-torch.optim"><i class="fa fa-check"></i><b>6.2</b> Simple net with <code>torch.optim</code></a></li>
</ul></li>
<li class="part"><span><b>II Deep learning: classical applications</b></span></li>
<li class="chapter" data-level="" data-path="deeplearning-applications-intro.html"><a href="deeplearning-applications-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="7" data-path="image-classification.html"><a href="image-classification.html"><i class="fa fa-check"></i><b>7</b> Classifying images</a>
<ul>
<li class="chapter" data-level="7.1" data-path="image-classification.html"><a href="image-classification.html#data-loading-and-transformation"><i class="fa fa-check"></i><b>7.1</b> Data loading and transformation</a></li>
<li class="chapter" data-level="7.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>7.2</b> Model</a></li>
<li class="chapter" data-level="7.3" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>7.3</b> Training</a></li>
<li class="chapter" data-level="7.4" data-path="image-classification.html"><a href="image-classification.html#performance-on-the-test-set"><i class="fa fa-check"></i><b>7.4</b> Performance on the test set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="gans.html"><a href="gans.html"><i class="fa fa-check"></i><b>8</b> Generative adversarial networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>8.1</b> Dataset</a></li>
<li class="chapter" data-level="8.2" data-path="gans.html"><a href="gans.html#model-1"><i class="fa fa-check"></i><b>8.2</b> Model</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="gans.html"><a href="gans.html#generator"><i class="fa fa-check"></i><b>8.2.1</b> Generator</a></li>
<li class="chapter" data-level="8.2.2" data-path="gans.html"><a href="gans.html#discriminator"><i class="fa fa-check"></i><b>8.2.2</b> Discriminator</a></li>
<li class="chapter" data-level="8.2.3" data-path="gans.html"><a href="gans.html#optimizers-and-loss-function"><i class="fa fa-check"></i><b>8.2.3</b> Optimizers and loss function</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="gans.html"><a href="gans.html#training-loop-1"><i class="fa fa-check"></i><b>8.3</b> Training loop</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="vaes.html"><a href="vaes.html"><i class="fa fa-check"></i><b>9</b> Variational autoencoders</a>
<ul>
<li class="chapter" data-level="9.1" data-path="vaes.html"><a href="vaes.html#dataset-1"><i class="fa fa-check"></i><b>9.1</b> Dataset</a></li>
<li class="chapter" data-level="9.2" data-path="vaes.html"><a href="vaes.html#model-2"><i class="fa fa-check"></i><b>9.2</b> Model</a></li>
<li class="chapter" data-level="9.3" data-path="vaes.html"><a href="vaes.html#training-the-vae"><i class="fa fa-check"></i><b>9.3</b> Training the VAE</a></li>
<li class="chapter" data-level="9.4" data-path="vaes.html"><a href="vaes.html#latent-space"><i class="fa fa-check"></i><b>9.4</b> Latent space</a></li>
</ul></li>
<li class="part"><span><b>III Intermediate deep learning</b></span></li>
<li class="chapter" data-level="" data-path="intermediate-DL-intro.html"><a href="intermediate-DL-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="10" data-path="seq2seq-att.html"><a href="seq2seq-att.html"><i class="fa fa-check"></i><b>10</b> Sequence-to-sequence models with attention</a>
<ul>
<li class="chapter" data-level="10.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#why-attention"><i class="fa fa-check"></i><b>10.1</b> Why attention?</a></li>
<li class="chapter" data-level="10.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#preprocessing-with-torchtext"><i class="fa fa-check"></i><b>10.2</b> Preprocessing with <code>torchtext</code></a></li>
<li class="chapter" data-level="10.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#model-3"><i class="fa fa-check"></i><b>10.3</b> Model</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>10.3.1</b> Encoder</a></li>
<li class="chapter" data-level="10.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#attention-module"><i class="fa fa-check"></i><b>10.3.2</b> Attention module</a></li>
<li class="chapter" data-level="10.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>10.3.3</b> Decoder</a></li>
<li class="chapter" data-level="10.3.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#seq2seq-module"><i class="fa fa-check"></i><b>10.3.4</b> Seq2seq module</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#training-and-evaluation"><i class="fa fa-check"></i><b>10.4</b> Training and evaluation</a></li>
<li class="chapter" data-level="10.5" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>10.5</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="transformer.html"><a href="transformer.html"><i class="fa fa-check"></i><b>11</b> Torch transformer modules</a>
<ul>
<li class="chapter" data-level="11.1" data-path="transformer.html"><a href="transformer.html#attention-is-all-you-need"><i class="fa fa-check"></i><b>11.1</b> “Attention is all you need”</a></li>
<li class="chapter" data-level="11.2" data-path="transformer.html"><a href="transformer.html#implementation-building-blocks"><i class="fa fa-check"></i><b>11.2</b> Implementation: building blocks</a></li>
<li class="chapter" data-level="11.3" data-path="transformer.html"><a href="transformer.html#results-1"><i class="fa fa-check"></i><b>11.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Torch book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="transformer" class="section level1" number="11">
<h1><span class="header-section-number">11</span> Torch transformer modules</h1>
<div id="attention-is-all-you-need" class="section level2" number="11.1">
<h2><span class="header-section-number">11.1</span> “Attention is all you need”</h2>
<p>When the original Transformer paper <span class="citation">(Vaswani et al. <a href="#ref-VaswaniSPUJGKP17" role="doc-biblioref">2017</a>)</span> appeared, its provocative title can only have speeded up its road to
fame. Why would it be provocative? At that time, sequential data were firmly thought to be the realm of RNNs (albeit extended
by encoder-decoder attention): If one input token’s probability depends on the previous input(s) (as in time series, language,
or music), it seems we need to keep some form of <em>state</em> to preserve sequential relationships over the whole calculation.</p>
<p>In fact, Transformer did not have RNNs, but compensated for lacking state in two ways: adding <em>positional encoding</em>, thus in
some way keeping track of where in the phrase a token is located, and most importantly, <em>self-attention</em>: making use of
context (surrounding tokens) when encoding each input token. With self-attention, <em>no token is an island</em>; instead, it only
gains its meaning through how it relates to its neighbors.</p>
<p>Seeing how excellent architectural explanations abund, ranging from code-oriented <span class="citation">(Rush <a href="#ref-rush-2018-annotated" role="doc-biblioref">2018</a>)</span> to
<a href="https://jalammar.github.io/illustrated-transformer/">visual</a>, we just give a brief conceptual characterization.</p>
<div id="self-attention-and-multi-head-attention" class="section level4" number="11.1.0.1">
<h4><span class="header-section-number">11.1.0.1</span> Self-attention and “multi-head attention”</h4>
<p>Each input word (after the usual embedding) plays three roles, designated by terms coming from <em>information retrieval</em>: query,
key, and value. In this chapter, we won’t be coding attention from scratch, but relying on <code>torch</code> modules; so strictly, we
don’t need to go there. But as query, key and value vectors have become part of the official transformer lingo, it’s good to
have heard those terms.</p>
<p>In a nutshell, self-attention does not encode every word separately, but at every position, works with a conglomerate of
semantic and syntactic information that is made up, in some way, of the complete input phrase. The basic operation, like in
the encoder-decoder setup, is a <em>dot product</em> used to determine some form of similarity/promixity/relevance in semantic space.</p>
<p>This dot product occurs between the word that is being encoded – appearing in its role as <em>query vector</em> – and every other
word, each wearing their <em>key vector</em> hats. Essentially, these measures of affinity are normalized and used to weight the
<em>value vectors</em> corresponding to every <em>key</em> that was used in the comparison. Finally, for every <em>query</em> we aggregate the
weighted value vectors into a composite result, which is passed on to the next layer.</p>
<p>Why does each token have to wear three different hats? If it didn’t, these affinity relationships would be symmetric (the dot
product per se being commutative), thus badly conforming with semantic and (especially!) syntactic reality. Thus, technically,
a word’s query, key and value vectors are not the same; instead, each is obtained as the output of a different feedforward
layer.</p>
<p>So that is self-attention - now what does “multi-head attention” refer to? This simply is a “bag of attention modules”, all
operating in parallel. Like that, multi-head attention is said to take care of taking into account multiple “representation
subspaces”.</p>
</div>
<div id="overall-architecture" class="section level4" number="11.1.0.2">
<h4><span class="header-section-number">11.1.0.2</span> Overall architecture</h4>
<p>Overall, transformer is an encoder stack, followed by a decoder stack. Both stacks are composed of several, identical
submodules combining multi-head attention, layer normalization <span class="citation">(Lei Ba, Kiros, and Hinton <a href="#ref-2016arXiv160706450L" role="doc-biblioref">2016</a>)</span>, residual connections, and feedforward
neural networks applied pointwise to each input.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> In addition to the self-attention mechanism (conceptually) shared with
encoder layers, the decoder layers exercise a second form of attention: <em>encoder-decoder</em> attention allows them to
differentially pay attention to the output passed by the encoder.</p>
</div>
</div>
<div id="implementation-building-blocks" class="section level2" number="11.2">
<h2><span class="header-section-number">11.2</span> Implementation: building blocks</h2>
<p>While it is certainly possible, and instructive, to build a transformer network from scratch, we won’t reinvent the wheel but
instead, make use of <code>torch</code> layers that simplify the process significantly. <code>TransformerEncoderLayer</code> and
<code>TransformerDecoderLayer</code> are the basic modules that make up encoder and decoder stacks, respectively.</p>
<p>Here is a single encoder submodule comprising multi-head self-attention, layer normalization and feedforward networks:</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb166-1"><a href="transformer.html#cb166-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> TransformerEncoderLayer</span>
<span id="cb166-2"><a href="transformer.html#cb166-2"></a></span>
<span id="cb166-3"><a href="transformer.html#cb166-3"></a>e <span class="op">=</span> TransformerEncoderLayer(d_model <span class="op">=</span> <span class="dv">256</span>, nhead <span class="op">=</span> <span class="dv">2</span>, dim_feedforward <span class="op">=</span> <span class="dv">256</span>, dropout <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb166-4"><a href="transformer.html#cb166-4"></a>e</span></code></pre></div>
<pre><code>TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): Linear(in_features=256, out_features=256, bias=True)
  )
  (linear1): Linear(in_features=256, out_features=256, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
  (linear2): Linear(in_features=256, out_features=256, bias=True)
  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.2, inplace=False)
  (dropout2): Dropout(p=0.2, inplace=False)
)</code></pre>
<p>A decoder submodule looks similar, apart from the fact that it has an additional <code>MultiHeadAttention</code> (sub-)submodule. One is
for self-attention, the other, for attending to encoder input:</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb168-1"><a href="transformer.html#cb168-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> TransformerDecoderLayer</span>
<span id="cb168-2"><a href="transformer.html#cb168-2"></a></span>
<span id="cb168-3"><a href="transformer.html#cb168-3"></a>d <span class="op">=</span> TransformerDecoderLayer(d_model <span class="op">=</span> <span class="dv">256</span>, nhead <span class="op">=</span> <span class="dv">2</span>, dim_feedforward <span class="op">=</span> <span class="dv">256</span>, dropout <span class="op">=</span> <span class="fl">0.2</span>)</span></code></pre></div>
<p>Next up in the hierarchy are <code>TransformerEncoder</code> and <code>TransformerDecoder</code>. These are just containers, making up the
respective stacks:</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb169-1"><a href="transformer.html#cb169-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> TransformerEncoder</span>
<span id="cb169-2"><a href="transformer.html#cb169-2"></a></span>
<span id="cb169-3"><a href="transformer.html#cb169-3"></a>TransformerEncoder(encoder_layer <span class="op">=</span> e, num_layers <span class="op">=</span> <span class="dv">6</span>)</span></code></pre></div>
<div class="sourceCode" id="cb170"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb170-1"><a href="transformer.html#cb170-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> TransformerDecoder</span>
<span id="cb170-2"><a href="transformer.html#cb170-2"></a></span>
<span id="cb170-3"><a href="transformer.html#cb170-3"></a>TransformerDecoder(decoder_layer <span class="op">=</span> d, num_layers <span class="op">=</span> <span class="dv">6</span>)</span></code></pre></div>
<p>There is even a <code>Transformer</code> module that takes in parameters for the sublayers
(<code>TransformerEncoderLayer</code>/<code>TransformerDecoderLayer</code>) as well as the containers (<code>TransformerEncoder</code>/<code>TransformerDecoder</code>).</p>
</div>
<div id="results-1" class="section level2" number="11.3">
<h2><span class="header-section-number">11.3</span> Results</h2>
<p>Like in the previous chapter, we show losses and translations after epochs 1, 5, and 9.</p>
<p>[…]</p>
<pre><code>Epoch: 01 
    Train Loss: 5.286 | Train PPL: 197.526
      Val. Loss: 3.937 |  Val. PPL:  51.272
    Test Loss: 3.937 | Test PPL:  51.272 |


Epoch: 05
    Train Loss: 2.844 | Train PPL:  17.187
      Val. Loss: 3.296 |  Val. PPL:  27.016
      Test Loss: 3.296 | Test PPL:  27.016 |
    
Epoch: 09
    Train Loss: 2.318 | Train PPL:  10.152
      Val. Loss: 3.391 |  Val. PPL:  29.701
      Test Loss: 3.391 | Test PPL:  29.701 |</code></pre>
<table>
<colgroup>
<col width="8%" />
<col width="92%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>most of the earthquakes and volcanoes are in the sea , at the bottom of the sea .</td>
</tr>
<tr class="even">
<td>Target</td>
<td>většina zemětřesení a vulkánů je v moři - na mořském dně .</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>většina z moře a &lt;unk v moře .</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>většina zemětřesení a &lt;unk jsou v moři , na mořském dně .</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>většina zemětřesení a &lt;unk jsou v moře a dole na dně moře z moře .</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="7%" />
<col width="92%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>and we knew it was volcanic back in the ’ 60s , ’ 70s .</td>
</tr>
<tr class="even">
<td>Target</td>
<td>víme , že tam byl vulkanismus , v 60 . a 70 . letech .</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>a věděl jsme , že v 60 . letech , 70 . letech .</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>věděli jsme , že jsme , že jsme to bylo v 70 . 70 . letech .</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>věděli jsme , že je ropa , v 60 . léta .</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="8%" />
<col width="92%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>so here , you ’ve got this valley with this incredible alien landscape of pillars and hot springs and volcanic
eruptions and earthquakes , inhabited by these very strange animals that live only on chemical energy coming out
of the ground .</td>
</tr>
<tr class="even">
<td>Target</td>
<td>takže tady máme to údolí s mimořádně nepřátelskou krajinou sloupů , horkých pramenů , vulkanických erupcí a
zemětřesení , obydlených těmito velmi zvláštními živočichy , co žijí pouze z chemické energie vycházející ze
země .</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>tady máme údolí , s touto údolí &lt;unk &lt;unk a &lt;unk &lt;unk &lt;unk &lt;unk &lt;unk &lt;unk a &lt;unk &lt;unk tyto &lt;unk &lt;unk
&lt;unk &lt;unk , které žijí jen na povrch .</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>takže tady máte tento údolí &lt;unk &lt;unk &lt;unk &lt;unk &lt;unk &lt;unk &lt;unk a zemětřesení a zemětřesení , &lt;unk &lt;unk
&lt;unk tyto velmi neobvyklé živočichy , které žijí na tomto místě , které žijí na zemi .</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>tady máte tady s touto neuvěřitelnou čistotu .</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="8%" />
<col width="91%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>and instead , what do we value ?</td>
</tr>
<tr class="even">
<td>Target</td>
<td>a místo toho , čeho si vážíme ?</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>a místo , co děláme hodnotu ?</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>a místo toho , co děláme ?</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>a místo toho , co děláme ?</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="8%" />
<col width="91%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>these guys are facts .</td>
</tr>
<tr class="even">
<td>Target</td>
<td>tohle jsou fakta , lidi .</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>tito jsou fakta .</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>tihle chlapíci jsou fakta .</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>tito chlapíci jsou fakta .</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="8%" />
<col width="91%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>we see in other countries that it matters much less into which social context you ’re born .</td>
</tr>
<tr class="even">
<td>Target</td>
<td>v jiných zemích naopak vidíme , že záleží mnohem méně na tom , do jaké sociální vrstvy se kdo narodí .</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>vidíme , že v jiných zemích záleží na sociální kontextu , které se &lt;unk&gt; .</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>vidíme v jiných zemích , které záleží na tom , že záleží na tom , že se rodíme .</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>vidíme v jiných zemích , které záleží na tom , že záleží mnohem méně sociální kontext , ve kterém se dostanete .</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Source</td>
<td>how do the media talk about schools and teachers ?</td>
</tr>
<tr class="even">
<td>Target</td>
<td>jak reflektují školy a učitele média ?</td>
</tr>
<tr class="odd">
<td>Epoch 1</td>
<td>jak se média o školách a učitelé ?</td>
</tr>
<tr class="even">
<td>Epoch 5</td>
<td>jak se média mluví o školách a učitelé ?</td>
</tr>
<tr class="odd">
<td>Epoch 9</td>
<td>jak se média mluví o školách a učitelé ?</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="5%" />
<col width="94%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>So
urce</td>
<td>when peter moves his arm , that yellow spot you see there is the interface to the functioning of peter ’s mind
taking place .</td>
</tr>
<tr class="even">
<td>Ta
rget</td>
<td>když petr pohne svojí paží , ta žlutá tečka , kterou vidíte tady je rozhraní petrovi mysli k této aktivitě .</td>
</tr>
<tr class="odd">
<td>E
poch
1</td>
<td>když petr pohne svojí paží , ta žlutá tečka , kterou vidíte tady je rozhraní petrovi mysli k této aktivitě .</td>
</tr>
<tr class="even">
<td>E
poch
5</td>
<td>když peter diamandis svého paže , která je zde rozhraní mezi tímhle rozhraním , které se peter diamandis .</td>
</tr>
<tr class="odd">
<td>E
poch
9</td>
<td>když se peter paži , ta žlutá tečka , kterou vidíte je rozhraní petrovi mysli k fungující peter &lt;unk&gt; si místo .</td>
</tr>
</tbody>
</table>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-2016arXiv160706450L">
<p>Lei Ba, Jimmy, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. “Layer Normalization.” <em>arXiv E-Prints</em>, July, arXiv:1607.06450. <a href="http://arxiv.org/abs/1607.06450">http://arxiv.org/abs/1607.06450</a>.</p>
</div>
<div id="ref-rush-2018-annotated">
<p>Rush, Alexander. 2018. “The Annotated Transformer.” In <em>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</em>, 52–60. Melbourne, Australia: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/W18-2509">https://doi.org/10.18653/v1/W18-2509</a>.</p>
</div>
<div id="ref-VaswaniSPUJGKP17">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” <em>CoRR</em> abs/1706.03762. <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="seq2seq-att.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
