<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Using autograd | Applied deep learning with torch from R</title>
  <meta name="description" content="tbd" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Using autograd | Applied deep learning with torch from R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="tbd" />
  <meta property="og:image" content="tbdcover.png" />
  <meta property="og:description" content="tbd" />
  <meta name="github-repo" content="tbd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Using autograd | Applied deep learning with torch from R" />
  
  <meta name="twitter:description" content="tbd" />
  <meta name="twitter:image" content="tbdcover.png" />

<meta name="author" content="tbd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-net-tensors.html"/>
<link rel="next" href="simple-net-modules.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
</ul></li>
<li class="part"><span><b>I Using Torch</b></span></li>
<li class="chapter" data-level="" data-path="using-torch-intro.html"><a href="using-torch-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="2" data-path="simple-net-R.html"><a href="simple-net-R.html"><i class="fa fa-check"></i><b>2</b> A simple neural network in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#whats-in-a-network"><i class="fa fa-check"></i><b>2.1</b> What’s in a network?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="simple-net-R.html"><a href="simple-net-R.html#gradient-descent"><i class="fa fa-check"></i><b>2.1.1</b> Gradient descent</a></li>
<li class="chapter" data-level="2.1.2" data-path="simple-net-R.html"><a href="simple-net-R.html#from-linear-regression-to-a-simple-network"><i class="fa fa-check"></i><b>2.1.2</b> From linear regression to a simple network</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#a-simple-network"><i class="fa fa-check"></i><b>2.2</b> A simple network</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#simulate-data"><i class="fa fa-check"></i><b>2.2.1</b> Simulate data</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#initialize-weights-and-biases"><i class="fa fa-check"></i><b>2.2.2</b> Initialize weights and biases</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>2.2.3</b> Training loop</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple-net-R.html"><a href="simple-net-R.html#complete-code"><i class="fa fa-check"></i><b>2.2.4</b> Complete code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html"><i class="fa fa-check"></i><b>3</b> Modifying the simple network to use torch tensors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#tensors"><i class="fa fa-check"></i><b>3.1</b> Tensors</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#creation"><i class="fa fa-check"></i><b>3.1.1</b> Creation</a></li>
<li class="chapter" data-level="3.1.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#conversion-to-built-in-r-data-types"><i class="fa fa-check"></i><b>3.1.2</b> Conversion to built-in R data types</a></li>
<li class="chapter" data-level="3.1.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#indexing-and-slicing-tensors"><i class="fa fa-check"></i><b>3.1.3</b> Indexing and slicing tensors</a></li>
<li class="chapter" data-level="3.1.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#reshaping-tensors"><i class="fa fa-check"></i><b>3.1.4</b> Reshaping tensors</a></li>
<li class="chapter" data-level="3.1.5" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#operations-on-tensors"><i class="fa fa-check"></i><b>3.1.5</b> Operations on tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#running-on-gpu"><i class="fa fa-check"></i><b>3.2</b> Running on GPU</a></li>
<li class="chapter" data-level="3.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#broadcasting"><i class="fa fa-check"></i><b>3.3</b> Broadcasting</a></li>
<li class="chapter" data-level="3.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#simple-neural-network-using-torch-tensors"><i class="fa fa-check"></i><b>3.4</b> Simple neural network using <code>torch</code> tensors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html"><i class="fa fa-check"></i><b>4</b> Using autograd</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#automatic-differentiation-with-autograd"><i class="fa fa-check"></i><b>4.1</b> Automatic differentiation with <em>autograd</em></a></li>
<li class="chapter" data-level="4.2" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#the-simple-network-now-using-autograd"><i class="fa fa-check"></i><b>4.2</b> The simple network, now using <em>autograd</em></a></li>
<li class="chapter" data-level="4.3" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#outlook"><i class="fa fa-check"></i><b>4.3</b> Outlook</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple-net-modules.html"><a href="simple-net-modules.html"><i class="fa fa-check"></i><b>5</b> Using torch modules</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#modules"><i class="fa fa-check"></i><b>5.1</b> Modules</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#layers-as-modules"><i class="fa fa-check"></i><b>5.1.1</b> Layers as modules</a></li>
<li class="chapter" data-level="5.1.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#models-as-modules"><i class="fa fa-check"></i><b>5.1.2</b> Models as modules</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#simple-network-using-modules"><i class="fa fa-check"></i><b>5.2</b> Simple network using modules</a></li>
<li class="chapter" data-level="5.3" data-path="simple-net-modules.html"><a href="simple-net-modules.html#appendix-python-code"><i class="fa fa-check"></i><b>5.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="simple-net-optim.html"><a href="simple-net-optim.html"><i class="fa fa-check"></i><b>6</b> Using <code>torch</code> optimizers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="simple-net-optim.html"><a href="simple-net-optim.html#torch-optimizers"><i class="fa fa-check"></i><b>6.1</b> Torch optimizers</a></li>
<li class="chapter" data-level="6.2" data-path="simple-net-optim.html"><a href="simple-net-optim.html#simple-net-with-optim"><i class="fa fa-check"></i><b>6.2</b> Simple net with <code>optim</code></a></li>
<li class="chapter" data-level="6.3" data-path="simple-net-modules.html"><a href="simple-net-modules.html#appendix-python-code"><i class="fa fa-check"></i><b>6.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="part"><span><b>II Image Recognition</b></span></li>
<li class="chapter" data-level="" data-path="image-recognition-intro.html"><a href="image-recognition-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="7" data-path="image-classification.html"><a href="image-classification.html"><i class="fa fa-check"></i><b>7</b> Classifying images</a>
<ul>
<li class="chapter" data-level="7.1" data-path="image-classification.html"><a href="image-classification.html#data-loading-and-transformation"><i class="fa fa-check"></i><b>7.1</b> Data loading and transformation</a></li>
<li class="chapter" data-level="7.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>7.2</b> Model</a></li>
<li class="chapter" data-level="7.3" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>7.3</b> Training</a></li>
<li class="chapter" data-level="7.4" data-path="image-classification.html"><a href="image-classification.html#performance-on-the-test-set"><i class="fa fa-check"></i><b>7.4</b> Performance on the test set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="unet.html"><a href="unet.html"><i class="fa fa-check"></i><b>8</b> Brain Image Segmentation with U-Net</a>
<ul>
<li class="chapter" data-level="8.1" data-path="unet.html"><a href="unet.html#image-segmentation-in-a-nutshell"><i class="fa fa-check"></i><b>8.1</b> Image segmentation in a nutshell</a></li>
<li class="chapter" data-level="8.2" data-path="unet.html"><a href="unet.html#u-net"><i class="fa fa-check"></i><b>8.2</b> U-Net</a></li>
<li class="chapter" data-level="8.3" data-path="unet.html"><a href="unet.html#example-application-mri-images"><i class="fa fa-check"></i><b>8.3</b> Example application: MRI images</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="unet.html"><a href="unet.html#preprocessing"><i class="fa fa-check"></i><b>8.3.1</b> Preprocessing</a></li>
<li class="chapter" data-level="8.3.2" data-path="unet.html"><a href="unet.html#u-net-model"><i class="fa fa-check"></i><b>8.3.2</b> U-Net model</a></li>
<li class="chapter" data-level="8.3.3" data-path="unet.html"><a href="unet.html#loss"><i class="fa fa-check"></i><b>8.3.3</b> Loss</a></li>
<li class="chapter" data-level="8.3.4" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>8.3.4</b> Training</a></li>
<li class="chapter" data-level="8.3.5" data-path="unet.html"><a href="unet.html#predictions"><i class="fa fa-check"></i><b>8.3.5</b> Predictions</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Natural language processing</b></span></li>
<li class="chapter" data-level="" data-path="NLP-intro.html"><a href="NLP-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="9" data-path="seq2seq-att.html"><a href="seq2seq-att.html"><i class="fa fa-check"></i><b>9</b> Sequence-to-sequence models with attention</a>
<ul>
<li class="chapter" data-level="9.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#why-attention"><i class="fa fa-check"></i><b>9.1</b> Why attention?</a></li>
<li class="chapter" data-level="9.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#preprocessing-with-torchtext"><i class="fa fa-check"></i><b>9.2</b> Preprocessing with <code>torchtext</code></a></li>
<li class="chapter" data-level="9.3" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>9.3</b> Model</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>9.3.1</b> Encoder</a></li>
<li class="chapter" data-level="9.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#attention-module"><i class="fa fa-check"></i><b>9.3.2</b> Attention module</a></li>
<li class="chapter" data-level="9.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>9.3.3</b> Decoder</a></li>
<li class="chapter" data-level="9.3.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#seq2seq-module"><i class="fa fa-check"></i><b>9.3.4</b> Seq2seq module</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#training-and-evaluation"><i class="fa fa-check"></i><b>9.4</b> Training and evaluation</a></li>
<li class="chapter" data-level="9.5" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>9.5</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="transformer.html"><a href="transformer.html"><i class="fa fa-check"></i><b>10</b> Torch transformer modules</a>
<ul>
<li class="chapter" data-level="10.1" data-path="transformer.html"><a href="transformer.html#attention-is-all-you-need"><i class="fa fa-check"></i><b>10.1</b> “Attention is all you need”</a></li>
<li class="chapter" data-level="10.2" data-path="transformer.html"><a href="transformer.html#implementation-building-blocks"><i class="fa fa-check"></i><b>10.2</b> Implementation: building blocks</a></li>
<li class="chapter" data-level="10.3" data-path="transformer.html"><a href="transformer.html#a-transformer-for-natural-language-translation"><i class="fa fa-check"></i><b>10.3</b> A Transformer for natural language translation</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="transformer.html"><a href="transformer.html#load-data"><i class="fa fa-check"></i><b>10.3.1</b> Load data</a></li>
<li class="chapter" data-level="10.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>10.3.2</b> Encoder</a></li>
<li class="chapter" data-level="10.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>10.3.3</b> Decoder</a></li>
<li class="chapter" data-level="10.3.4" data-path="transformer.html"><a href="transformer.html#overall-model"><i class="fa fa-check"></i><b>10.3.4</b> Overall model</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>10.4</b> Results</a></li>
</ul></li>
<li class="part"><span><b>IV Deep learning for tabular data</b></span></li>
<li class="chapter" data-level="" data-path="tabular-intro.html"><a href="tabular-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>V Time series</b></span></li>
<li class="chapter" data-level="" data-path="timeseries-intro.html"><a href="timeseries-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VI Generative deep learning</b></span></li>
<li class="chapter" data-level="" data-path="generative-intro.html"><a href="generative-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="11" data-path="gans.html"><a href="gans.html"><i class="fa fa-check"></i><b>11</b> Generative adversarial networks</a>
<ul>
<li class="chapter" data-level="11.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>11.1</b> Dataset</a></li>
<li class="chapter" data-level="11.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>11.2</b> Model</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="gans.html"><a href="gans.html#generator"><i class="fa fa-check"></i><b>11.2.1</b> Generator</a></li>
<li class="chapter" data-level="11.2.2" data-path="gans.html"><a href="gans.html#discriminator"><i class="fa fa-check"></i><b>11.2.2</b> Discriminator</a></li>
<li class="chapter" data-level="11.2.3" data-path="gans.html"><a href="gans.html#optimizers-and-loss-function"><i class="fa fa-check"></i><b>11.2.3</b> Optimizers and loss function</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>11.3</b> Training loop</a></li>
<li class="chapter" data-level="11.4" data-path="gans.html"><a href="gans.html#artifacts"><i class="fa fa-check"></i><b>11.4</b> Artifacts</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="vaes.html"><a href="vaes.html"><i class="fa fa-check"></i><b>12</b> Variational autoencoders</a>
<ul>
<li class="chapter" data-level="12.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>12.1</b> Dataset</a></li>
<li class="chapter" data-level="12.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>12.2</b> Model</a></li>
<li class="chapter" data-level="12.3" data-path="vaes.html"><a href="vaes.html#training-the-vae"><i class="fa fa-check"></i><b>12.3</b> Training the VAE</a></li>
<li class="chapter" data-level="12.4" data-path="vaes.html"><a href="vaes.html#latent-space"><i class="fa fa-check"></i><b>12.4</b> Latent space</a></li>
</ul></li>
<li class="part"><span><b>VII Deep learning on graphs</b></span></li>
<li class="chapter" data-level="" data-path="graph-intro.html"><a href="graph-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VIII Probabilistic deep learning</b></span></li>
<li class="chapter" data-level="" data-path="probabilistic-intro.html"><a href="probabilistic-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>IX Private and secure deep learning</b></span></li>
<li class="chapter" data-level="" data-path="private-secure-intro.html"><a href="private-secure-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>X Research topics</b></span></li>
<li class="chapter" data-level="" data-path="research-intro.html"><a href="research-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied deep learning with torch from R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple_net_autograd" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Using autograd</h1>
<p>In the previous section, we saw how to code a simple network from
scratch, using nothing but <code>torch</code> <em>tensors</em>. Predictions, loss,
gradients, weight updates – all these things we’ve been computing
ourselves. Here, we make a significant change: Namely, we spare
ourselves the cumbersome calculation of gradients, and have <code>torch</code> do
it for us.</p>
<p>Prior to that though, let’s get some background.</p>
<div id="automatic-differentiation-with-autograd" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Automatic differentiation with <em>autograd</em></h2>
<p><code>torch</code> uses a module called <em>autograd</em> to</p>
<ol style="list-style-type: decimal">
<li><p>record operations performed on tensors, and</p></li>
<li><p>store what will have to be done to obtain the corresponding
gradients, once we’re entering the backward pass.</p></li>
</ol>
<p>These prospective actions are stored internally as functions, and when
it’s time to compute the gradients, these functions are applied in
order: Application starts from the output node, and calculated gradients
are successively <em>propagated</em> <em>back</em> through the network. This is a form
of <em>reverse mode automatic differentiation</em>.</p>
<div id="autograd-basics" class="section level4" number="4.1.0.1">
<h4><span class="header-section-number">4.1.0.1</span> <em>Autograd</em> basics</h4>
<p>As users, we can see a bit of the implementation. As a prerequisite for
this “recording” to happen, tensors have to be created with
<code>requires_grad = TRUE</code>. For example:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="simple-net-autograd.html#cb104-1"></a><span class="kw">library</span>(torch)</span>
<span id="cb104-2"><a href="simple-net-autograd.html#cb104-2"></a></span>
<span id="cb104-3"><a href="simple-net-autograd.html#cb104-3"></a>x &lt;-<span class="st"> </span><span class="kw">torch_ones</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>To be clear, <code>x</code> now is a tensor <em>with respect to which</em> gradients have
to be calculated – normally, a tensor representing a weight or a bias,
not the input data.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> If we subsequently perform
some operation on that tensor, assigning the result to <code>y</code>,</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="simple-net-autograd.html#cb105-1"></a>y &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">mean</span>()</span></code></pre></div>
<p>we find that <code>y</code> now has a non-empty <code>grad_fn</code> that tells <code>torch</code> how to
compute the gradient of <code>y</code> with respect to <code>x</code>:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="simple-net-autograd.html#cb106-1"></a>y<span class="op">$</span>grad_fn</span></code></pre></div>
<pre><code>MeanBackward0</code></pre>
<p>Actual <em>computation</em> of gradients is triggered by calling <code>backward()</code>
on the output tensor.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="simple-net-autograd.html#cb108-1"></a>y<span class="op">$</span><span class="kw">backward</span>()</span></code></pre></div>
<p>After <code>backward()</code> has been called, <code>x</code> has a non-null field termed
<code>grad</code> that stores the gradient of <code>y</code> with respect to <code>x</code>:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="simple-net-autograd.html#cb109-1"></a>x<span class="op">$</span>grad</span></code></pre></div>
<pre><code>torch_tensor 
 0.2500  0.2500
 0.2500  0.2500
[ CPUFloatType{2,2} ]</code></pre>
<p>With longer chains of computations, we can take a glance at how <code>torch</code>
builds up a graph of backward operations. Here is a slightly more
complex example – feel free to skip if you’re not the type who just
<em>has</em> to peek into things for them to make sense.</p>
</div>
<div id="digging-deeper" class="section level4" number="4.1.0.2">
<h4><span class="header-section-number">4.1.0.2</span> Digging deeper</h4>
<p>We build up a simple graph of tensors, with inputs <code>x1</code> and <code>x2</code> being
connected to output <code>out</code> by intermediaries <code>y</code> and <code>z</code>.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="simple-net-autograd.html#cb111-1"></a>x1 &lt;-<span class="st"> </span><span class="kw">torch_ones</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb111-2"><a href="simple-net-autograd.html#cb111-2"></a>x2 &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="fl">1.1</span>, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb111-3"><a href="simple-net-autograd.html#cb111-3"></a></span>
<span id="cb111-4"><a href="simple-net-autograd.html#cb111-4"></a>y &lt;-<span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span>(x2 <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</span>
<span id="cb111-5"><a href="simple-net-autograd.html#cb111-5"></a></span>
<span id="cb111-6"><a href="simple-net-autograd.html#cb111-6"></a>z &lt;-<span class="st"> </span>y<span class="op">$</span><span class="kw">pow</span>(<span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="dv">3</span></span>
<span id="cb111-7"><a href="simple-net-autograd.html#cb111-7"></a></span>
<span id="cb111-8"><a href="simple-net-autograd.html#cb111-8"></a>out &lt;-<span class="st"> </span>z<span class="op">$</span><span class="kw">mean</span>()</span></code></pre></div>
<p>To save memory, intermediate gradients are normally not being stored.
Calling <code>retain_grad()</code> on a tensor allows one to deviate from this
default. Let’s do this here, for the sake of demonstration:</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="simple-net-autograd.html#cb112-1"></a>y<span class="op">$</span><span class="kw">retain_grad</span>()</span>
<span id="cb112-2"><a href="simple-net-autograd.html#cb112-2"></a></span>
<span id="cb112-3"><a href="simple-net-autograd.html#cb112-3"></a>z<span class="op">$</span><span class="kw">retain_grad</span>()</span></code></pre></div>
<p>Now we can go backwards through the graph and inspect <code>torch</code>’s action
plan for backprop, starting from <code>out$grad_fn</code>, like so:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="simple-net-autograd.html#cb113-1"></a><span class="co"># how to compute the gradient for mean, the last operation executed</span></span>
<span id="cb113-2"><a href="simple-net-autograd.html#cb113-2"></a>out<span class="op">$</span>grad_fn</span></code></pre></div>
<pre><code>MeanBackward0</code></pre>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="simple-net-autograd.html#cb115-1"></a><span class="co"># how to compute the gradient for the multiplication by 3 in z = y.pow(2) * 3</span></span>
<span id="cb115-2"><a href="simple-net-autograd.html#cb115-2"></a>out<span class="op">$</span>grad_fn<span class="op">$</span>next_functions</span></code></pre></div>
<pre><code>[[1]]
MulBackward1</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="simple-net-autograd.html#cb117-1"></a><span class="co"># how to compute the gradient for pow in z = y.pow(2) * 3</span></span>
<span id="cb117-2"><a href="simple-net-autograd.html#cb117-2"></a>out<span class="op">$</span>grad_fn<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions</span></code></pre></div>
<pre><code>[[1]]
PowBackward0</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="simple-net-autograd.html#cb119-1"></a><span class="co"># how to compute the gradient for the multiplication in y = x * (x + 2)</span></span>
<span id="cb119-2"><a href="simple-net-autograd.html#cb119-2"></a>out<span class="op">$</span>grad_fn<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions</span></code></pre></div>
<pre><code>[[1]]
MulBackward0</code></pre>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="simple-net-autograd.html#cb121-1"></a><span class="co"># how to compute the gradient for the two branches of y = x * (x + 2),</span></span>
<span id="cb121-2"><a href="simple-net-autograd.html#cb121-2"></a><span class="co"># where the left branch is a leaf node (AccumulateGrad for x1)</span></span>
<span id="cb121-3"><a href="simple-net-autograd.html#cb121-3"></a>out<span class="op">$</span>grad_fn<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions</span></code></pre></div>
<pre><code>[[1]]
torch::autograd::AccumulateGrad
[[2]]
AddBackward1</code></pre>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="simple-net-autograd.html#cb123-1"></a><span class="co"># here we arrive at the other leaf node (AccumulateGrad for x2)</span></span>
<span id="cb123-2"><a href="simple-net-autograd.html#cb123-2"></a>out<span class="op">$</span>grad_fn<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">2</span>]]<span class="op">$</span>next_functions</span></code></pre></div>
<pre><code>[[1]]
torch::autograd::AccumulateGrad</code></pre>
<p>If we now call <code>out$backward()</code>, all tensors in the graph will have
their respective gradients calculated.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="simple-net-autograd.html#cb125-1"></a>out<span class="op">$</span><span class="kw">backward</span>()</span>
<span id="cb125-2"><a href="simple-net-autograd.html#cb125-2"></a></span>
<span id="cb125-3"><a href="simple-net-autograd.html#cb125-3"></a>z<span class="op">$</span>grad</span>
<span id="cb125-4"><a href="simple-net-autograd.html#cb125-4"></a>y<span class="op">$</span>grad</span>
<span id="cb125-5"><a href="simple-net-autograd.html#cb125-5"></a>x2<span class="op">$</span>grad</span>
<span id="cb125-6"><a href="simple-net-autograd.html#cb125-6"></a>x1<span class="op">$</span>grad</span></code></pre></div>
<pre><code>torch_tensor 
 0.2500  0.2500
 0.2500  0.2500
[ CPUFloatType{2,2} ]
torch_tensor 
 4.6500  4.6500
 4.6500  4.6500
[ CPUFloatType{2,2} ]
torch_tensor 
 18.6000
[ CPUFloatType{1} ]
torch_tensor 
 14.4150  14.4150
 14.4150  14.4150
[ CPUFloatType{2,2} ]</code></pre>
<p>After this nerdy excursion, let’s see how <em>autograd</em> makes our network
simpler.</p>
</div>
</div>
<div id="the-simple-network-now-using-autograd" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> The simple network, now using <em>autograd</em></h2>
<p>Thanks to <em>autograd</em>, we say good-bye to the tedious, error-prone
process of coding backpropagation ourselves. A single method call does
it all: <code>loss$backward()</code>.</p>
<p>With <code>torch</code> keeping track of operations as required, we don’t even have
to explicitly name the intermediate tensors any more. We can code
forward pass, loss calculation, and backward pass in just three lines:</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="simple-net-autograd.html#cb127-1"></a>y_pred &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">mm</span>(w1)<span class="op">$</span><span class="kw">add</span>(b1)<span class="op">$</span><span class="kw">clamp</span>(<span class="dt">min =</span> <span class="dv">0</span>)<span class="op">$</span><span class="kw">mm</span>(w2)<span class="op">$</span><span class="kw">add</span>(b2)</span>
<span id="cb127-2"><a href="simple-net-autograd.html#cb127-2"></a>  </span>
<span id="cb127-3"><a href="simple-net-autograd.html#cb127-3"></a>loss &lt;-<span class="st"> </span>(y_pred <span class="op">-</span><span class="st"> </span>y)<span class="op">$</span><span class="kw">pow</span>(<span class="dv">2</span>)<span class="op">$</span><span class="kw">sum</span>()</span>
<span id="cb127-4"><a href="simple-net-autograd.html#cb127-4"></a></span>
<span id="cb127-5"><a href="simple-net-autograd.html#cb127-5"></a>loss<span class="op">$</span><span class="kw">backward</span>()</span></code></pre></div>
<p>Here is the complete code. We’re at an intermediate stage: We still
manually compute the forward pass and the loss, and we still manually
update the weights. Due to the latter, there is something I need to
explain. But I’ll let you check out the new version first:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="simple-net-autograd.html#cb128-1"></a><span class="kw">library</span>(torch)</span>
<span id="cb128-2"><a href="simple-net-autograd.html#cb128-2"></a></span>
<span id="cb128-3"><a href="simple-net-autograd.html#cb128-3"></a><span class="co">### generate training data -----------------------------------------------------</span></span>
<span id="cb128-4"><a href="simple-net-autograd.html#cb128-4"></a></span>
<span id="cb128-5"><a href="simple-net-autograd.html#cb128-5"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb128-6"><a href="simple-net-autograd.html#cb128-6"></a>d_in &lt;-<span class="st"> </span><span class="dv">3</span></span>
<span id="cb128-7"><a href="simple-net-autograd.html#cb128-7"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb128-8"><a href="simple-net-autograd.html#cb128-8"></a>d_out &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb128-9"><a href="simple-net-autograd.html#cb128-9"></a><span class="co"># number of observations in training set</span></span>
<span id="cb128-10"><a href="simple-net-autograd.html#cb128-10"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb128-11"><a href="simple-net-autograd.html#cb128-11"></a></span>
<span id="cb128-12"><a href="simple-net-autograd.html#cb128-12"></a></span>
<span id="cb128-13"><a href="simple-net-autograd.html#cb128-13"></a><span class="co"># create random data</span></span>
<span id="cb128-14"><a href="simple-net-autograd.html#cb128-14"></a>x &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(n, d_in)</span>
<span id="cb128-15"><a href="simple-net-autograd.html#cb128-15"></a>y &lt;-<span class="st"> </span>x[, <span class="dv">1</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">0.2</span> <span class="op">-</span><span class="st"> </span>x[, <span class="dv">2</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">1.3</span> <span class="op">-</span><span class="st"> </span>x[, <span class="dv">3</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">+</span><span class="st"> </span><span class="kw">torch_randn</span>(n, <span class="dv">1</span>)</span>
<span id="cb128-16"><a href="simple-net-autograd.html#cb128-16"></a></span>
<span id="cb128-17"><a href="simple-net-autograd.html#cb128-17"></a></span>
<span id="cb128-18"><a href="simple-net-autograd.html#cb128-18"></a><span class="co">### initialize weights ---------------------------------------------------------</span></span>
<span id="cb128-19"><a href="simple-net-autograd.html#cb128-19"></a></span>
<span id="cb128-20"><a href="simple-net-autograd.html#cb128-20"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb128-21"><a href="simple-net-autograd.html#cb128-21"></a>d_hidden &lt;-<span class="st"> </span><span class="dv">32</span></span>
<span id="cb128-22"><a href="simple-net-autograd.html#cb128-22"></a><span class="co"># weights connecting input to hidden layer</span></span>
<span id="cb128-23"><a href="simple-net-autograd.html#cb128-23"></a>w1 &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(d_in, d_hidden, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb128-24"><a href="simple-net-autograd.html#cb128-24"></a><span class="co"># weights connecting hidden to output layer</span></span>
<span id="cb128-25"><a href="simple-net-autograd.html#cb128-25"></a>w2 &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(d_hidden, d_out, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb128-26"><a href="simple-net-autograd.html#cb128-26"></a></span>
<span id="cb128-27"><a href="simple-net-autograd.html#cb128-27"></a><span class="co"># hidden layer bias</span></span>
<span id="cb128-28"><a href="simple-net-autograd.html#cb128-28"></a>b1 &lt;-<span class="st"> </span><span class="kw">torch_zeros</span>(<span class="dv">1</span>, d_hidden, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb128-29"><a href="simple-net-autograd.html#cb128-29"></a><span class="co"># output layer bias</span></span>
<span id="cb128-30"><a href="simple-net-autograd.html#cb128-30"></a>b2 &lt;-<span class="st"> </span><span class="kw">torch_zeros</span>(<span class="dv">1</span>, d_out, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb128-31"><a href="simple-net-autograd.html#cb128-31"></a></span>
<span id="cb128-32"><a href="simple-net-autograd.html#cb128-32"></a><span class="co">### network parameters ---------------------------------------------------------</span></span>
<span id="cb128-33"><a href="simple-net-autograd.html#cb128-33"></a></span>
<span id="cb128-34"><a href="simple-net-autograd.html#cb128-34"></a>learning_rate &lt;-<span class="st"> </span><span class="fl">1e-4</span></span>
<span id="cb128-35"><a href="simple-net-autograd.html#cb128-35"></a></span>
<span id="cb128-36"><a href="simple-net-autograd.html#cb128-36"></a><span class="co">### training loop --------------------------------------------------------------</span></span>
<span id="cb128-37"><a href="simple-net-autograd.html#cb128-37"></a></span>
<span id="cb128-38"><a href="simple-net-autograd.html#cb128-38"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">200</span>) {</span>
<span id="cb128-39"><a href="simple-net-autograd.html#cb128-39"></a>  <span class="co">### -------- Forward pass --------</span></span>
<span id="cb128-40"><a href="simple-net-autograd.html#cb128-40"></a>  </span>
<span id="cb128-41"><a href="simple-net-autograd.html#cb128-41"></a>  y_pred &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">mm</span>(w1)<span class="op">$</span><span class="kw">add</span>(b1)<span class="op">$</span><span class="kw">clamp</span>(<span class="dt">min =</span> <span class="dv">0</span>)<span class="op">$</span><span class="kw">mm</span>(w2)<span class="op">$</span><span class="kw">add</span>(b2)</span>
<span id="cb128-42"><a href="simple-net-autograd.html#cb128-42"></a>  </span>
<span id="cb128-43"><a href="simple-net-autograd.html#cb128-43"></a>  <span class="co">### -------- compute loss -------- </span></span>
<span id="cb128-44"><a href="simple-net-autograd.html#cb128-44"></a>  loss &lt;-<span class="st"> </span>(y_pred <span class="op">-</span><span class="st"> </span>y)<span class="op">$</span><span class="kw">pow</span>(<span class="dv">2</span>)<span class="op">$</span><span class="kw">sum</span>()</span>
<span id="cb128-45"><a href="simple-net-autograd.html#cb128-45"></a>  <span class="cf">if</span> (t <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)</span>
<span id="cb128-46"><a href="simple-net-autograd.html#cb128-46"></a>    <span class="kw">cat</span>(<span class="st">&quot;Epoch: &quot;</span>, t, <span class="st">&quot;   Loss: &quot;</span>, loss<span class="op">$</span><span class="kw">item</span>(), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb128-47"><a href="simple-net-autograd.html#cb128-47"></a>  </span>
<span id="cb128-48"><a href="simple-net-autograd.html#cb128-48"></a>  <span class="co">### -------- Backpropagation --------</span></span>
<span id="cb128-49"><a href="simple-net-autograd.html#cb128-49"></a>  </span>
<span id="cb128-50"><a href="simple-net-autograd.html#cb128-50"></a>  <span class="co"># compute gradient of loss w.r.t. all tensors with requires_grad = TRUE</span></span>
<span id="cb128-51"><a href="simple-net-autograd.html#cb128-51"></a>  loss<span class="op">$</span><span class="kw">backward</span>()</span>
<span id="cb128-52"><a href="simple-net-autograd.html#cb128-52"></a>  </span>
<span id="cb128-53"><a href="simple-net-autograd.html#cb128-53"></a>  <span class="co">### -------- Update weights -------- </span></span>
<span id="cb128-54"><a href="simple-net-autograd.html#cb128-54"></a>  </span>
<span id="cb128-55"><a href="simple-net-autograd.html#cb128-55"></a>  <span class="co"># Wrap in with_no_grad() because this is a part we DON&#39;T </span></span>
<span id="cb128-56"><a href="simple-net-autograd.html#cb128-56"></a>  <span class="co"># want to record for automatic gradient computation</span></span>
<span id="cb128-57"><a href="simple-net-autograd.html#cb128-57"></a>   <span class="kw">with_no_grad</span>({</span>
<span id="cb128-58"><a href="simple-net-autograd.html#cb128-58"></a>     w1 &lt;-<span class="st"> </span>w1<span class="op">$</span><span class="kw">sub_</span>(learning_rate <span class="op">*</span><span class="st"> </span>w1<span class="op">$</span>grad)</span>
<span id="cb128-59"><a href="simple-net-autograd.html#cb128-59"></a>     w2 &lt;-<span class="st"> </span>w2<span class="op">$</span><span class="kw">sub_</span>(learning_rate <span class="op">*</span><span class="st"> </span>w2<span class="op">$</span>grad)</span>
<span id="cb128-60"><a href="simple-net-autograd.html#cb128-60"></a>     b1 &lt;-<span class="st"> </span>b1<span class="op">$</span><span class="kw">sub_</span>(learning_rate <span class="op">*</span><span class="st"> </span>b1<span class="op">$</span>grad)</span>
<span id="cb128-61"><a href="simple-net-autograd.html#cb128-61"></a>     b2 &lt;-<span class="st"> </span>b2<span class="op">$</span><span class="kw">sub_</span>(learning_rate <span class="op">*</span><span class="st"> </span>b2<span class="op">$</span>grad)  </span>
<span id="cb128-62"><a href="simple-net-autograd.html#cb128-62"></a>     </span>
<span id="cb128-63"><a href="simple-net-autograd.html#cb128-63"></a>     <span class="co"># Zero gradients after every pass, as they&#39;d accumulate otherwise</span></span>
<span id="cb128-64"><a href="simple-net-autograd.html#cb128-64"></a>     w1<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</span>
<span id="cb128-65"><a href="simple-net-autograd.html#cb128-65"></a>     w2<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</span>
<span id="cb128-66"><a href="simple-net-autograd.html#cb128-66"></a>     b1<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</span>
<span id="cb128-67"><a href="simple-net-autograd.html#cb128-67"></a>     b2<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()  </span>
<span id="cb128-68"><a href="simple-net-autograd.html#cb128-68"></a>   })</span>
<span id="cb128-69"><a href="simple-net-autograd.html#cb128-69"></a></span>
<span id="cb128-70"><a href="simple-net-autograd.html#cb128-70"></a>}</span></code></pre></div>
<p>As explained above, after <code>some_tensor$backward()</code>, all tensors
preceding it in the graph<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> will have their <code>grad</code>
fields populated. We make use of these fields to update the weights. But
now that <em>autograd</em> is “on”, whenever we execute an operation we <em>don’t</em>
want recorded for backprop, we need to explicitly exempt it: This is why
we wrap the weight updates in a call to <code>with_no_grad()</code>.</p>
<p>While this is something you may file under “nice to know” – after all,
once we arrive at the final section in this chapter, this manual
updating of weights will be gone – the idiom of <em>zeroing gradients</em> is
here to stay: Values stored in <code>grad</code> fields accumulate; whenever we’re
done using them, we need to zero them out before reuse.</p>
</div>
<div id="outlook" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Outlook</h2>
<p>So where do we stand? We started out coding a network completely from
scratch, making use of nothing but <code>torch</code> tensors. In this section, we
got significant help from <em>autograd</em>.</p>
<p>But we’re still manually updating the weights, – and aren’t deep
learning frameworks known to provide abstractions (“layers”, or:
“modules”) on top of tensor computations …? Stay tuned.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>Unless we <em>want</em> to change the data, as when
generating adversarial examples.<a href="simple-net-autograd.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>All that have <code>requires_grad</code> set to <code>TRUE</code>,
to be precise.<a href="simple-net-autograd.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-net-tensors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple-net-modules.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
