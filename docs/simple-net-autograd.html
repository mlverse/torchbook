<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Using autograd | Applied deep learning with torch from R</title>
  <meta name="description" content="tbd" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Using autograd | Applied deep learning with torch from R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="tbd" />
  <meta property="og:image" content="tbdcover.png" />
  <meta property="og:description" content="tbd" />
  <meta name="github-repo" content="tbd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Using autograd | Applied deep learning with torch from R" />
  
  <meta name="twitter:description" content="tbd" />
  <meta name="twitter:image" content="tbdcover.png" />

<meta name="author" content="tbd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-net-tensors.html"/>
<link rel="next" href="simple-net-modules.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
</ul></li>
<li class="part"><span><b>I Using Torch</b></span></li>
<li class="chapter" data-level="" data-path="using-torch-intro.html"><a href="using-torch-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="2" data-path="simple-net-R.html"><a href="simple-net-R.html"><i class="fa fa-check"></i><b>2</b> A simple neural network in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#whats-in-a-network"><i class="fa fa-check"></i><b>2.1</b> What’s in a network?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="simple-net-R.html"><a href="simple-net-R.html#gradient-descent"><i class="fa fa-check"></i><b>2.1.1</b> Gradient descent</a></li>
<li class="chapter" data-level="2.1.2" data-path="simple-net-R.html"><a href="simple-net-R.html#from-linear-regression-to-a-simple-network"><i class="fa fa-check"></i><b>2.1.2</b> From linear regression to a simple network</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#a-simple-network"><i class="fa fa-check"></i><b>2.2</b> A simple network</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#simulate-data"><i class="fa fa-check"></i><b>2.2.1</b> Simulate data</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#initialize-weights-and-biases"><i class="fa fa-check"></i><b>2.2.2</b> Initialize weights and biases</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>2.2.3</b> Training loop</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple-net-R.html"><a href="simple-net-R.html#complete-code"><i class="fa fa-check"></i><b>2.2.4</b> Complete code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>2.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html"><i class="fa fa-check"></i><b>3</b> Modifying the simple network to use torch tensors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#simple-network-torchified-step-1"><i class="fa fa-check"></i><b>3.1</b> Simple network torchified, step 1</a></li>
<li class="chapter" data-level="3.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#more-on-tensors"><i class="fa fa-check"></i><b>3.2</b> More on tensors</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#creating-tensors"><i class="fa fa-check"></i><b>3.2.1</b> Creating tensors</a></li>
<li class="chapter" data-level="3.2.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#conversion-between-torch-tensors-and-r-values"><i class="fa fa-check"></i><b>3.2.2</b> Conversion between <code>torch</code> tensors and R values</a></li>
<li class="chapter" data-level="3.2.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#indexing-and-slicing-tensors"><i class="fa fa-check"></i><b>3.2.3</b> Indexing and slicing tensors</a></li>
<li class="chapter" data-level="3.2.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#reshaping-tensors"><i class="fa fa-check"></i><b>3.2.4</b> Reshaping tensors</a></li>
<li class="chapter" data-level="3.2.5" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#operations-on-tensors"><i class="fa fa-check"></i><b>3.2.5</b> Operations on tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#broadcasting"><i class="fa fa-check"></i><b>3.3</b> Broadcasting</a></li>
<li class="chapter" data-level="3.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#running-on-gpu"><i class="fa fa-check"></i><b>3.4</b> Running on GPU</a></li>
<li class="chapter" data-level="3.5" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#appendix-python-code-1"><i class="fa fa-check"></i><b>3.5</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html"><i class="fa fa-check"></i><b>4</b> Using autograd</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#automatic-differentiation-with-autograd"><i class="fa fa-check"></i><b>4.1</b> Automatic differentiation with autograd</a></li>
<li class="chapter" data-level="4.2" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#the-simple-network-now-using-autograd"><i class="fa fa-check"></i><b>4.2</b> The simple network, now using autograd</a></li>
<li class="chapter" data-level="4.3" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#appendix-python-code-2"><i class="fa fa-check"></i><b>4.3</b> Appendix: Python code</a></li>
<li class="chapter" data-level="4.4" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#section"><i class="fa fa-check"></i><b>4.4</b> </a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple-net-modules.html"><a href="simple-net-modules.html"><i class="fa fa-check"></i><b>5</b> Using torch modules</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#modules"><i class="fa fa-check"></i><b>5.1</b> Modules</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#layers-as-modules"><i class="fa fa-check"></i><b>5.1.1</b> Layers as modules</a></li>
<li class="chapter" data-level="5.1.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#models-as-modules"><i class="fa fa-check"></i><b>5.1.2</b> Models as modules</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#simple-network-using-modules"><i class="fa fa-check"></i><b>5.2</b> Simple network using modules</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="simple-net-optim.html"><a href="simple-net-optim.html"><i class="fa fa-check"></i><b>6</b> Using torch optimizers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="simple-net-optim.html"><a href="simple-net-optim.html#torch-optimizers"><i class="fa fa-check"></i><b>6.1</b> Torch optimizers</a></li>
<li class="chapter" data-level="6.2" data-path="simple-net-optim.html"><a href="simple-net-optim.html#simple-net-with-torch.optim"><i class="fa fa-check"></i><b>6.2</b> Simple net with <code>torch.optim</code></a></li>
</ul></li>
<li class="part"><span><b>II Image Recognition</b></span></li>
<li class="chapter" data-level="" data-path="image-recognition-intro.html"><a href="image-recognition-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="7" data-path="image-classification.html"><a href="image-classification.html"><i class="fa fa-check"></i><b>7</b> Classifying images</a>
<ul>
<li class="chapter" data-level="7.1" data-path="image-classification.html"><a href="image-classification.html#data-loading-and-transformation"><i class="fa fa-check"></i><b>7.1</b> Data loading and transformation</a></li>
<li class="chapter" data-level="7.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>7.2</b> Model</a></li>
<li class="chapter" data-level="7.3" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>7.3</b> Training</a></li>
<li class="chapter" data-level="7.4" data-path="image-classification.html"><a href="image-classification.html#performance-on-the-test-set"><i class="fa fa-check"></i><b>7.4</b> Performance on the test set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="unet.html"><a href="unet.html"><i class="fa fa-check"></i><b>8</b> Brain Image Segmentation with U-Net</a>
<ul>
<li class="chapter" data-level="8.1" data-path="unet.html"><a href="unet.html#image-segmentation-in-a-nutshell"><i class="fa fa-check"></i><b>8.1</b> Image segmentation in a nutshell</a></li>
<li class="chapter" data-level="8.2" data-path="unet.html"><a href="unet.html#u-net"><i class="fa fa-check"></i><b>8.2</b> U-Net</a></li>
<li class="chapter" data-level="8.3" data-path="unet.html"><a href="unet.html#example-application-mri-images"><i class="fa fa-check"></i><b>8.3</b> Example application: MRI images</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="unet.html"><a href="unet.html#preprocessing"><i class="fa fa-check"></i><b>8.3.1</b> Preprocessing</a></li>
<li class="chapter" data-level="8.3.2" data-path="unet.html"><a href="unet.html#u-net-model"><i class="fa fa-check"></i><b>8.3.2</b> U-Net model</a></li>
<li class="chapter" data-level="8.3.3" data-path="unet.html"><a href="unet.html#loss"><i class="fa fa-check"></i><b>8.3.3</b> Loss</a></li>
<li class="chapter" data-level="8.3.4" data-path="unet.html"><a href="unet.html#training-1"><i class="fa fa-check"></i><b>8.3.4</b> Training</a></li>
<li class="chapter" data-level="8.3.5" data-path="unet.html"><a href="unet.html#predictions"><i class="fa fa-check"></i><b>8.3.5</b> Predictions</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Natural language processing</b></span></li>
<li class="chapter" data-level="" data-path="NLP-intro.html"><a href="NLP-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="9" data-path="seq2seq-att.html"><a href="seq2seq-att.html"><i class="fa fa-check"></i><b>9</b> Sequence-to-sequence models with attention</a>
<ul>
<li class="chapter" data-level="9.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#why-attention"><i class="fa fa-check"></i><b>9.1</b> Why attention?</a></li>
<li class="chapter" data-level="9.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#preprocessing-with-torchtext"><i class="fa fa-check"></i><b>9.2</b> Preprocessing with <code>torchtext</code></a></li>
<li class="chapter" data-level="9.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#model-1"><i class="fa fa-check"></i><b>9.3</b> Model</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>9.3.1</b> Encoder</a></li>
<li class="chapter" data-level="9.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#attention-module"><i class="fa fa-check"></i><b>9.3.2</b> Attention module</a></li>
<li class="chapter" data-level="9.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>9.3.3</b> Decoder</a></li>
<li class="chapter" data-level="9.3.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#seq2seq-module"><i class="fa fa-check"></i><b>9.3.4</b> Seq2seq module</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#training-and-evaluation"><i class="fa fa-check"></i><b>9.4</b> Training and evaluation</a></li>
<li class="chapter" data-level="9.5" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>9.5</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="transformer.html"><a href="transformer.html"><i class="fa fa-check"></i><b>10</b> Torch transformer modules</a>
<ul>
<li class="chapter" data-level="10.1" data-path="transformer.html"><a href="transformer.html#attention-is-all-you-need"><i class="fa fa-check"></i><b>10.1</b> “Attention is all you need”</a></li>
<li class="chapter" data-level="10.2" data-path="transformer.html"><a href="transformer.html#implementation-building-blocks"><i class="fa fa-check"></i><b>10.2</b> Implementation: building blocks</a></li>
<li class="chapter" data-level="10.3" data-path="transformer.html"><a href="transformer.html#a-transformer-for-natural-language-translation"><i class="fa fa-check"></i><b>10.3</b> A Transformer for natural language translation</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="transformer.html"><a href="transformer.html#load-data"><i class="fa fa-check"></i><b>10.3.1</b> Load data</a></li>
<li class="chapter" data-level="10.3.2" data-path="transformer.html"><a href="transformer.html#encoder-1"><i class="fa fa-check"></i><b>10.3.2</b> Encoder</a></li>
<li class="chapter" data-level="10.3.3" data-path="transformer.html"><a href="transformer.html#decoder-1"><i class="fa fa-check"></i><b>10.3.3</b> Decoder</a></li>
<li class="chapter" data-level="10.3.4" data-path="transformer.html"><a href="transformer.html#overall-model"><i class="fa fa-check"></i><b>10.3.4</b> Overall model</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="transformer.html"><a href="transformer.html#results-1"><i class="fa fa-check"></i><b>10.4</b> Results</a></li>
</ul></li>
<li class="part"><span><b>IV Deep learning for tabular data</b></span></li>
<li class="chapter" data-level="" data-path="tabular-intro.html"><a href="tabular-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>V Time series</b></span></li>
<li class="chapter" data-level="" data-path="timeseries-intro.html"><a href="timeseries-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VI Generative deep learning</b></span></li>
<li class="chapter" data-level="" data-path="generative-intro.html"><a href="generative-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="11" data-path="gans.html"><a href="gans.html"><i class="fa fa-check"></i><b>11</b> Generative adversarial networks</a>
<ul>
<li class="chapter" data-level="11.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>11.1</b> Dataset</a></li>
<li class="chapter" data-level="11.2" data-path="gans.html"><a href="gans.html#model-2"><i class="fa fa-check"></i><b>11.2</b> Model</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="gans.html"><a href="gans.html#generator"><i class="fa fa-check"></i><b>11.2.1</b> Generator</a></li>
<li class="chapter" data-level="11.2.2" data-path="gans.html"><a href="gans.html#discriminator"><i class="fa fa-check"></i><b>11.2.2</b> Discriminator</a></li>
<li class="chapter" data-level="11.2.3" data-path="gans.html"><a href="gans.html#optimizers-and-loss-function"><i class="fa fa-check"></i><b>11.2.3</b> Optimizers and loss function</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="gans.html"><a href="gans.html#training-loop-1"><i class="fa fa-check"></i><b>11.3</b> Training loop</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="vaes.html"><a href="vaes.html"><i class="fa fa-check"></i><b>12</b> Variational autoencoders</a>
<ul>
<li class="chapter" data-level="12.1" data-path="vaes.html"><a href="vaes.html#dataset-1"><i class="fa fa-check"></i><b>12.1</b> Dataset</a></li>
<li class="chapter" data-level="12.2" data-path="vaes.html"><a href="vaes.html#model-3"><i class="fa fa-check"></i><b>12.2</b> Model</a></li>
<li class="chapter" data-level="12.3" data-path="vaes.html"><a href="vaes.html#training-the-vae"><i class="fa fa-check"></i><b>12.3</b> Training the VAE</a></li>
<li class="chapter" data-level="12.4" data-path="vaes.html"><a href="vaes.html#latent-space"><i class="fa fa-check"></i><b>12.4</b> Latent space</a></li>
</ul></li>
<li class="part"><span><b>VII Deep learning on graphs</b></span></li>
<li class="chapter" data-level="" data-path="graph-intro.html"><a href="graph-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VIII Probabilistic deep learning</b></span></li>
<li class="chapter" data-level="" data-path="probabilistic-intro.html"><a href="probabilistic-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>IX Private and secure deep learning</b></span></li>
<li class="chapter" data-level="" data-path="private-secure-intro.html"><a href="private-secure-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>X Research topics</b></span></li>
<li class="chapter" data-level="" data-path="research-intro.html"><a href="research-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied deep learning with torch from R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple_net_autograd" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Using autograd</h1>
<p>So far, all we’ve been using from <code>torch</code> is <em>tensors</em>. Predictions, loss, gradients, desired weight updates, new weights –
all these things we’ve been computing ourselves. In this chapter, we’ll make a significant change: Namely, we spare ourselves
the cumbersome calculation of gradients, and have <code>torch</code> do it for us.</p>
<p>Before we see that in action, let’s get some more background.</p>
<div id="automatic-differentiation-with-autograd" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Automatic differentiation with autograd</h2>
<p>Torch uses a module called <em>autograd</em> to record operations performed on tensors, and store what has to be done to obtain the
respective gradients. These actions are stored as functions, and those functions are applied in order when the gradient of the
output (the loss, usually) with respect to those tensors is calculated: Function application starts from the output node, and
the gradients calculated are <em>propagated</em> <em>back</em> through the network. This is a form of <em>reverse mode automatic
differentiation</em>.</p>
<p>As users, we can see a bit of this implementation. As a prerequisite for this “recording” to happen, tensors have to be
created with <code>requires_grad = TRUE</code>. E.g.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="simple-net-autograd.html#cb64-1"></a>x &lt;-<span class="st"> </span><span class="kw">torch_ones</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>To be clear, this is a tensor <em>with respect to which</em> gradients have to be calculated – normally, a tensor representing a
weight or a bias, not the input data.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> If we now perform some operation on that tensor, assigning the result to <code>y</code></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="simple-net-autograd.html#cb65-1"></a>y &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">mean</span>()</span></code></pre></div>
<p>we find that <code>y</code> now has a non-empty <code>grad_fn</code> that tells <code>torch</code> how to compute the gradient of <code>y</code> with respect to <code>x</code>:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="simple-net-autograd.html#cb66-1"></a>y<span class="op">$</span>grad_fn</span></code></pre></div>
<p>Actual computation of gradients is triggered by calling <code>backward()</code> on the output tensor.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="simple-net-autograd.html#cb67-1"></a>y<span class="op">$</span><span class="kw">backward</span>()</span></code></pre></div>
<p>After <code>backward()</code> has been called, <code>x</code> has a non-empty field <code>grad</code> that stores the gradient of <code>y</code> with respect to <code>x</code>:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="simple-net-autograd.html#cb68-1"></a>x<span class="op">$</span>grad</span></code></pre></div>
<p>With longer chains of computations, we can peek at how <code>torch</code> builds up a graph of backward operations.</p>
<p>Here is a slightly more complex example. We call <code>retain_grad()</code> on <code>y</code> and <code>z</code> just for demonstration purposes; by default,
intermediate gradients – while of course they have to be computed – aren’t stored, in order to save memory.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="simple-net-autograd.html#cb69-1"></a>x1 &lt;-<span class="st"> </span><span class="kw">torch_ones</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb69-2"><a href="simple-net-autograd.html#cb69-2"></a>x2 &lt;-<span class="st"> </span><span class="kw">torch_tensor</span>(<span class="fl">1.1</span>, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb69-3"><a href="simple-net-autograd.html#cb69-3"></a></span>
<span id="cb69-4"><a href="simple-net-autograd.html#cb69-4"></a>y &lt;-<span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span>(x2 <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</span>
<span id="cb69-5"><a href="simple-net-autograd.html#cb69-5"></a>y<span class="op">$</span><span class="kw">retain_grad</span>()</span>
<span id="cb69-6"><a href="simple-net-autograd.html#cb69-6"></a></span>
<span id="cb69-7"><a href="simple-net-autograd.html#cb69-7"></a>z &lt;-<span class="st"> </span>y<span class="op">$</span><span class="kw">pow</span>(<span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="dv">3</span></span>
<span id="cb69-8"><a href="simple-net-autograd.html#cb69-8"></a>z<span class="op">$</span><span class="kw">retain_grad</span>()</span>
<span id="cb69-9"><a href="simple-net-autograd.html#cb69-9"></a></span>
<span id="cb69-10"><a href="simple-net-autograd.html#cb69-10"></a>out &lt;-<span class="st"> </span>z<span class="op">$</span><span class="kw">mean</span>()</span></code></pre></div>
<p>Starting from <code>out$grad_fn</code>, we can follow the graph all back to the leaf nodes:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="simple-net-autograd.html#cb70-1"></a><span class="co"># how to compute the gradient for mean, the last operation executed</span></span>
<span id="cb70-2"><a href="simple-net-autograd.html#cb70-2"></a>out<span class="op">$</span>grad_fn</span>
<span id="cb70-3"><a href="simple-net-autograd.html#cb70-3"></a></span>
<span id="cb70-4"><a href="simple-net-autograd.html#cb70-4"></a><span class="co"># how to compute the gradient for the multiplication by 3 in z = y.pow(2) * 3</span></span>
<span id="cb70-5"><a href="simple-net-autograd.html#cb70-5"></a>out<span class="op">$</span>grad_fn<span class="op">$</span>next_functions</span>
<span id="cb70-6"><a href="simple-net-autograd.html#cb70-6"></a></span>
<span id="cb70-7"><a href="simple-net-autograd.html#cb70-7"></a><span class="co"># how to compute the gradient for pow in z = y.pow(2) * 3</span></span>
<span id="cb70-8"><a href="simple-net-autograd.html#cb70-8"></a>out<span class="op">$</span>grad_fn<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions</span>
<span id="cb70-9"><a href="simple-net-autograd.html#cb70-9"></a></span>
<span id="cb70-10"><a href="simple-net-autograd.html#cb70-10"></a><span class="co"># how to compute the gradient for the multiplication in y = x * (x + 2)</span></span>
<span id="cb70-11"><a href="simple-net-autograd.html#cb70-11"></a>out<span class="op">$</span>grad_fn<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions</span>
<span id="cb70-12"><a href="simple-net-autograd.html#cb70-12"></a></span>
<span id="cb70-13"><a href="simple-net-autograd.html#cb70-13"></a><span class="co"># how to compute the gradient for the two branches of y = x * (x + 2),</span></span>
<span id="cb70-14"><a href="simple-net-autograd.html#cb70-14"></a><span class="co"># where the left branch is a leaf node (AccumulateGrad for x1)</span></span>
<span id="cb70-15"><a href="simple-net-autograd.html#cb70-15"></a>out<span class="op">$</span>grad_fn<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions</span>
<span id="cb70-16"><a href="simple-net-autograd.html#cb70-16"></a></span>
<span id="cb70-17"><a href="simple-net-autograd.html#cb70-17"></a><span class="co"># here we arrive at the other leaf node (AccumulateGrad for x2)</span></span>
<span id="cb70-18"><a href="simple-net-autograd.html#cb70-18"></a>out<span class="op">$</span>grad_fn<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">1</span>]]<span class="op">$</span>next_functions[[<span class="dv">2</span>]]<span class="op">$</span>next_functions</span></code></pre></div>
<p>After calling <code>out$backward()</code>, all tensors in the graph will have their respective gradients created. Without our calls to
<code>retain_grad</code> above, <code>z$grad</code> and <code>y$grad</code> would be empty:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="simple-net-autograd.html#cb71-1"></a>out<span class="op">$</span><span class="kw">backward</span>()</span>
<span id="cb71-2"><a href="simple-net-autograd.html#cb71-2"></a></span>
<span id="cb71-3"><a href="simple-net-autograd.html#cb71-3"></a>z<span class="op">$</span>grad</span>
<span id="cb71-4"><a href="simple-net-autograd.html#cb71-4"></a>y<span class="op">$</span>grad</span>
<span id="cb71-5"><a href="simple-net-autograd.html#cb71-5"></a>x2<span class="op">$</span>grad</span>
<span id="cb71-6"><a href="simple-net-autograd.html#cb71-6"></a>x1<span class="op">$</span>grad</span></code></pre></div>
<p>Thus acquainted with autograd, we’re ready to modify our example.</p>
</div>
<div id="the-simple-network-now-using-autograd" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> The simple network, now using autograd</h2>
<p>For a single new line calling <code>loss$backward()</code>, now a number of lines (that did manual backprop) are gone:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="simple-net-autograd.html#cb72-1"></a><span class="kw">library</span>(torch)</span>
<span id="cb72-2"><a href="simple-net-autograd.html#cb72-2"></a></span>
<span id="cb72-3"><a href="simple-net-autograd.html#cb72-3"></a><span class="co">### generate training data -----------------------------------------------------</span></span>
<span id="cb72-4"><a href="simple-net-autograd.html#cb72-4"></a></span>
<span id="cb72-5"><a href="simple-net-autograd.html#cb72-5"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb72-6"><a href="simple-net-autograd.html#cb72-6"></a>d_in &lt;-<span class="st"> </span><span class="dv">3</span></span>
<span id="cb72-7"><a href="simple-net-autograd.html#cb72-7"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb72-8"><a href="simple-net-autograd.html#cb72-8"></a>d_out &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb72-9"><a href="simple-net-autograd.html#cb72-9"></a><span class="co"># number of observations in training set</span></span>
<span id="cb72-10"><a href="simple-net-autograd.html#cb72-10"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb72-11"><a href="simple-net-autograd.html#cb72-11"></a></span>
<span id="cb72-12"><a href="simple-net-autograd.html#cb72-12"></a></span>
<span id="cb72-13"><a href="simple-net-autograd.html#cb72-13"></a><span class="co"># create random data</span></span>
<span id="cb72-14"><a href="simple-net-autograd.html#cb72-14"></a>x &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(n, d_in)</span>
<span id="cb72-15"><a href="simple-net-autograd.html#cb72-15"></a>y &lt;-</span>
<span id="cb72-16"><a href="simple-net-autograd.html#cb72-16"></a><span class="st">  </span>x[, <span class="dv">0</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">0.2</span> <span class="op">-</span><span class="st"> </span>x[, <span class="dv">1</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">1.3</span> <span class="op">-</span><span class="st"> </span>x[, <span class="dv">2</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">+</span><span class="st"> </span><span class="kw">torch_randn</span>(n, <span class="dv">1</span>)</span>
<span id="cb72-17"><a href="simple-net-autograd.html#cb72-17"></a></span>
<span id="cb72-18"><a href="simple-net-autograd.html#cb72-18"></a></span>
<span id="cb72-19"><a href="simple-net-autograd.html#cb72-19"></a><span class="co">### initialize weights ---------------------------------------------------------</span></span>
<span id="cb72-20"><a href="simple-net-autograd.html#cb72-20"></a></span>
<span id="cb72-21"><a href="simple-net-autograd.html#cb72-21"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb72-22"><a href="simple-net-autograd.html#cb72-22"></a>d_hidden &lt;-<span class="st"> </span><span class="dv">32</span></span>
<span id="cb72-23"><a href="simple-net-autograd.html#cb72-23"></a><span class="co"># weights connecting input to hidden layer</span></span>
<span id="cb72-24"><a href="simple-net-autograd.html#cb72-24"></a>w1 &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(d_in, d_hidden, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb72-25"><a href="simple-net-autograd.html#cb72-25"></a><span class="co"># weights connecting hidden to output layer</span></span>
<span id="cb72-26"><a href="simple-net-autograd.html#cb72-26"></a>w2 &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(d_hidden, d_out, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb72-27"><a href="simple-net-autograd.html#cb72-27"></a></span>
<span id="cb72-28"><a href="simple-net-autograd.html#cb72-28"></a><span class="co"># hidden layer bias</span></span>
<span id="cb72-29"><a href="simple-net-autograd.html#cb72-29"></a>b1 &lt;-<span class="st"> </span><span class="kw">torch_zeros</span>(<span class="dv">1</span>, d_hidden, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb72-30"><a href="simple-net-autograd.html#cb72-30"></a><span class="co"># output layer bias</span></span>
<span id="cb72-31"><a href="simple-net-autograd.html#cb72-31"></a>b2 &lt;-<span class="st"> </span><span class="kw">torch_zeros</span>(<span class="dv">1</span>, d_out, <span class="dt">requires_grad =</span> <span class="ot">TRUE</span>)</span>
<span id="cb72-32"><a href="simple-net-autograd.html#cb72-32"></a></span>
<span id="cb72-33"><a href="simple-net-autograd.html#cb72-33"></a><span class="co">### network parameters ---------------------------------------------------------</span></span>
<span id="cb72-34"><a href="simple-net-autograd.html#cb72-34"></a></span>
<span id="cb72-35"><a href="simple-net-autograd.html#cb72-35"></a>learning_rate &lt;-<span class="st"> </span><span class="fl">1e-4</span></span>
<span id="cb72-36"><a href="simple-net-autograd.html#cb72-36"></a></span>
<span id="cb72-37"><a href="simple-net-autograd.html#cb72-37"></a><span class="co">### training loop --------------------------------------------------------------</span></span>
<span id="cb72-38"><a href="simple-net-autograd.html#cb72-38"></a></span>
<span id="cb72-39"><a href="simple-net-autograd.html#cb72-39"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">200</span>) {</span>
<span id="cb72-40"><a href="simple-net-autograd.html#cb72-40"></a>  <span class="co">### -------- Forward pass --------</span></span>
<span id="cb72-41"><a href="simple-net-autograd.html#cb72-41"></a>  </span>
<span id="cb72-42"><a href="simple-net-autograd.html#cb72-42"></a>  y_pred &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">mm</span>(w1)<span class="op">$</span><span class="kw">add</span>(b1)<span class="op">$</span><span class="kw">clamp</span>(<span class="dt">min =</span> <span class="dv">0</span>)<span class="op">$</span><span class="kw">mm</span>(w2)<span class="op">$</span><span class="kw">add</span>(b2)</span>
<span id="cb72-43"><a href="simple-net-autograd.html#cb72-43"></a>  </span>
<span id="cb72-44"><a href="simple-net-autograd.html#cb72-44"></a>  <span class="co">### -------- compute loss -------- </span></span>
<span id="cb72-45"><a href="simple-net-autograd.html#cb72-45"></a>  loss &lt;-<span class="st"> </span>(y_pred <span class="op">-</span><span class="st"> </span>y)<span class="op">$</span><span class="kw">pow</span>(<span class="dv">2</span>)<span class="op">$</span><span class="kw">sum</span>()</span>
<span id="cb72-46"><a href="simple-net-autograd.html#cb72-46"></a>  <span class="cf">if</span> (t <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)</span>
<span id="cb72-47"><a href="simple-net-autograd.html#cb72-47"></a>    <span class="kw">cat</span>(<span class="st">&quot;Epoch: &quot;</span>, t, <span class="st">&quot;   Loss: &quot;</span>, loss<span class="op">$</span><span class="kw">item</span>(), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb72-48"><a href="simple-net-autograd.html#cb72-48"></a>  </span>
<span id="cb72-49"><a href="simple-net-autograd.html#cb72-49"></a>  <span class="co">### -------- Backpropagation --------</span></span>
<span id="cb72-50"><a href="simple-net-autograd.html#cb72-50"></a>  </span>
<span id="cb72-51"><a href="simple-net-autograd.html#cb72-51"></a>  <span class="co"># compute the gradient of loss with respect to all tensors with requires_grad = True.</span></span>
<span id="cb72-52"><a href="simple-net-autograd.html#cb72-52"></a>  loss<span class="op">$</span><span class="kw">backward</span>()</span>
<span id="cb72-53"><a href="simple-net-autograd.html#cb72-53"></a>  </span>
<span id="cb72-54"><a href="simple-net-autograd.html#cb72-54"></a>  <span class="co">### -------- Update weights -------- </span></span>
<span id="cb72-55"><a href="simple-net-autograd.html#cb72-55"></a>  </span>
<span id="cb72-56"><a href="simple-net-autograd.html#cb72-56"></a>  <span class="co"># Wrap in with_no_grad() because this is a part we DON&#39;T want to record for automatic gradient computation</span></span>
<span id="cb72-57"><a href="simple-net-autograd.html#cb72-57"></a>   <span class="kw">with_no_grad</span>({</span>
<span id="cb72-58"><a href="simple-net-autograd.html#cb72-58"></a>     w1 &lt;-<span class="st"> </span>w1<span class="op">$</span><span class="kw">sub_</span>(learning_rate <span class="op">*</span><span class="st"> </span>w1<span class="op">$</span>grad)</span>
<span id="cb72-59"><a href="simple-net-autograd.html#cb72-59"></a>     w2 &lt;-<span class="st"> </span>w2<span class="op">$</span><span class="kw">sub_</span>(learning_rate <span class="op">*</span><span class="st"> </span>w2<span class="op">$</span>grad)</span>
<span id="cb72-60"><a href="simple-net-autograd.html#cb72-60"></a>     b1 &lt;-<span class="st"> </span>b1<span class="op">$</span><span class="kw">sub_</span>(learning_rate <span class="op">*</span><span class="st"> </span>b1<span class="op">$</span>grad)</span>
<span id="cb72-61"><a href="simple-net-autograd.html#cb72-61"></a>     b2 &lt;-<span class="st"> </span>b2<span class="op">$</span><span class="kw">sub_</span>(learning_rate <span class="op">*</span><span class="st"> </span>b2<span class="op">$</span>grad)  </span>
<span id="cb72-62"><a href="simple-net-autograd.html#cb72-62"></a>     </span>
<span id="cb72-63"><a href="simple-net-autograd.html#cb72-63"></a>     <span class="co"># Zero the gradients after every pass, because they&#39;d accumulate otherwise</span></span>
<span id="cb72-64"><a href="simple-net-autograd.html#cb72-64"></a>     w1<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</span>
<span id="cb72-65"><a href="simple-net-autograd.html#cb72-65"></a>     w2<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</span>
<span id="cb72-66"><a href="simple-net-autograd.html#cb72-66"></a>     b1<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</span>
<span id="cb72-67"><a href="simple-net-autograd.html#cb72-67"></a>     b2<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()  </span>
<span id="cb72-68"><a href="simple-net-autograd.html#cb72-68"></a>   })</span>
<span id="cb72-69"><a href="simple-net-autograd.html#cb72-69"></a></span>
<span id="cb72-70"><a href="simple-net-autograd.html#cb72-70"></a>}</span></code></pre></div>
<p>We still manually compute the forward pass, and we still manually update the weights. In the last two chapters of this
section, we’ll see how these parts of the logic can be made more modular and reusable, as well.</p>
</div>
<div id="appendix-python-code-2" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Appendix: Python code</h2>
<div class="sourceCode" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="simple-net-autograd.html#cb73-1"></a><span class="im">import</span> torch</span>
<span id="cb73-2"><a href="simple-net-autograd.html#cb73-2"></a></span>
<span id="cb73-3"><a href="simple-net-autograd.html#cb73-3"></a><span class="co">### generate training data -----------------------------------------------------</span></span>
<span id="cb73-4"><a href="simple-net-autograd.html#cb73-4"></a></span>
<span id="cb73-5"><a href="simple-net-autograd.html#cb73-5"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb73-6"><a href="simple-net-autograd.html#cb73-6"></a>d_in <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb73-7"><a href="simple-net-autograd.html#cb73-7"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb73-8"><a href="simple-net-autograd.html#cb73-8"></a>d_out <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb73-9"><a href="simple-net-autograd.html#cb73-9"></a><span class="co"># number of observations in training set</span></span>
<span id="cb73-10"><a href="simple-net-autograd.html#cb73-10"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb73-11"><a href="simple-net-autograd.html#cb73-11"></a></span>
<span id="cb73-12"><a href="simple-net-autograd.html#cb73-12"></a><span class="co"># create random data</span></span>
<span id="cb73-13"><a href="simple-net-autograd.html#cb73-13"></a>x <span class="op">=</span> torch.randn(n, d_in) </span>
<span id="cb73-14"><a href="simple-net-autograd.html#cb73-14"></a>y <span class="op">=</span> x[ : , <span class="dv">0</span>, <span class="va">None</span>] <span class="op">*</span> <span class="fl">0.2</span> <span class="op">-</span> x[ : , <span class="dv">1</span>, <span class="va">None</span>] <span class="op">*</span> <span class="fl">1.3</span> <span class="op">-</span> x[ : , <span class="dv">2</span>, <span class="va">None</span>] <span class="op">*</span> <span class="fl">0.5</span> <span class="op">+</span> torch.randn(n, <span class="dv">1</span>)</span>
<span id="cb73-15"><a href="simple-net-autograd.html#cb73-15"></a></span>
<span id="cb73-16"><a href="simple-net-autograd.html#cb73-16"></a></span>
<span id="cb73-17"><a href="simple-net-autograd.html#cb73-17"></a><span class="co">### initialize weights ---------------------------------------------------------</span></span>
<span id="cb73-18"><a href="simple-net-autograd.html#cb73-18"></a></span>
<span id="cb73-19"><a href="simple-net-autograd.html#cb73-19"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb73-20"><a href="simple-net-autograd.html#cb73-20"></a>d_hidden <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb73-21"><a href="simple-net-autograd.html#cb73-21"></a><span class="co"># weights connecting input to hidden layer</span></span>
<span id="cb73-22"><a href="simple-net-autograd.html#cb73-22"></a>w1 <span class="op">=</span> torch.randn(d_in, d_hidden, requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb73-23"><a href="simple-net-autograd.html#cb73-23"></a><span class="co"># weights connecting hidden to output layer</span></span>
<span id="cb73-24"><a href="simple-net-autograd.html#cb73-24"></a>w2 <span class="op">=</span> torch.randn(d_hidden, d_out, requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb73-25"><a href="simple-net-autograd.html#cb73-25"></a></span>
<span id="cb73-26"><a href="simple-net-autograd.html#cb73-26"></a><span class="co"># hidden layer bias</span></span>
<span id="cb73-27"><a href="simple-net-autograd.html#cb73-27"></a>b1 <span class="op">=</span> torch.zeros((<span class="dv">1</span>, d_hidden), requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb73-28"><a href="simple-net-autograd.html#cb73-28"></a><span class="co"># output layer bias</span></span>
<span id="cb73-29"><a href="simple-net-autograd.html#cb73-29"></a>b2 <span class="op">=</span> torch.zeros((<span class="dv">1</span>, d_out), requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb73-30"><a href="simple-net-autograd.html#cb73-30"></a></span>
<span id="cb73-31"><a href="simple-net-autograd.html#cb73-31"></a><span class="co">### network parameters ---------------------------------------------------------</span></span>
<span id="cb73-32"><a href="simple-net-autograd.html#cb73-32"></a></span>
<span id="cb73-33"><a href="simple-net-autograd.html#cb73-33"></a>learning_rate <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb73-34"><a href="simple-net-autograd.html#cb73-34"></a></span>
<span id="cb73-35"><a href="simple-net-autograd.html#cb73-35"></a><span class="co">### training loop --------------------------------------------------------------</span></span>
<span id="cb73-36"><a href="simple-net-autograd.html#cb73-36"></a></span>
<span id="cb73-37"><a href="simple-net-autograd.html#cb73-37"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb73-38"><a href="simple-net-autograd.html#cb73-38"></a>    </span>
<span id="cb73-39"><a href="simple-net-autograd.html#cb73-39"></a>    <span class="co">### -------- Forward pass -------- </span></span>
<span id="cb73-40"><a href="simple-net-autograd.html#cb73-40"></a></span>
<span id="cb73-41"><a href="simple-net-autograd.html#cb73-41"></a>    y_pred <span class="op">=</span> x.mm(w1).add(b1).clamp(<span class="bu">min</span> <span class="op">=</span> <span class="dv">0</span>).mm(w2).add(b2)</span>
<span id="cb73-42"><a href="simple-net-autograd.html#cb73-42"></a></span>
<span id="cb73-43"><a href="simple-net-autograd.html#cb73-43"></a>    <span class="co">### -------- compute loss -------- </span></span>
<span id="cb73-44"><a href="simple-net-autograd.html#cb73-44"></a>    loss <span class="op">=</span> (y_pred <span class="op">-</span> y).<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb73-45"><a href="simple-net-autograd.html#cb73-45"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span>(t, loss.item())</span>
<span id="cb73-46"><a href="simple-net-autograd.html#cb73-46"></a></span>
<span id="cb73-47"><a href="simple-net-autograd.html#cb73-47"></a>    <span class="co">### -------- Backpropagation -------- </span></span>
<span id="cb73-48"><a href="simple-net-autograd.html#cb73-48"></a></span>
<span id="cb73-49"><a href="simple-net-autograd.html#cb73-49"></a>    <span class="co"># compute the gradient of loss with respect to all tensors with requires_grad = True.</span></span>
<span id="cb73-50"><a href="simple-net-autograd.html#cb73-50"></a>    loss.backward()</span>
<span id="cb73-51"><a href="simple-net-autograd.html#cb73-51"></a> </span>
<span id="cb73-52"><a href="simple-net-autograd.html#cb73-52"></a>    <span class="co">### -------- Update weights -------- </span></span>
<span id="cb73-53"><a href="simple-net-autograd.html#cb73-53"></a>    </span>
<span id="cb73-54"><a href="simple-net-autograd.html#cb73-54"></a>    <span class="co"># Wrap in torch.no_grad() because this is a part we DON&#39;T want to record for automatic gradient computation</span></span>
<span id="cb73-55"><a href="simple-net-autograd.html#cb73-55"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb73-56"><a href="simple-net-autograd.html#cb73-56"></a>        w1 <span class="op">-=</span> learning_rate <span class="op">*</span> w1.grad</span>
<span id="cb73-57"><a href="simple-net-autograd.html#cb73-57"></a>        w2 <span class="op">-=</span> learning_rate <span class="op">*</span> w2.grad</span>
<span id="cb73-58"><a href="simple-net-autograd.html#cb73-58"></a>        b1 <span class="op">-=</span> learning_rate <span class="op">*</span> b1.grad</span>
<span id="cb73-59"><a href="simple-net-autograd.html#cb73-59"></a>        b2 <span class="op">-=</span> learning_rate <span class="op">*</span> b2.grad</span>
<span id="cb73-60"><a href="simple-net-autograd.html#cb73-60"></a></span>
<span id="cb73-61"><a href="simple-net-autograd.html#cb73-61"></a>        <span class="co"># Zero the gradients after every pass, because they&#39;d accumulate otherwise</span></span>
<span id="cb73-62"><a href="simple-net-autograd.html#cb73-62"></a>        w1.grad.zero_()<span class="op">;</span></span>
<span id="cb73-63"><a href="simple-net-autograd.html#cb73-63"></a>        w2.grad.zero_()<span class="op">;</span></span>
<span id="cb73-64"><a href="simple-net-autograd.html#cb73-64"></a>        b1.grad.zero_()<span class="op">;</span></span>
<span id="cb73-65"><a href="simple-net-autograd.html#cb73-65"></a>        b2.grad.zero_()<span class="op">;</span></span></code></pre></div>
</div>
<div id="section" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> </h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>Here “pointwise” implies that the weights are the same for every input fed into the layer. These feedforward neural
networks would therefore be analogous to convolutional layers with kernel siez 1.<a href="simple-net-autograd.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-net-tensors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple-net-modules.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
