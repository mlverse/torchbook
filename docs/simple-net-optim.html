<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Using torch optimizers | Applied deep learning with torch from R</title>
  <meta name="description" content="tbd" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Using torch optimizers | Applied deep learning with torch from R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="tbd" />
  <meta property="og:image" content="tbdcover.png" />
  <meta property="og:description" content="tbd" />
  <meta name="github-repo" content="tbd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Using torch optimizers | Applied deep learning with torch from R" />
  
  <meta name="twitter:description" content="tbd" />
  <meta name="twitter:image" content="tbdcover.png" />

<meta name="author" content="tbd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-net-modules.html"/>
<link rel="next" href="image-recognition-intro.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
</ul></li>
<li class="part"><span><b>I Using Torch</b></span></li>
<li class="chapter" data-level="" data-path="using-torch-intro.html"><a href="using-torch-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="2" data-path="simple-net-R.html"><a href="simple-net-R.html"><i class="fa fa-check"></i><b>2</b> A simple neural network in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#whats-in-a-network"><i class="fa fa-check"></i><b>2.1</b> What’s in a network?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="simple-net-R.html"><a href="simple-net-R.html#gradient-descent"><i class="fa fa-check"></i><b>2.1.1</b> Gradient descent</a></li>
<li class="chapter" data-level="2.1.2" data-path="simple-net-R.html"><a href="simple-net-R.html#from-linear-regression-to-a-simple-network"><i class="fa fa-check"></i><b>2.1.2</b> From linear regression to a simple network</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#a-simple-network"><i class="fa fa-check"></i><b>2.2</b> A simple network</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#simulate-data"><i class="fa fa-check"></i><b>2.2.1</b> Simulate data</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#initialize-weights-and-biases"><i class="fa fa-check"></i><b>2.2.2</b> Initialize weights and biases</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>2.2.3</b> Training loop</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple-net-R.html"><a href="simple-net-R.html#complete-code"><i class="fa fa-check"></i><b>2.2.4</b> Complete code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html"><i class="fa fa-check"></i><b>3</b> Modifying the simple network to use torch tensors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#tensors"><i class="fa fa-check"></i><b>3.1</b> Tensors</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#creation"><i class="fa fa-check"></i><b>3.1.1</b> Creation</a></li>
<li class="chapter" data-level="3.1.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#conversion-to-built-in-r-data-types"><i class="fa fa-check"></i><b>3.1.2</b> Conversion to built-in R data types</a></li>
<li class="chapter" data-level="3.1.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#indexing-and-slicing-tensors"><i class="fa fa-check"></i><b>3.1.3</b> Indexing and slicing tensors</a></li>
<li class="chapter" data-level="3.1.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#reshaping-tensors"><i class="fa fa-check"></i><b>3.1.4</b> Reshaping tensors</a></li>
<li class="chapter" data-level="3.1.5" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#operations-on-tensors"><i class="fa fa-check"></i><b>3.1.5</b> Operations on tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#running-on-gpu"><i class="fa fa-check"></i><b>3.2</b> Running on GPU</a></li>
<li class="chapter" data-level="3.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#broadcasting"><i class="fa fa-check"></i><b>3.3</b> Broadcasting</a></li>
<li class="chapter" data-level="3.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#simple-neural-network-using-torch-tensors"><i class="fa fa-check"></i><b>3.4</b> Simple neural network using <code>torch</code> tensors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html"><i class="fa fa-check"></i><b>4</b> Using autograd</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#automatic-differentiation-with-autograd"><i class="fa fa-check"></i><b>4.1</b> Automatic differentiation with <em>autograd</em></a></li>
<li class="chapter" data-level="4.2" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#the-simple-network-now-using-autograd"><i class="fa fa-check"></i><b>4.2</b> The simple network, now using <em>autograd</em></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple-net-modules.html"><a href="simple-net-modules.html"><i class="fa fa-check"></i><b>5</b> Using torch modules</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#modules"><i class="fa fa-check"></i><b>5.1</b> Modules</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#base-modules-layers"><i class="fa fa-check"></i><b>5.1.1</b> Base modules (“layers”)</a></li>
<li class="chapter" data-level="5.1.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#container-modules-models"><i class="fa fa-check"></i><b>5.1.2</b> Container modules (“models”)</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#simple-network-using-modules"><i class="fa fa-check"></i><b>5.2</b> Simple network using modules</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="simple-net-optim.html"><a href="simple-net-optim.html"><i class="fa fa-check"></i><b>6</b> Using <code>torch</code> optimizers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="simple-net-optim.html"><a href="simple-net-optim.html#losses-and-loss-functions"><i class="fa fa-check"></i><b>6.1</b> Losses and loss functions</a></li>
<li class="chapter" data-level="6.2" data-path="simple-net-optim.html"><a href="simple-net-optim.html#optimizers"><i class="fa fa-check"></i><b>6.2</b> Optimizers</a></li>
<li class="chapter" data-level="6.3" data-path="simple-net-optim.html"><a href="simple-net-optim.html#simple-network-final-version"><i class="fa fa-check"></i><b>6.3</b> Simple network: final version</a></li>
</ul></li>
<li class="part"><span><b>II Image Recognition</b></span></li>
<li class="chapter" data-level="" data-path="image-recognition-intro.html"><a href="image-recognition-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="7" data-path="image-classification.html"><a href="image-classification.html"><i class="fa fa-check"></i><b>7</b> Classifying images</a>
<ul>
<li class="chapter" data-level="7.1" data-path="image-classification.html"><a href="image-classification.html#data-loading-and-transformation"><i class="fa fa-check"></i><b>7.1</b> Data loading and transformation</a></li>
<li class="chapter" data-level="7.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>7.2</b> Model</a></li>
<li class="chapter" data-level="7.3" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>7.3</b> Training</a></li>
<li class="chapter" data-level="7.4" data-path="image-classification.html"><a href="image-classification.html#performance-on-the-test-set"><i class="fa fa-check"></i><b>7.4</b> Performance on the test set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="unet.html"><a href="unet.html"><i class="fa fa-check"></i><b>8</b> Brain Image Segmentation with U-Net</a>
<ul>
<li class="chapter" data-level="8.1" data-path="unet.html"><a href="unet.html#image-segmentation-in-a-nutshell"><i class="fa fa-check"></i><b>8.1</b> Image segmentation in a nutshell</a></li>
<li class="chapter" data-level="8.2" data-path="unet.html"><a href="unet.html#u-net"><i class="fa fa-check"></i><b>8.2</b> U-Net</a></li>
<li class="chapter" data-level="8.3" data-path="unet.html"><a href="unet.html#example-application-mri-images"><i class="fa fa-check"></i><b>8.3</b> Example application: MRI images</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="unet.html"><a href="unet.html#preprocessing"><i class="fa fa-check"></i><b>8.3.1</b> Preprocessing</a></li>
<li class="chapter" data-level="8.3.2" data-path="unet.html"><a href="unet.html#u-net-model"><i class="fa fa-check"></i><b>8.3.2</b> U-Net model</a></li>
<li class="chapter" data-level="8.3.3" data-path="unet.html"><a href="unet.html#loss"><i class="fa fa-check"></i><b>8.3.3</b> Loss</a></li>
<li class="chapter" data-level="8.3.4" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>8.3.4</b> Training</a></li>
<li class="chapter" data-level="8.3.5" data-path="unet.html"><a href="unet.html#predictions"><i class="fa fa-check"></i><b>8.3.5</b> Predictions</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Natural language processing</b></span></li>
<li class="chapter" data-level="" data-path="NLP-intro.html"><a href="NLP-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="9" data-path="seq2seq-att.html"><a href="seq2seq-att.html"><i class="fa fa-check"></i><b>9</b> Sequence-to-sequence models with attention</a>
<ul>
<li class="chapter" data-level="9.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#why-attention"><i class="fa fa-check"></i><b>9.1</b> Why attention?</a></li>
<li class="chapter" data-level="9.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#preprocessing-with-torchtext"><i class="fa fa-check"></i><b>9.2</b> Preprocessing with <code>torchtext</code></a></li>
<li class="chapter" data-level="9.3" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>9.3</b> Model</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>9.3.1</b> Encoder</a></li>
<li class="chapter" data-level="9.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#attention-module"><i class="fa fa-check"></i><b>9.3.2</b> Attention module</a></li>
<li class="chapter" data-level="9.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>9.3.3</b> Decoder</a></li>
<li class="chapter" data-level="9.3.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#seq2seq-module"><i class="fa fa-check"></i><b>9.3.4</b> Seq2seq module</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#training-and-evaluation"><i class="fa fa-check"></i><b>9.4</b> Training and evaluation</a></li>
<li class="chapter" data-level="9.5" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>9.5</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="transformer.html"><a href="transformer.html"><i class="fa fa-check"></i><b>10</b> Torch transformer modules</a>
<ul>
<li class="chapter" data-level="10.1" data-path="transformer.html"><a href="transformer.html#attention-is-all-you-need"><i class="fa fa-check"></i><b>10.1</b> “Attention is all you need”</a></li>
<li class="chapter" data-level="10.2" data-path="transformer.html"><a href="transformer.html#implementation-building-blocks"><i class="fa fa-check"></i><b>10.2</b> Implementation: building blocks</a></li>
<li class="chapter" data-level="10.3" data-path="transformer.html"><a href="transformer.html#a-transformer-for-natural-language-translation"><i class="fa fa-check"></i><b>10.3</b> A Transformer for natural language translation</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="transformer.html"><a href="transformer.html#load-data"><i class="fa fa-check"></i><b>10.3.1</b> Load data</a></li>
<li class="chapter" data-level="10.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>10.3.2</b> Encoder</a></li>
<li class="chapter" data-level="10.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>10.3.3</b> Decoder</a></li>
<li class="chapter" data-level="10.3.4" data-path="transformer.html"><a href="transformer.html#overall-model"><i class="fa fa-check"></i><b>10.3.4</b> Overall model</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>10.4</b> Results</a></li>
</ul></li>
<li class="part"><span><b>IV Deep learning for tabular data</b></span></li>
<li class="chapter" data-level="" data-path="tabular-intro.html"><a href="tabular-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>V Time series</b></span></li>
<li class="chapter" data-level="" data-path="timeseries-intro.html"><a href="timeseries-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VI Generative deep learning</b></span></li>
<li class="chapter" data-level="" data-path="generative-intro.html"><a href="generative-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="11" data-path="gans.html"><a href="gans.html"><i class="fa fa-check"></i><b>11</b> Generative adversarial networks</a>
<ul>
<li class="chapter" data-level="11.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>11.1</b> Dataset</a></li>
<li class="chapter" data-level="11.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>11.2</b> Model</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="gans.html"><a href="gans.html#generator"><i class="fa fa-check"></i><b>11.2.1</b> Generator</a></li>
<li class="chapter" data-level="11.2.2" data-path="gans.html"><a href="gans.html#discriminator"><i class="fa fa-check"></i><b>11.2.2</b> Discriminator</a></li>
<li class="chapter" data-level="11.2.3" data-path="gans.html"><a href="gans.html#optimizers-and-loss-function"><i class="fa fa-check"></i><b>11.2.3</b> Optimizers and loss function</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>11.3</b> Training loop</a></li>
<li class="chapter" data-level="11.4" data-path="gans.html"><a href="gans.html#artifacts"><i class="fa fa-check"></i><b>11.4</b> Artifacts</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="vaes.html"><a href="vaes.html"><i class="fa fa-check"></i><b>12</b> Variational autoencoders</a>
<ul>
<li class="chapter" data-level="12.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>12.1</b> Dataset</a></li>
<li class="chapter" data-level="12.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>12.2</b> Model</a></li>
<li class="chapter" data-level="12.3" data-path="vaes.html"><a href="vaes.html#training-the-vae"><i class="fa fa-check"></i><b>12.3</b> Training the VAE</a></li>
<li class="chapter" data-level="12.4" data-path="vaes.html"><a href="vaes.html#latent-space"><i class="fa fa-check"></i><b>12.4</b> Latent space</a></li>
</ul></li>
<li class="part"><span><b>VII Deep learning on graphs</b></span></li>
<li class="chapter" data-level="" data-path="graph-intro.html"><a href="graph-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VIII Probabilistic deep learning</b></span></li>
<li class="chapter" data-level="" data-path="probabilistic-intro.html"><a href="probabilistic-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>IX Private and secure deep learning</b></span></li>
<li class="chapter" data-level="" data-path="private-secure-intro.html"><a href="private-secure-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>X Research topics</b></span></li>
<li class="chapter" data-level="" data-path="research-intro.html"><a href="research-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied deep learning with torch from R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple_net_optim" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Using <code>torch</code> optimizers</h1>
<p>Based on where we ended up in the last section, there are just two more things to do. For one, we still compute the loss by hand. And secondly, even though we get the gradients all nicely computed from <em>autograd</em>, we still loop over the model’s parameters, updating them all ourselves. You won’t be surprised to hear that none of this is necessary.</p>
<div id="losses-and-loss-functions" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Losses and loss functions</h2>
<p><code>torch</code> comes with all the usual loss functions, such as mean squared error, cross entropy, Kullback-Leibler divergence, and the like. In general, there are two usage modes.</p>
<p>Take the example of calculating mean squared error. One way is to call <code>nnf_mse_loss()</code> directly on the prediction and ground truth tensors. For example:</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="simple-net-optim.html#cb155-1"></a><span class="kw">library</span>(torch)</span>
<span id="cb155-2"><a href="simple-net-optim.html#cb155-2"></a>x &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb155-3"><a href="simple-net-optim.html#cb155-3"></a>y &lt;-<span class="st"> </span><span class="kw">torch_zeros</span>(<span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb155-4"><a href="simple-net-optim.html#cb155-4"></a></span>
<span id="cb155-5"><a href="simple-net-optim.html#cb155-5"></a><span class="kw">nnf_mse_loss</span>(x, y)</span></code></pre></div>
<pre><code>torch_tensor 
0.682362
[ CPUFloatType{} ]</code></pre>
<p>Other loss functions designed to be called directly start with <code>nnf_</code> as well: <code>nnf_binary_cross_entropy()</code>, <code>nnf_nll_loss()</code>, <code>nnf_kl_div()</code> … and so on.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>The second way is to define the algorithm in advance and call it at some later time. Here, respective constructors all start with <code>nn_</code> and end in <code>_loss</code>. For example: <code>nn_bce_loss()</code>, <code>nn_nll_loss(),</code> <code>nn_kl_div_loss()</code> …<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="simple-net-optim.html#cb157-1"></a>loss &lt;-<span class="st"> </span><span class="kw">nn_mse_loss</span>()</span>
<span id="cb157-2"><a href="simple-net-optim.html#cb157-2"></a></span>
<span id="cb157-3"><a href="simple-net-optim.html#cb157-3"></a><span class="kw">loss</span>(x, y)</span></code></pre></div>
<pre><code>torch_tensor 
0.682362
[ CPUFloatType{} ]</code></pre>
<p>This method may be preferable when one and the same algorithm should be applied to more than one pair of tensors.</p>
</div>
<div id="optimizers" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Optimizers</h2>
<p>So far, we’ve been updating model parameters following a simple strategy: The gradients told us which direction on the loss curve was downward; the learning rate told us how big of a step to take. What we did was a straightforward implementation of <em>gradient descent</em>.</p>
<p>However, optimization algorithms used in deep learning get a lot more sophisticated than that. Below, we’ll see how to replace our manual updates using <code>optim_adam()</code>, <code>torch</code>’s implementation of the Adam algorithm <span class="citation">(Kingma and Ba <a href="#ref-kingma2017adam" role="doc-biblioref">2017</a>)</span>. First though, let’s take a quick look at how <code>torch</code> optimizers work.</p>
<p>Here is a very simple network, consisting of just one linear layer, to be called on a single data point.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="simple-net-optim.html#cb159-1"></a>data &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb159-2"><a href="simple-net-optim.html#cb159-2"></a></span>
<span id="cb159-3"><a href="simple-net-optim.html#cb159-3"></a>model &lt;-<span class="st"> </span><span class="kw">nn_linear</span>(<span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb159-4"><a href="simple-net-optim.html#cb159-4"></a>model<span class="op">$</span>parameters</span></code></pre></div>
<pre><code>$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]</code></pre>
<p>When we create an optimizer, we tell it what parameters it is supposed to work on.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="simple-net-optim.html#cb161-1"></a>optimizer &lt;-<span class="st"> </span><span class="kw">optim_adam</span>(model<span class="op">$</span>parameters, <span class="dt">lr =</span> <span class="fl">0.01</span>)</span>
<span id="cb161-2"><a href="simple-net-optim.html#cb161-2"></a>optimizer</span></code></pre></div>
<pre><code>&lt;optim_adam&gt;
  Inherits from: &lt;torch_Optimizer&gt;
  Public:
    add_param_group: function (param_group) 
    clone: function (deep = FALSE) 
    defaults: list
    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, 
    param_groups: list
    state: list
    step: function (closure = NULL) 
    zero_grad: function () </code></pre>
<p>At any time, we can inspect those parameters:</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="simple-net-optim.html#cb163-1"></a>optimizer<span class="op">$</span>param_groups[[<span class="dv">1</span>]]<span class="op">$</span>params</span></code></pre></div>
<pre><code>$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]</code></pre>
<p>Now we perform the forward and backward passes. The backward pass calculates the gradients, but does <em>not</em> update the parameters, as we can see both from the model <em>and</em> the optimizer objects:</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="simple-net-optim.html#cb165-1"></a>out &lt;-<span class="st"> </span><span class="kw">model</span>(data)</span>
<span id="cb165-2"><a href="simple-net-optim.html#cb165-2"></a>out<span class="op">$</span><span class="kw">backward</span>()</span>
<span id="cb165-3"><a href="simple-net-optim.html#cb165-3"></a></span>
<span id="cb165-4"><a href="simple-net-optim.html#cb165-4"></a>optimizer<span class="op">$</span>param_groups[[<span class="dv">1</span>]]<span class="op">$</span>params</span>
<span id="cb165-5"><a href="simple-net-optim.html#cb165-5"></a>model<span class="op">$</span>parameters</span></code></pre></div>
<pre><code>$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]

$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]</code></pre>
<p>Calling <code>step()</code> on the optimizer actually <em>performs</em> the updates. Again, let’s check that both model and optimizer now hold the updated values:</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="simple-net-optim.html#cb167-1"></a>optimizer<span class="op">$</span><span class="kw">step</span>()</span>
<span id="cb167-2"><a href="simple-net-optim.html#cb167-2"></a></span>
<span id="cb167-3"><a href="simple-net-optim.html#cb167-3"></a>optimizer<span class="op">$</span>param_groups[[<span class="dv">1</span>]]<span class="op">$</span>params</span>
<span id="cb167-4"><a href="simple-net-optim.html#cb167-4"></a>model<span class="op">$</span>parameters</span></code></pre></div>
<pre><code>NULL
$weight
torch_tensor 
-0.0285  0.1312 -0.5536
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.2050
[ CPUFloatType{1} ]

$weight
torch_tensor 
-0.0285  0.1312 -0.5536
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.2050
[ CPUFloatType{1} ]</code></pre>
<p>If we perform optimization in a loop, we need to make sure to call <code>optimizer$zero_grad()</code> on every step, as otherwise gradients would be accumulated. You can see this in our final version of the network.</p>
</div>
<div id="simple-network-final-version" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Simple network: final version</h2>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="simple-net-optim.html#cb169-1"></a><span class="kw">library</span>(torch)</span>
<span id="cb169-2"><a href="simple-net-optim.html#cb169-2"></a></span>
<span id="cb169-3"><a href="simple-net-optim.html#cb169-3"></a><span class="co">### generate training data -----------------------------------------------------</span></span>
<span id="cb169-4"><a href="simple-net-optim.html#cb169-4"></a></span>
<span id="cb169-5"><a href="simple-net-optim.html#cb169-5"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb169-6"><a href="simple-net-optim.html#cb169-6"></a>d_in &lt;-<span class="st"> </span><span class="dv">3</span></span>
<span id="cb169-7"><a href="simple-net-optim.html#cb169-7"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb169-8"><a href="simple-net-optim.html#cb169-8"></a>d_out &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb169-9"><a href="simple-net-optim.html#cb169-9"></a><span class="co"># number of observations in training set</span></span>
<span id="cb169-10"><a href="simple-net-optim.html#cb169-10"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb169-11"><a href="simple-net-optim.html#cb169-11"></a></span>
<span id="cb169-12"><a href="simple-net-optim.html#cb169-12"></a></span>
<span id="cb169-13"><a href="simple-net-optim.html#cb169-13"></a><span class="co"># create random data</span></span>
<span id="cb169-14"><a href="simple-net-optim.html#cb169-14"></a>x &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(n, d_in)</span>
<span id="cb169-15"><a href="simple-net-optim.html#cb169-15"></a>y &lt;-<span class="st"> </span>x[, <span class="dv">1</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">0.2</span> <span class="op">-</span><span class="st"> </span>x[, <span class="dv">2</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">1.3</span> <span class="op">-</span><span class="st"> </span>x[, <span class="dv">3</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">+</span><span class="st"> </span><span class="kw">torch_randn</span>(n, <span class="dv">1</span>)</span>
<span id="cb169-16"><a href="simple-net-optim.html#cb169-16"></a></span>
<span id="cb169-17"><a href="simple-net-optim.html#cb169-17"></a></span>
<span id="cb169-18"><a href="simple-net-optim.html#cb169-18"></a></span>
<span id="cb169-19"><a href="simple-net-optim.html#cb169-19"></a><span class="co">### define the network ---------------------------------------------------------</span></span>
<span id="cb169-20"><a href="simple-net-optim.html#cb169-20"></a></span>
<span id="cb169-21"><a href="simple-net-optim.html#cb169-21"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb169-22"><a href="simple-net-optim.html#cb169-22"></a>d_hidden &lt;-<span class="st"> </span><span class="dv">32</span></span>
<span id="cb169-23"><a href="simple-net-optim.html#cb169-23"></a></span>
<span id="cb169-24"><a href="simple-net-optim.html#cb169-24"></a>model &lt;-<span class="st"> </span><span class="kw">nn_sequential</span>(</span>
<span id="cb169-25"><a href="simple-net-optim.html#cb169-25"></a>  <span class="kw">nn_linear</span>(d_in, d_hidden),</span>
<span id="cb169-26"><a href="simple-net-optim.html#cb169-26"></a>  <span class="kw">nn_relu</span>(),</span>
<span id="cb169-27"><a href="simple-net-optim.html#cb169-27"></a>  <span class="kw">nn_linear</span>(d_hidden, d_out)</span>
<span id="cb169-28"><a href="simple-net-optim.html#cb169-28"></a>)</span>
<span id="cb169-29"><a href="simple-net-optim.html#cb169-29"></a></span>
<span id="cb169-30"><a href="simple-net-optim.html#cb169-30"></a><span class="co">### network parameters ---------------------------------------------------------</span></span>
<span id="cb169-31"><a href="simple-net-optim.html#cb169-31"></a></span>
<span id="cb169-32"><a href="simple-net-optim.html#cb169-32"></a><span class="co"># for adam, need to choose a much higher learning rate in this problem</span></span>
<span id="cb169-33"><a href="simple-net-optim.html#cb169-33"></a>learning_rate &lt;-<span class="st"> </span><span class="fl">0.08</span></span>
<span id="cb169-34"><a href="simple-net-optim.html#cb169-34"></a></span>
<span id="cb169-35"><a href="simple-net-optim.html#cb169-35"></a>optimizer &lt;-<span class="st"> </span><span class="kw">optim_adam</span>(model<span class="op">$</span>parameters, <span class="dt">lr =</span> learning_rate)</span>
<span id="cb169-36"><a href="simple-net-optim.html#cb169-36"></a></span>
<span id="cb169-37"><a href="simple-net-optim.html#cb169-37"></a><span class="co">### training loop --------------------------------------------------------------</span></span>
<span id="cb169-38"><a href="simple-net-optim.html#cb169-38"></a></span>
<span id="cb169-39"><a href="simple-net-optim.html#cb169-39"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">200</span>) {</span>
<span id="cb169-40"><a href="simple-net-optim.html#cb169-40"></a>  </span>
<span id="cb169-41"><a href="simple-net-optim.html#cb169-41"></a>  <span class="co">### -------- Forward pass -------- </span></span>
<span id="cb169-42"><a href="simple-net-optim.html#cb169-42"></a>  </span>
<span id="cb169-43"><a href="simple-net-optim.html#cb169-43"></a>  y_pred &lt;-<span class="st"> </span><span class="kw">model</span>(x)</span>
<span id="cb169-44"><a href="simple-net-optim.html#cb169-44"></a>  </span>
<span id="cb169-45"><a href="simple-net-optim.html#cb169-45"></a>  <span class="co">### -------- compute loss -------- </span></span>
<span id="cb169-46"><a href="simple-net-optim.html#cb169-46"></a>  loss &lt;-<span class="st"> </span><span class="kw">nnf_mse_loss</span>(y_pred, y, <span class="dt">reduction =</span> <span class="st">&quot;sum&quot;</span>)</span>
<span id="cb169-47"><a href="simple-net-optim.html#cb169-47"></a>  <span class="cf">if</span> (t <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)</span>
<span id="cb169-48"><a href="simple-net-optim.html#cb169-48"></a>    <span class="kw">cat</span>(<span class="st">&quot;Epoch: &quot;</span>, t, <span class="st">&quot;   Loss: &quot;</span>, loss<span class="op">$</span><span class="kw">item</span>(), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb169-49"><a href="simple-net-optim.html#cb169-49"></a>  </span>
<span id="cb169-50"><a href="simple-net-optim.html#cb169-50"></a>  <span class="co">### -------- Backpropagation -------- </span></span>
<span id="cb169-51"><a href="simple-net-optim.html#cb169-51"></a>  </span>
<span id="cb169-52"><a href="simple-net-optim.html#cb169-52"></a>  <span class="co"># Still need to zero out the gradients before the backward pass, only this time,</span></span>
<span id="cb169-53"><a href="simple-net-optim.html#cb169-53"></a>  <span class="co"># on the optimizer object</span></span>
<span id="cb169-54"><a href="simple-net-optim.html#cb169-54"></a>  optimizer<span class="op">$</span><span class="kw">zero_grad</span>()</span>
<span id="cb169-55"><a href="simple-net-optim.html#cb169-55"></a>  </span>
<span id="cb169-56"><a href="simple-net-optim.html#cb169-56"></a>  <span class="co"># gradients are still computed on the loss tensor (no change here)</span></span>
<span id="cb169-57"><a href="simple-net-optim.html#cb169-57"></a>  loss<span class="op">$</span><span class="kw">backward</span>()</span>
<span id="cb169-58"><a href="simple-net-optim.html#cb169-58"></a>  </span>
<span id="cb169-59"><a href="simple-net-optim.html#cb169-59"></a>  <span class="co">### -------- Update weights -------- </span></span>
<span id="cb169-60"><a href="simple-net-optim.html#cb169-60"></a>  </span>
<span id="cb169-61"><a href="simple-net-optim.html#cb169-61"></a>  <span class="co"># use the optimizer to update model parameters</span></span>
<span id="cb169-62"><a href="simple-net-optim.html#cb169-62"></a>  optimizer<span class="op">$</span><span class="kw">step</span>()</span>
<span id="cb169-63"><a href="simple-net-optim.html#cb169-63"></a>}</span></code></pre></div>
<p>And that’s it! We’ve seen all the major actors on stage: tensors, <em>autograd</em>, modules, loss functions, and optimizers. In the following chapters, we’ll explore how to use <em>torch</em> for standard deep learning tasks involving images, text, tabular data, and more.</p>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-kingma2017adam">
<p>Kingma, Diederik P., and Jimmy Ba. 2017. “Adam: A Method for Stochastic Optimization.” <a href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>The prefix <code>nnf_</code> was chosen because in PyTorch, the corresponding functions live in <a href="https://pytorch.org/docs/stable/nn.functional.html">torch.nn.functional</a>.<a href="simple-net-optim.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>This time, the corresponding PyTorch module is <a href="https://pytorch.org/docs/stable/nn.html">torch.nn</a>.<a href="simple-net-optim.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-net-modules.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="image-recognition-intro.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
