<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Using torch optimizers | Applied deep learning with torch from R</title>
  <meta name="description" content="tbd" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Using torch optimizers | Applied deep learning with torch from R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="tbd" />
  <meta property="og:image" content="tbdcover.png" />
  <meta property="og:description" content="tbd" />
  <meta name="github-repo" content="tbd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Using torch optimizers | Applied deep learning with torch from R" />
  
  <meta name="twitter:description" content="tbd" />
  <meta name="twitter:image" content="tbdcover.png" />

<meta name="author" content="tbd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-net-modules.html"/>
<link rel="next" href="image-recognition-intro.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
</ul></li>
<li class="part"><span><b>I Using Torch</b></span></li>
<li class="chapter" data-level="" data-path="using-torch-intro.html"><a href="using-torch-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="2" data-path="simple-net-R.html"><a href="simple-net-R.html"><i class="fa fa-check"></i><b>2</b> A simple neural network in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#whats-in-a-network"><i class="fa fa-check"></i><b>2.1</b> What’s in a network?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="simple-net-R.html"><a href="simple-net-R.html#gradient-descent"><i class="fa fa-check"></i><b>2.1.1</b> Gradient descent</a></li>
<li class="chapter" data-level="2.1.2" data-path="simple-net-R.html"><a href="simple-net-R.html#from-linear-regression-to-a-simple-network"><i class="fa fa-check"></i><b>2.1.2</b> From linear regression to a simple network</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#a-simple-network"><i class="fa fa-check"></i><b>2.2</b> A simple network</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="simple-net-R.html"><a href="simple-net-R.html#simulate-data"><i class="fa fa-check"></i><b>2.2.1</b> Simulate data</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple-net-R.html"><a href="simple-net-R.html#initialize-weights-and-biases"><i class="fa fa-check"></i><b>2.2.2</b> Initialize weights and biases</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>2.2.3</b> Training loop</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple-net-R.html"><a href="simple-net-R.html#complete-code"><i class="fa fa-check"></i><b>2.2.4</b> Complete code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>2.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html"><i class="fa fa-check"></i><b>3</b> Modifying the simple network to use torch tensors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#tensors"><i class="fa fa-check"></i><b>3.1</b> Tensors</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#creation"><i class="fa fa-check"></i><b>3.1.1</b> Creation</a></li>
<li class="chapter" data-level="3.1.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#conversion-to-built-in-r-data-types"><i class="fa fa-check"></i><b>3.1.2</b> Conversion to built-in R data types</a></li>
<li class="chapter" data-level="3.1.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#indexing-and-slicing-tensors"><i class="fa fa-check"></i><b>3.1.3</b> Indexing and slicing tensors</a></li>
<li class="chapter" data-level="3.1.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#reshaping-tensors"><i class="fa fa-check"></i><b>3.1.4</b> Reshaping tensors</a></li>
<li class="chapter" data-level="3.1.5" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#operations-on-tensors"><i class="fa fa-check"></i><b>3.1.5</b> Operations on tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#running-on-gpu"><i class="fa fa-check"></i><b>3.2</b> Running on GPU</a></li>
<li class="chapter" data-level="3.3" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#broadcasting"><i class="fa fa-check"></i><b>3.3</b> Broadcasting</a></li>
<li class="chapter" data-level="3.4" data-path="simple-net-tensors.html"><a href="simple-net-tensors.html#simple-neural-network-using-torch-tensors"><i class="fa fa-check"></i><b>3.4</b> Simple neural network using <code>torch</code> tensors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html"><i class="fa fa-check"></i><b>4</b> Using autograd</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#automatic-differentiation-with-autograd"><i class="fa fa-check"></i><b>4.1</b> Automatic differentiation with autograd</a></li>
<li class="chapter" data-level="4.2" data-path="simple-net-autograd.html"><a href="simple-net-autograd.html#the-simple-network-now-using-autograd"><i class="fa fa-check"></i><b>4.2</b> The simple network, now using autograd</a></li>
<li class="chapter" data-level="4.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>4.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple-net-modules.html"><a href="simple-net-modules.html"><i class="fa fa-check"></i><b>5</b> Using torch modules</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#modules"><i class="fa fa-check"></i><b>5.1</b> Modules</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="simple-net-modules.html"><a href="simple-net-modules.html#layers-as-modules"><i class="fa fa-check"></i><b>5.1.1</b> Layers as modules</a></li>
<li class="chapter" data-level="5.1.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#models-as-modules"><i class="fa fa-check"></i><b>5.1.2</b> Models as modules</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="simple-net-modules.html"><a href="simple-net-modules.html#simple-network-using-modules"><i class="fa fa-check"></i><b>5.2</b> Simple network using modules</a></li>
<li class="chapter" data-level="5.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>5.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="simple-net-optim.html"><a href="simple-net-optim.html"><i class="fa fa-check"></i><b>6</b> Using <code>torch</code> optimizers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="simple-net-optim.html"><a href="simple-net-optim.html#torch-optimizers"><i class="fa fa-check"></i><b>6.1</b> Torch optimizers</a></li>
<li class="chapter" data-level="6.2" data-path="simple-net-optim.html"><a href="simple-net-optim.html#simple-net-with-optim"><i class="fa fa-check"></i><b>6.2</b> Simple net with <code>optim</code></a></li>
<li class="chapter" data-level="6.3" data-path="simple-net-R.html"><a href="simple-net-R.html#appendix-python-code"><i class="fa fa-check"></i><b>6.3</b> Appendix: Python code</a></li>
</ul></li>
<li class="part"><span><b>II Image Recognition</b></span></li>
<li class="chapter" data-level="" data-path="image-recognition-intro.html"><a href="image-recognition-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="7" data-path="image-classification.html"><a href="image-classification.html"><i class="fa fa-check"></i><b>7</b> Classifying images</a>
<ul>
<li class="chapter" data-level="7.1" data-path="image-classification.html"><a href="image-classification.html#data-loading-and-transformation"><i class="fa fa-check"></i><b>7.1</b> Data loading and transformation</a></li>
<li class="chapter" data-level="7.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>7.2</b> Model</a></li>
<li class="chapter" data-level="7.3" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>7.3</b> Training</a></li>
<li class="chapter" data-level="7.4" data-path="image-classification.html"><a href="image-classification.html#performance-on-the-test-set"><i class="fa fa-check"></i><b>7.4</b> Performance on the test set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="unet.html"><a href="unet.html"><i class="fa fa-check"></i><b>8</b> Brain Image Segmentation with U-Net</a>
<ul>
<li class="chapter" data-level="8.1" data-path="unet.html"><a href="unet.html#image-segmentation-in-a-nutshell"><i class="fa fa-check"></i><b>8.1</b> Image segmentation in a nutshell</a></li>
<li class="chapter" data-level="8.2" data-path="unet.html"><a href="unet.html#u-net"><i class="fa fa-check"></i><b>8.2</b> U-Net</a></li>
<li class="chapter" data-level="8.3" data-path="unet.html"><a href="unet.html#example-application-mri-images"><i class="fa fa-check"></i><b>8.3</b> Example application: MRI images</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="unet.html"><a href="unet.html#preprocessing"><i class="fa fa-check"></i><b>8.3.1</b> Preprocessing</a></li>
<li class="chapter" data-level="8.3.2" data-path="unet.html"><a href="unet.html#u-net-model"><i class="fa fa-check"></i><b>8.3.2</b> U-Net model</a></li>
<li class="chapter" data-level="8.3.3" data-path="unet.html"><a href="unet.html#loss"><i class="fa fa-check"></i><b>8.3.3</b> Loss</a></li>
<li class="chapter" data-level="8.3.4" data-path="image-classification.html"><a href="image-classification.html#training"><i class="fa fa-check"></i><b>8.3.4</b> Training</a></li>
<li class="chapter" data-level="8.3.5" data-path="unet.html"><a href="unet.html#predictions"><i class="fa fa-check"></i><b>8.3.5</b> Predictions</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Natural language processing</b></span></li>
<li class="chapter" data-level="" data-path="NLP-intro.html"><a href="NLP-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="9" data-path="seq2seq-att.html"><a href="seq2seq-att.html"><i class="fa fa-check"></i><b>9</b> Sequence-to-sequence models with attention</a>
<ul>
<li class="chapter" data-level="9.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#why-attention"><i class="fa fa-check"></i><b>9.1</b> Why attention?</a></li>
<li class="chapter" data-level="9.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#preprocessing-with-torchtext"><i class="fa fa-check"></i><b>9.2</b> Preprocessing with <code>torchtext</code></a></li>
<li class="chapter" data-level="9.3" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>9.3</b> Model</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>9.3.1</b> Encoder</a></li>
<li class="chapter" data-level="9.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#attention-module"><i class="fa fa-check"></i><b>9.3.2</b> Attention module</a></li>
<li class="chapter" data-level="9.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>9.3.3</b> Decoder</a></li>
<li class="chapter" data-level="9.3.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#seq2seq-module"><i class="fa fa-check"></i><b>9.3.4</b> Seq2seq module</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#training-and-evaluation"><i class="fa fa-check"></i><b>9.4</b> Training and evaluation</a></li>
<li class="chapter" data-level="9.5" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>9.5</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="transformer.html"><a href="transformer.html"><i class="fa fa-check"></i><b>10</b> Torch transformer modules</a>
<ul>
<li class="chapter" data-level="10.1" data-path="transformer.html"><a href="transformer.html#attention-is-all-you-need"><i class="fa fa-check"></i><b>10.1</b> “Attention is all you need”</a></li>
<li class="chapter" data-level="10.2" data-path="transformer.html"><a href="transformer.html#implementation-building-blocks"><i class="fa fa-check"></i><b>10.2</b> Implementation: building blocks</a></li>
<li class="chapter" data-level="10.3" data-path="transformer.html"><a href="transformer.html#a-transformer-for-natural-language-translation"><i class="fa fa-check"></i><b>10.3</b> A Transformer for natural language translation</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="transformer.html"><a href="transformer.html#load-data"><i class="fa fa-check"></i><b>10.3.1</b> Load data</a></li>
<li class="chapter" data-level="10.3.2" data-path="seq2seq-att.html"><a href="seq2seq-att.html#encoder"><i class="fa fa-check"></i><b>10.3.2</b> Encoder</a></li>
<li class="chapter" data-level="10.3.3" data-path="seq2seq-att.html"><a href="seq2seq-att.html#decoder"><i class="fa fa-check"></i><b>10.3.3</b> Decoder</a></li>
<li class="chapter" data-level="10.3.4" data-path="transformer.html"><a href="transformer.html#overall-model"><i class="fa fa-check"></i><b>10.3.4</b> Overall model</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="seq2seq-att.html"><a href="seq2seq-att.html#results"><i class="fa fa-check"></i><b>10.4</b> Results</a></li>
</ul></li>
<li class="part"><span><b>IV Deep learning for tabular data</b></span></li>
<li class="chapter" data-level="" data-path="tabular-intro.html"><a href="tabular-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>V Time series</b></span></li>
<li class="chapter" data-level="" data-path="timeseries-intro.html"><a href="timeseries-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VI Generative deep learning</b></span></li>
<li class="chapter" data-level="" data-path="generative-intro.html"><a href="generative-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="11" data-path="gans.html"><a href="gans.html"><i class="fa fa-check"></i><b>11</b> Generative adversarial networks</a>
<ul>
<li class="chapter" data-level="11.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>11.1</b> Dataset</a></li>
<li class="chapter" data-level="11.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>11.2</b> Model</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="gans.html"><a href="gans.html#generator"><i class="fa fa-check"></i><b>11.2.1</b> Generator</a></li>
<li class="chapter" data-level="11.2.2" data-path="gans.html"><a href="gans.html#discriminator"><i class="fa fa-check"></i><b>11.2.2</b> Discriminator</a></li>
<li class="chapter" data-level="11.2.3" data-path="gans.html"><a href="gans.html#optimizers-and-loss-function"><i class="fa fa-check"></i><b>11.2.3</b> Optimizers and loss function</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="simple-net-R.html"><a href="simple-net-R.html#training-loop"><i class="fa fa-check"></i><b>11.3</b> Training loop</a></li>
<li class="chapter" data-level="11.4" data-path="gans.html"><a href="gans.html#artifacts"><i class="fa fa-check"></i><b>11.4</b> Artifacts</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="vaes.html"><a href="vaes.html"><i class="fa fa-check"></i><b>12</b> Variational autoencoders</a>
<ul>
<li class="chapter" data-level="12.1" data-path="gans.html"><a href="gans.html#dataset"><i class="fa fa-check"></i><b>12.1</b> Dataset</a></li>
<li class="chapter" data-level="12.2" data-path="image-classification.html"><a href="image-classification.html#model"><i class="fa fa-check"></i><b>12.2</b> Model</a></li>
<li class="chapter" data-level="12.3" data-path="vaes.html"><a href="vaes.html#training-the-vae"><i class="fa fa-check"></i><b>12.3</b> Training the VAE</a></li>
<li class="chapter" data-level="12.4" data-path="vaes.html"><a href="vaes.html#latent-space"><i class="fa fa-check"></i><b>12.4</b> Latent space</a></li>
</ul></li>
<li class="part"><span><b>VII Deep learning on graphs</b></span></li>
<li class="chapter" data-level="" data-path="graph-intro.html"><a href="graph-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>VIII Probabilistic deep learning</b></span></li>
<li class="chapter" data-level="" data-path="probabilistic-intro.html"><a href="probabilistic-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>IX Private and secure deep learning</b></span></li>
<li class="chapter" data-level="" data-path="private-secure-intro.html"><a href="private-secure-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>X Research topics</b></span></li>
<li class="chapter" data-level="" data-path="research-intro.html"><a href="research-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied deep learning with torch from R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple_net_optim" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Using <code>torch</code> optimizers</h1>
<p>So far, we have been updating model parameters ourselves, following a simple algorithm: The calculated gradients told us which
direction, on the loss curve, was “downward”; the learning rate told us how big a step to take in that direction. This is a
straightforward implementation of <em>gradient descent</em>.</p>
<p>However, more sophisticated optimization algorithms exist, and many of these are provided by <code>torch</code>: for example,
<a href="">Rmsprop</a>, <a href="">Adam</a>, or <a href="">Adadelta</a>. In this chapter, we’ll see how to replace our manual updates using <code>SGD</code>, <code>torch</code>’s
stochastic gradient descent optimizer.</p>
<div id="torch-optimizers" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Torch optimizers</h2>
<p>For demonstration, here is a very simple network, consisting of just a single linear layer, and a single data point to call
the model on.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="simple-net-optim.html#cb132-1"></a>model &lt;-<span class="st"> </span><span class="kw">nn_linear</span>(<span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb132-2"><a href="simple-net-optim.html#cb132-2"></a>model<span class="op">$</span>parameters</span>
<span id="cb132-3"><a href="simple-net-optim.html#cb132-3"></a></span>
<span id="cb132-4"><a href="simple-net-optim.html#cb132-4"></a>data &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(<span class="dv">1</span>, <span class="dv">3</span>)</span></code></pre></div>
<p>On creation, an optimizer is told what parameters to work on:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="simple-net-optim.html#cb133-1"></a>opt &lt;-<span class="st"> </span><span class="kw">optim_sgd</span>(model<span class="op">$</span>parameters, <span class="dt">lr =</span> <span class="fl">0.01</span>)</span>
<span id="cb133-2"><a href="simple-net-optim.html#cb133-2"></a>opt</span></code></pre></div>
<p>At any time, we can inspect those parameters:</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="simple-net-optim.html#cb134-1"></a>opt<span class="op">$</span>param_groups[[<span class="dv">1</span>]]<span class="op">$</span>params</span></code></pre></div>
<p>Now we perform the forward and the backward pass. The backward pass calculates the gradients, but does not update the
parameters, as we can see both from the model <em>and</em> the optimizer objects:</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="simple-net-optim.html#cb135-1"></a>out &lt;-<span class="st"> </span><span class="kw">model</span>(data)</span>
<span id="cb135-2"><a href="simple-net-optim.html#cb135-2"></a>out<span class="op">$</span><span class="kw">backward</span>()</span>
<span id="cb135-3"><a href="simple-net-optim.html#cb135-3"></a>opt<span class="op">$</span>param_groups[[<span class="dv">1</span>]]<span class="op">$</span>params</span>
<span id="cb135-4"><a href="simple-net-optim.html#cb135-4"></a>model<span class="op">$</span>parameters</span></code></pre></div>
<p>Calling <code>step()</code> on the optimizer actually does the updates:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="simple-net-optim.html#cb136-1"></a>opt<span class="op">$</span><span class="kw">step</span>()</span>
<span id="cb136-2"><a href="simple-net-optim.html#cb136-2"></a>opt<span class="op">$</span>param_groups[[<span class="dv">1</span>]]<span class="op">$</span>params</span>
<span id="cb136-3"><a href="simple-net-optim.html#cb136-3"></a>model<span class="op">$</span>parameters</span></code></pre></div>
<p>If we perform optimization in a loop, as we’d usually do with neural networks, we need to make sure we call
<code>optimizer$zero_grad()</code> on every step, as otherwise gradients would be accumulated. We’ll see this in our final version of the
network.</p>
</div>
<div id="simple-net-with-optim" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Simple net with <code>optim</code></h2>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="simple-net-optim.html#cb137-1"></a><span class="kw">library</span>(torch)</span>
<span id="cb137-2"><a href="simple-net-optim.html#cb137-2"></a></span>
<span id="cb137-3"><a href="simple-net-optim.html#cb137-3"></a><span class="co">### generate training data -----------------------------------------------------</span></span>
<span id="cb137-4"><a href="simple-net-optim.html#cb137-4"></a></span>
<span id="cb137-5"><a href="simple-net-optim.html#cb137-5"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb137-6"><a href="simple-net-optim.html#cb137-6"></a>d_in &lt;-<span class="st"> </span><span class="dv">3</span></span>
<span id="cb137-7"><a href="simple-net-optim.html#cb137-7"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb137-8"><a href="simple-net-optim.html#cb137-8"></a>d_out &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb137-9"><a href="simple-net-optim.html#cb137-9"></a><span class="co"># number of observations in training set</span></span>
<span id="cb137-10"><a href="simple-net-optim.html#cb137-10"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb137-11"><a href="simple-net-optim.html#cb137-11"></a></span>
<span id="cb137-12"><a href="simple-net-optim.html#cb137-12"></a></span>
<span id="cb137-13"><a href="simple-net-optim.html#cb137-13"></a><span class="co"># create random data</span></span>
<span id="cb137-14"><a href="simple-net-optim.html#cb137-14"></a>x &lt;-<span class="st"> </span><span class="kw">torch_randn</span>(n, d_in)</span>
<span id="cb137-15"><a href="simple-net-optim.html#cb137-15"></a>y &lt;-</span>
<span id="cb137-16"><a href="simple-net-optim.html#cb137-16"></a><span class="st">  </span>x[, <span class="dv">1</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">0.2</span> <span class="op">-</span><span class="st"> </span>x[, <span class="dv">2</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">1.3</span> <span class="op">-</span><span class="st"> </span>x[, <span class="dv">3</span>, <span class="ot">NULL</span>] <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">+</span><span class="st"> </span><span class="kw">torch_randn</span>(n, <span class="dv">1</span>)</span>
<span id="cb137-17"><a href="simple-net-optim.html#cb137-17"></a></span>
<span id="cb137-18"><a href="simple-net-optim.html#cb137-18"></a></span>
<span id="cb137-19"><a href="simple-net-optim.html#cb137-19"></a></span>
<span id="cb137-20"><a href="simple-net-optim.html#cb137-20"></a><span class="co">### define the network ---------------------------------------------------------</span></span>
<span id="cb137-21"><a href="simple-net-optim.html#cb137-21"></a></span>
<span id="cb137-22"><a href="simple-net-optim.html#cb137-22"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb137-23"><a href="simple-net-optim.html#cb137-23"></a>d_hidden &lt;-<span class="st"> </span><span class="dv">32</span></span>
<span id="cb137-24"><a href="simple-net-optim.html#cb137-24"></a></span>
<span id="cb137-25"><a href="simple-net-optim.html#cb137-25"></a>model &lt;-<span class="st"> </span><span class="kw">nn_sequential</span>(</span>
<span id="cb137-26"><a href="simple-net-optim.html#cb137-26"></a>  <span class="kw">nn_linear</span>(d_in, d_hidden),</span>
<span id="cb137-27"><a href="simple-net-optim.html#cb137-27"></a>  <span class="kw">nn_relu</span>(),</span>
<span id="cb137-28"><a href="simple-net-optim.html#cb137-28"></a>  <span class="kw">nn_linear</span>(d_hidden, d_out)</span>
<span id="cb137-29"><a href="simple-net-optim.html#cb137-29"></a>)</span>
<span id="cb137-30"><a href="simple-net-optim.html#cb137-30"></a></span>
<span id="cb137-31"><a href="simple-net-optim.html#cb137-31"></a><span class="co"># </span><span class="al">TBD</span></span>
<span id="cb137-32"><a href="simple-net-optim.html#cb137-32"></a><span class="co">#mse_loss = torch.nn.MSELoss(reduction=&#39;sum&#39;)</span></span>
<span id="cb137-33"><a href="simple-net-optim.html#cb137-33"></a></span>
<span id="cb137-34"><a href="simple-net-optim.html#cb137-34"></a></span>
<span id="cb137-35"><a href="simple-net-optim.html#cb137-35"></a><span class="co">### network parameters ---------------------------------------------------------</span></span>
<span id="cb137-36"><a href="simple-net-optim.html#cb137-36"></a></span>
<span id="cb137-37"><a href="simple-net-optim.html#cb137-37"></a>learning_rate &lt;-<span class="st"> </span><span class="fl">1e-4</span></span>
<span id="cb137-38"><a href="simple-net-optim.html#cb137-38"></a></span>
<span id="cb137-39"><a href="simple-net-optim.html#cb137-39"></a>optimizer &lt;-<span class="st"> </span><span class="kw">optim_sgd</span>(model<span class="op">$</span>parameters, <span class="dt">lr =</span> learning_rate)</span>
<span id="cb137-40"><a href="simple-net-optim.html#cb137-40"></a></span>
<span id="cb137-41"><a href="simple-net-optim.html#cb137-41"></a><span class="co">### training loop --------------------------------------------------------------</span></span>
<span id="cb137-42"><a href="simple-net-optim.html#cb137-42"></a></span>
<span id="cb137-43"><a href="simple-net-optim.html#cb137-43"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">200</span>) {</span>
<span id="cb137-44"><a href="simple-net-optim.html#cb137-44"></a>  </span>
<span id="cb137-45"><a href="simple-net-optim.html#cb137-45"></a>  <span class="co">### -------- Forward pass -------- </span></span>
<span id="cb137-46"><a href="simple-net-optim.html#cb137-46"></a>  </span>
<span id="cb137-47"><a href="simple-net-optim.html#cb137-47"></a>  y_pred &lt;-<span class="st"> </span><span class="kw">model</span>(x)</span>
<span id="cb137-48"><a href="simple-net-optim.html#cb137-48"></a>  </span>
<span id="cb137-49"><a href="simple-net-optim.html#cb137-49"></a>  <span class="co">### -------- compute loss -------- </span></span>
<span id="cb137-50"><a href="simple-net-optim.html#cb137-50"></a>  loss &lt;-<span class="st"> </span><span class="kw">nnf_mse_loss</span>(y_pred, y, <span class="dt">reduction =</span> <span class="st">&quot;sum&quot;</span>)</span>
<span id="cb137-51"><a href="simple-net-optim.html#cb137-51"></a>  <span class="kw">print</span>(<span class="kw">as.numeric</span>(loss))</span>
<span id="cb137-52"><a href="simple-net-optim.html#cb137-52"></a>  <span class="co">#if t % 10 == 0: print(t, as.numeric(loss))</span></span>
<span id="cb137-53"><a href="simple-net-optim.html#cb137-53"></a>  </span>
<span id="cb137-54"><a href="simple-net-optim.html#cb137-54"></a>  <span class="co">### -------- Backpropagation -------- </span></span>
<span id="cb137-55"><a href="simple-net-optim.html#cb137-55"></a>  </span>
<span id="cb137-56"><a href="simple-net-optim.html#cb137-56"></a>  <span class="co"># Still need to zero out the gradients before the backward pass, only this time, on the optimizer object</span></span>
<span id="cb137-57"><a href="simple-net-optim.html#cb137-57"></a>  optimizer<span class="op">$</span><span class="kw">zero_grad</span>()</span>
<span id="cb137-58"><a href="simple-net-optim.html#cb137-58"></a>  </span>
<span id="cb137-59"><a href="simple-net-optim.html#cb137-59"></a>  <span class="co"># gradients are still computed on the loss tensor</span></span>
<span id="cb137-60"><a href="simple-net-optim.html#cb137-60"></a>  loss<span class="op">$</span><span class="kw">backward</span>()</span>
<span id="cb137-61"><a href="simple-net-optim.html#cb137-61"></a>  </span>
<span id="cb137-62"><a href="simple-net-optim.html#cb137-62"></a>  <span class="co">### -------- Update weights -------- </span></span>
<span id="cb137-63"><a href="simple-net-optim.html#cb137-63"></a>  </span>
<span id="cb137-64"><a href="simple-net-optim.html#cb137-64"></a>  <span class="co"># use the ptimizer to update model parameters</span></span>
<span id="cb137-65"><a href="simple-net-optim.html#cb137-65"></a>  optimizer<span class="op">$</span><span class="kw">step</span>()</span>
<span id="cb137-66"><a href="simple-net-optim.html#cb137-66"></a>}</span></code></pre></div>
<p>At this point, we’re fully completed the transformation; the network has been fully torchified. In the next section, we’ll see
classical applications of deep learning for supervised and unsupervised learning.</p>
</div>
<div id="appendix-python-code" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Appendix: Python code</h2>
<div class="sourceCode" id="cb138"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb138-1"><a href="simple-net-optim.html#cb138-1"></a><span class="im">import</span> torch</span>
<span id="cb138-2"><a href="simple-net-optim.html#cb138-2"></a></span>
<span id="cb138-3"><a href="simple-net-optim.html#cb138-3"></a><span class="co">### generate training data -----------------------------------------------------</span></span>
<span id="cb138-4"><a href="simple-net-optim.html#cb138-4"></a></span>
<span id="cb138-5"><a href="simple-net-optim.html#cb138-5"></a><span class="co"># input dimensionality (number of input features)</span></span>
<span id="cb138-6"><a href="simple-net-optim.html#cb138-6"></a>d_in <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb138-7"><a href="simple-net-optim.html#cb138-7"></a><span class="co"># output dimensionality (number of predicted features)</span></span>
<span id="cb138-8"><a href="simple-net-optim.html#cb138-8"></a>d_out <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb138-9"><a href="simple-net-optim.html#cb138-9"></a><span class="co"># number of observations in training set</span></span>
<span id="cb138-10"><a href="simple-net-optim.html#cb138-10"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb138-11"><a href="simple-net-optim.html#cb138-11"></a></span>
<span id="cb138-12"><a href="simple-net-optim.html#cb138-12"></a><span class="co"># create random data</span></span>
<span id="cb138-13"><a href="simple-net-optim.html#cb138-13"></a>x <span class="op">=</span> torch.randn(n, d_in) </span>
<span id="cb138-14"><a href="simple-net-optim.html#cb138-14"></a>y <span class="op">=</span> x[ : , <span class="dv">0</span>, <span class="va">None</span>] <span class="op">*</span> <span class="fl">0.2</span> <span class="op">-</span> x[ : , <span class="dv">1</span>, <span class="va">None</span>] <span class="op">*</span> <span class="fl">1.3</span> <span class="op">-</span> x[ : , <span class="dv">2</span>, <span class="va">None</span>] <span class="op">*</span> <span class="fl">0.5</span> <span class="op">+</span> torch.randn(n, <span class="dv">1</span>)</span>
<span id="cb138-15"><a href="simple-net-optim.html#cb138-15"></a></span>
<span id="cb138-16"><a href="simple-net-optim.html#cb138-16"></a><span class="co">### define the network ---------------------------------------------------------</span></span>
<span id="cb138-17"><a href="simple-net-optim.html#cb138-17"></a></span>
<span id="cb138-18"><a href="simple-net-optim.html#cb138-18"></a><span class="co"># dimensionality of hidden layer</span></span>
<span id="cb138-19"><a href="simple-net-optim.html#cb138-19"></a>d_hidden <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb138-20"><a href="simple-net-optim.html#cb138-20"></a></span>
<span id="cb138-21"><a href="simple-net-optim.html#cb138-21"></a>model <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb138-22"><a href="simple-net-optim.html#cb138-22"></a>    torch.nn.Linear(d_in, d_hidden),</span>
<span id="cb138-23"><a href="simple-net-optim.html#cb138-23"></a>    torch.nn.ReLU(),</span>
<span id="cb138-24"><a href="simple-net-optim.html#cb138-24"></a>    torch.nn.Linear(d_hidden, d_out),</span>
<span id="cb138-25"><a href="simple-net-optim.html#cb138-25"></a>)</span>
<span id="cb138-26"><a href="simple-net-optim.html#cb138-26"></a></span>
<span id="cb138-27"><a href="simple-net-optim.html#cb138-27"></a>mse_loss <span class="op">=</span> torch.nn.MSELoss(reduction<span class="op">=</span><span class="st">&#39;sum&#39;</span>)</span>
<span id="cb138-28"><a href="simple-net-optim.html#cb138-28"></a></span>
<span id="cb138-29"><a href="simple-net-optim.html#cb138-29"></a></span>
<span id="cb138-30"><a href="simple-net-optim.html#cb138-30"></a><span class="co">### training loop --------------------------------------------------------------</span></span>
<span id="cb138-31"><a href="simple-net-optim.html#cb138-31"></a>learning_rate <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb138-32"><a href="simple-net-optim.html#cb138-32"></a></span>
<span id="cb138-33"><a href="simple-net-optim.html#cb138-33"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb138-34"><a href="simple-net-optim.html#cb138-34"></a></span>
<span id="cb138-35"><a href="simple-net-optim.html#cb138-35"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb138-36"><a href="simple-net-optim.html#cb138-36"></a>    </span>
<span id="cb138-37"><a href="simple-net-optim.html#cb138-37"></a>    <span class="co">### -------- Forward pass -------- </span></span>
<span id="cb138-38"><a href="simple-net-optim.html#cb138-38"></a></span>
<span id="cb138-39"><a href="simple-net-optim.html#cb138-39"></a>    y_pred <span class="op">=</span> model(x)</span>
<span id="cb138-40"><a href="simple-net-optim.html#cb138-40"></a></span>
<span id="cb138-41"><a href="simple-net-optim.html#cb138-41"></a>    <span class="co">### -------- compute loss -------- </span></span>
<span id="cb138-42"><a href="simple-net-optim.html#cb138-42"></a>    loss <span class="op">=</span> mse_loss(y_pred, y)</span>
<span id="cb138-43"><a href="simple-net-optim.html#cb138-43"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span>(t, loss.item())</span>
<span id="cb138-44"><a href="simple-net-optim.html#cb138-44"></a></span>
<span id="cb138-45"><a href="simple-net-optim.html#cb138-45"></a>    <span class="co">### -------- Backpropagation -------- </span></span>
<span id="cb138-46"><a href="simple-net-optim.html#cb138-46"></a></span>
<span id="cb138-47"><a href="simple-net-optim.html#cb138-47"></a>    <span class="co"># Still need to zero out the gradients before the backward pass, only this time, on the optimizer object</span></span>
<span id="cb138-48"><a href="simple-net-optim.html#cb138-48"></a>    optimizer.zero_grad()</span>
<span id="cb138-49"><a href="simple-net-optim.html#cb138-49"></a>    </span>
<span id="cb138-50"><a href="simple-net-optim.html#cb138-50"></a>    <span class="co"># gradients are still computed on the loss tensor</span></span>
<span id="cb138-51"><a href="simple-net-optim.html#cb138-51"></a>    loss.backward()</span>
<span id="cb138-52"><a href="simple-net-optim.html#cb138-52"></a> </span>
<span id="cb138-53"><a href="simple-net-optim.html#cb138-53"></a>    <span class="co">### -------- Update weights -------- </span></span>
<span id="cb138-54"><a href="simple-net-optim.html#cb138-54"></a>    </span>
<span id="cb138-55"><a href="simple-net-optim.html#cb138-55"></a>     <span class="co"># use the ptimizer to update model parameters</span></span>
<span id="cb138-56"><a href="simple-net-optim.html#cb138-56"></a>    optimizer.step()</span></code></pre></div>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="simple-net-modules.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="image-recognition-intro.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
