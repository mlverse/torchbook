
# Generative adversarial networks {#gans}

After image classification, our next deep learning "classic" is one of the -- as of today -- two main representatives of unsupervised deep learning. In generative adversarial networks, there is no well-defined loss function; instead, the setup is fundamentally game-theoretic: Two actors, the _generator_ and the _discriminator_, each try to minimize their loss; the outcome should be some artifact -- image, text, what have you -- that resembles the training data but does not copy them. 

In theory, this is a highly fascinating approach; in practice, it can be a challenge to set parameters in a way that good results are achieved. The architecture and settings presented here follow those reported in the original DCGAN article [@goodfellow2014generative]. In the meantime, a lot of research has been done; minor changes to loss functions, optimizers and/or parameters may make an important difference.


## Dataset

For this task, we use [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist) [@clanuwat2018deep], one of the more recent MNIST drop-ins. Kuzushiji-MNIST contains 70,000 grayscale images, of size 28x28 px just like MNIST, and also like MNIST, divided into 10 classes. 

We can use torch for loading it. With an unsupervised learning task such as this one, we only need the training set: 

```{python}
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import torchvision
from torchvision import transforms, datasets, utils
import os

transform = transforms.Compose([transforms.ToTensor()])

kmnist = torchvision.datasets.KMNIST("data/kmnist", train = True, transform = transform, download = True)

dataloader = torch.utils.data.DataLoader(kmnist, batch_size=128, shuffle = True, num_workers = 8)

```

Let's view a few of those. Here are the first 128 images:

```{python}
images = next(iter(dataloader))[0].cpu()
images.device
```

... of which, let's plot the first 16:

```{r}
library(reticulate)
library(dplyr)

index <- 1:16
images <- py$images$numpy()[index,,,] 

par(mfcol = c(4,4), mar = c(0, 0, 0, 0))
images %>%
  purrr::array_tree(1) %>%
  purrr::map(function(img) image(img, col = gray.colors(n = 255), xaxt = 'n', yaxt = 'n', asp = 1))
```


## Model

The model, in the abstract sense, consists of the interplay of two models, in the concrete sense -- two torch _modules_. The _generator_ produces fake artifacts -- fake Kuzushiji digits, in our case -- in the hope of getting better and better at it; the _discriminator_ is tasked with telling actual from fake images. (Its task should, if all goes well, get more difficult over time.)

Let's start with the generator.

### Generator

The generator is given a random noise vector (1d), and has to produce images (2d, of a given resolution).
Its main tool is repeated application of _transposed convolutions_ that upsample from a resolution of 1x1 to the required resolution of 28x28.

Following the DCGAN paper, the generator's `ConvTranspose2d` and `BatchNorm2d` layers are initialized according to a normal distribution with mean 0 and standard deviation 0.02.

```{python}
device = torch.device("cuda:0" if torch.cuda.is_available()  else "cpu")

# size of generator input
latent_input_size = 100

image_size = 28

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            # torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')
            # h_out = (h_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1
            # (1 - 1) * 1 - 2 * 0 + 1 * (4 -1 ) + 0 + 1
            # 4 x 4
            nn.ConvTranspose2d(latent_input_size, image_size * 4, 4, 1, 0, bias=False),
            nn.BatchNorm2d(image_size * 4),
            nn.ReLU(True),
            # 8 * 8
            nn.ConvTranspose2d(image_size * 4, image_size * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(image_size * 2),
            nn.ReLU(True),
            # 16 x 16
            nn.ConvTranspose2d(image_size * 2, image_size, 4, 2, 2, bias=False),
            nn.BatchNorm2d(image_size),
            nn.ReLU(True),
            # 28 x 28
            nn.ConvTranspose2d(image_size, 1, 4, 2, 1, bias=False),
            nn.Tanh()
        )
    def forward(self, input):
        return self.main(input)

generator = Generator().to(device)

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)
        
generator.apply(weights_init)

```


### Discriminator

The discriminator is a pretty conventional convnet. Its layers' weights are initialized in the same way as the generator's.

```{python}
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            # 14 x 14
            nn.Conv2d(1, image_size, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # 7 x 7
            nn.Conv2d(image_size, image_size * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(image_size * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # 3 x 3
            nn.Conv2d(image_size * 2, image_size * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(image_size * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # 1 x 1
            nn.Conv2d(image_size * 4, 1, 4, 2, 1, bias=False),
            nn.Sigmoid()
        )
    def forward(self, input):
        return self.main(input)


discriminator = Discriminator().to(device)

discriminator.apply(weights_init)

```


### Optimizers and loss function

While generator and discriminator have each their own losses, mathematically both use the same calculation, namely, binary crossentropy:

```{python}
criterion = nn.BCELoss()
```

They each have their own optimizer:

```{python}
lr = 0.0002
beta1 = 0.5

disc_optimizer = optim.Adam(discriminator.parameters(), lr = lr, betas = (beta1, 0.999))
gen_optimizer = optim.Adam(generator.parameters(), lr = lr, betas = (beta1, 0.999))
```

### Training loop

In every epoch, the training loop consists of three parts. 

First, the discriminator is trained. This is a two-stage procedure:
- In stage (1), it is given the real images, together with labels (fabricated on the fly) that say "these are real images". Binary cross entropy will be minimized when all those images are, in fact, classified as real by the discriminator.
- In stage (2), first the generator is asked to generate some images, and then the discriminator is asked to rate them. Again, binary cross entropy is calculated, but this time, it will be minimal if all images are characterized as fake.

Once gradients have been obtained in both computations, the discriminator's weights are updated. Then it's the generator's turn

```{python}
# random noise - input for generator
fixed_noise = torch.randn(64, latent_input_size, 1, 1, device = device)

real_label = 1
fake_label = 0

img_list = []
generator_losses = []
discriminator_losses = []

for epoch in range(num_epochs):

    for i, data in enumerate(dataloader, 0):
    
        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
        
        ## Train with all-real batch
        discriminator.zero_grad()
        real_cpu = data[0].to(device)
        b_size = real_cpu.size(0)
        label = torch.full((b_size,), real_label, device=device)
        output = discriminator(real_cpu).view(-1)
        discriminator_loss_real = criterion(output, label)
        discriminator_loss_real.backward()
        discriminator_real_verdict = output.mean().item()
        
        ## Train with all-fake batch
        noise = torch.randn(b_size, latent_input_size, 1, 1, device=device)
        fake = generator(noise)
        label.fill_(fake_label)
        output = discriminator(fake.detach()).view(-1)
        discriminator_loss_fake = criterion(output, label)
        discriminator_loss_fake.backward()
        
        discriminator_loss = discriminator_loss_real + discriminator_loss_fake
        # Update D
        disc_optimizer.step()
        ############################
        # (2) Update G network: maximize log(D(G(z)))
        ###########################
        generator.zero_grad()
        label.fill_(real_label)  # fake labels are real for generator cost
        # Since we just updated D, perform another forward pass of all-fake batch through D
        output = discriminator(fake).view(-1)
        # Calculate G's loss based on this output
        generator_loss = criterion(output, label)
        # Calculate gradients for G
        generator_loss.backward()
        discriminator_fake_verdict = output.mean().item()
        # Update G
        gen_optimizer.step()
        # Output training stats
        if i % 50 == 0:
            print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f '
                  % (epoch, num_epochs, i, len(dataloader),
                     discriminator_loss.item(), generator_loss.item(), discriminator_real_verdict, discriminator_fake_verdict))
        # Save Losses for plotting later
        generator_losses.append(generator_loss.item())
        discriminator_losses.append(discriminator_loss.item())
        # Check how the generator is doing by saving G's output on fixed_noise
        with torch.no_grad():
          fake = generator(fixed_noise).detach().cpu()
          img_list.append(utils.make_grid(fake, padding=2, normalize=True))

```

