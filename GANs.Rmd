
# Generative adversarial networks {#gans}

After image classification, our next deep learning "classic" is one of the -- as of today -- two main representatives of unsupervised deep learning. In generative adversarial networks, there is no well-defined loss function; instead, the setup is fundamentally game-theoretic: Two actors, the _generator_ and the _discriminator_, each try to minimize their loss; the outcome should be some artifact -- image, text, what have you -- that resembles the training data but does not copy them. 

In theory, this is a highly fascinating approach; in practice, it can be a challenge to set parameters in a way that good results are achieved. The architecture and settings presented here follow those reported in the original DCGAN article [@goodfellow2014generative]. In the meantime, a lot of research has been done; minor changes to loss functions, optimizers and/or parameters may make an important difference.


## Dataset

For this task, we use [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist) [@clanuwat2018deep], one of the more recent MNIST drop-ins. Kuzushiji-MNIST contains 70,000 grayscale images, of size 28x28 px just like MNIST, and also like MNIST, divided into 10 classes. 

We can use torch for loading it. With an unsupervised learning task such as this one, we only need the training set: 

```{python}
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import torchvision
from torchvision import transforms, datasets, utils
import os

transform = transforms.Compose([transforms.ToTensor()])

kmnist = torchvision.datasets.KMNIST("data/kmnist", train = True, transform = transform, download = True)

dataloader = torch.utils.data.DataLoader(kmnist, batch_size=128, shuffle = True, num_workers = 8)

```

Let's view a few of those. Here are the first 128 images:

```{python, eval = FALSE}
images = next(iter(dataloader))[0].cpu()
images.device
```

... of which, let's plot the first 16:

```{r, eval = FALSE}
library(reticulate)
library(dplyr)

index <- 1:16
images <- py$images$numpy()[index,,,] 

par(mfcol = c(4,4), mar = c(0, 0, 0, 0))
images %>%
  purrr::array_tree(1) %>%
  purrr::map(function(img) image(img, col = gray.colors(n = 255), xaxt = 'n', yaxt = 'n', asp = 1))
```


## Model

The model, in the abstract sense, consists of the interplay of two models, in the concrete sense -- two torch _modules_. The _generator_ produces fake artifacts -- fake Kuzushiji digits, in our case -- in the hope of getting better and better at it; the _discriminator_ is tasked with telling actual from fake images. (Its task should, if all goes well, get more difficult over time.)

Let's start with the generator.

### Generator

The generator is given a random noise vector (1d), and has to produce images (2d, of a given resolution).
Its main tool is repeated application of _transposed convolutions_ that upsample from a resolution of 1x1 to the required resolution of 28x28.

Following the DCGAN paper, the generator's `ConvTranspose2d` and `BatchNorm2d` layers are initialized according to a normal distribution with mean 0 and standard deviation 0.02.

```{python, eval = FALSE}
device = torch.device("cuda:0" if torch.cuda.is_available()  else "cpu")

# size of generator input
latent_input_size = 100

image_size = 28

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            # h_out = (h_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1
            # h_out = (  1  - 1) *   1    - 2 *    0    +     1    * (      4     - 1) +        0       + 1
            # 4 x 4
            nn.ConvTranspose2d(latent_input_size, image_size * 4, kernel_size = 4, stride = 1, padding = 0, bias = False),
            nn.BatchNorm2d(image_size * 4),
            nn.ReLU(True),
            # 8 * 8
            nn.ConvTranspose2d(image_size * 4, image_size * 2, kernel_size = 4, stride = 2, padding = 1, bias = False),
            nn.BatchNorm2d(image_size * 2),
            nn.ReLU(True),
            # 16 x 16
            nn.ConvTranspose2d(image_size * 2, image_size, kernel_size = 4, stride = 2, padding = 2, bias = False),
            nn.BatchNorm2d(image_size),
            nn.ReLU(True),
            # 28 x 28
            nn.ConvTranspose2d(image_size, 1, kernel_size = 4, stride = 2, padding = 1, bias = False),
            nn.Tanh()
        )
    def forward(self, input):
        return self.main(input)

generator = Generator().to(device)

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)
        
generator.apply(weights_init)

```


### Discriminator

The discriminator is a pretty conventional convnet. Its layers' weights are initialized in the same way as the generator's.

```{python, eval = FALSE}
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            # 14 x 14
            nn.Conv2d(1, image_size, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # 7 x 7
            nn.Conv2d(image_size, image_size * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(image_size * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # 3 x 3
            nn.Conv2d(image_size * 2, image_size * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(image_size * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # 1 x 1
            nn.Conv2d(image_size * 4, 1, 4, 2, 1, bias=False),
            nn.Sigmoid()
        )
    def forward(self, input):
        return self.main(input)


discriminator = Discriminator().to(device)

discriminator.apply(weights_init)

```


### Optimizers and loss function

While generator and discriminator have each their own losses, mathematically both use the same calculation, namely, binary crossentropy:

```{python, eval = FALSE}
criterion = nn.BCELoss()
```

They each have their own optimizer:

```{python, eval = FALSE}
lr = 0.0002
beta1 = 0.5

disc_optimizer = optim.Adam(discriminator.parameters(), lr = lr, betas = (beta1, 0.999))
gen_optimizer = optim.Adam(generator.parameters(), lr = lr, betas = (beta1, 0.999))
```

### Training loop

In every epoch, the training loop consists of three parts. 

First, the discriminator is trained. This is a two-stage procedure:
- In stage (1), it is given the real images, together with labels (fabricated on the fly) that say "these are real images". Binary cross entropy will be minimized when all those images are, in fact, classified as real by the discriminator.
- In stage (2), first the generator is asked to generate some images, and then the discriminator is asked to rate them. Again, binary cross entropy is calculated, but this time, it will be minimal if all images are characterized as fake.

Once gradients have been obtained in both computations, the discriminator's weights are updated. Then it's the generator's turn. We pass the newly generated fakes to the discriminator again; only this time, the desired verdict is "no fake", so the labels are set to "real". The binary cross entropy loss then reflects the generator's performance, not that of the discriminator.


```{python, eval = FALSE}
# random noise - input for generator
fixed_noise = torch.randn(64, latent_input_size, 1, 1, device = device)

real_label = 1
fake_label = 0

img_list = []
generator_losses = []
discriminator_losses = []
discriminator_outputs_fake = []
discriminator_outputs_real = []

num_epochs = 5

for epoch in range(num_epochs):

    for i, data in enumerate(dataloader, 0):
    
        # (1) Train discriminator
        
        ## Train with all-real batch

        discriminator.zero_grad()
        real_cpu = data[0].to(device)
        b_size = real_cpu.size(0)
        label = torch.full((b_size,), real_label, device=device)
        output = discriminator(real_cpu).view(-1)
        discriminator_loss_real = criterion(output, label)
        discriminator_loss_real.backward()
        discriminator_output_real = output.mean().item()
        
        ## Train with all-fake batch

        noise = torch.randn(b_size, latent_input_size, 1, 1, device=device)
        fake = generator(noise)
        label.fill_(fake_label)
        output = discriminator(fake.detach()).view(-1)
        discriminator_loss_fake = criterion(output, label)
        discriminator_loss_fake.backward()
    
        # Update discriminator weights
    
        discriminator_loss = discriminator_loss_real + discriminator_loss_fake    
        disc_optimizer.step()

        # (2) Train generator

        generator.zero_grad()
        label.fill_(real_label)  
        output = discriminator(fake).view(-1)
        generator_loss = criterion(output, label)
        generator_loss.backward()
        discriminator_output_fake = output.mean().item()

        # Update generator weights
        gen_optimizer.step()

        # Output training stats
        if i % 50 == 0:
            print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f '
                  % (epoch, num_epochs, i, len(dataloader),
                     discriminator_loss.item(), generator_loss.item(), discriminator_output_real, discriminator_output_fake))
            generator_losses.append(generator_loss.item())
            discriminator_losses.append(discriminator_loss.item())
            discriminator_outputs_fake.append(discriminator_output_fake)
            discriminator_outputs_real.append(discriminator_output_real)
            with torch.no_grad():
              fake = generator(fixed_noise).detach().cpu()
              img_list.append(utils.make_grid(fake, padding=2, normalize=True))

```

Now let's see samples of generated images, spread out over training time:

```{r, eval = FALSE}
index <- seq(1, length(py$img_list), length.out = 16)
images <- py$img_list[index]

par(mfcol = c(4,4), mar = rep(0, 4), oma = rep(0, 4))
rasterize <- function(x) {
     x <- x$numpy()
     x <- aperm(x, perm = c(2, 3, 1))
     as.raster(x)
}
images %>%
   purrr::map(rasterize) %>%
   purrr::iwalk(~{plot(.x)})
```

To my (untrained) eyes, the final results look pretty good! Let's generate a fresh batch: 

```{python, eval = FALSE}
new = generator(fixed_noise).cpu().detach().numpy()
```

```{r, eval = FALSE}
images <- py$new[,1,,] 

par(mfcol = c(8,8), mar = c(0, 0, 0, 0))
images %>%
  purrr::array_tree(1) %>%
  purrr::map(function(img) image(img, col = gray.colors(n = 255), xaxt = 'n', yaxt = 'n', asp = 1))
```

We can also inspect how the respective losses developed over time:

```{r, eval = FALSE}
library(ggplot2)
library(tidyr)

generator_losses <- py$generator_losses
discriminator_losses <- py$discriminator_losses
iterations <- 1:length(discriminator_losses)

df <- data.frame(iteration = iterations, discriminator = discriminator_losses, generator = generator_losses)
df %>%
  gather(module, loss, discriminator, generator) %>%
    ggplot(aes(x = iteration, y = loss, colour = module)) +
    geom_line()
```


As well as find out what proportion of real images was classified as real by the discriminator (green line), contrasting this with the proportion of fake images the discriminator thought were real. Ideally, we would hope for both lines to close to 0.5 (and thus, close to each other), which obviously isn't the case here. This means there is still room for improvement.


```{r, eval = FALSE}
library(ggplot2)
library(tidyr)

discriminator_outputs_fake <- py$discriminator_outputs_fake
discriminator_outputs_real <- py$discriminator_outputs_real
iterations <- 1:length(discriminator_outputs_real)

df <- data.frame(iteration = iterations, real = discriminator_outputs_real, fake = discriminator_outputs_fake)
df %>%
  gather(images, ratio_accepted, real, fake) %>%
    ggplot(aes(x = iteration, y = ratio_accepted, colour = images)) +
    geom_line()
```


